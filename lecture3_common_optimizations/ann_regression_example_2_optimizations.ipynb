{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN for regression, example 2, house market data (how to handle categorical variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Version 2: using some common optimization approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports / modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-learn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "# pip install tensorflow\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = pd.read_csv(\"Housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "545"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deep learning / neural network-wise, this is quite a small dataset\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>stories</th>\n",
       "      <th>mainroad</th>\n",
       "      <th>guestroom</th>\n",
       "      <th>basement</th>\n",
       "      <th>hotwaterheating</th>\n",
       "      <th>airconditioning</th>\n",
       "      <th>parking</th>\n",
       "      <th>prefarea</th>\n",
       "      <th>furnishingstatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13300000</td>\n",
       "      <td>7420</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12250000</td>\n",
       "      <td>8960</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12250000</td>\n",
       "      <td>9960</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>semi-furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12215000</td>\n",
       "      <td>7500</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>yes</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11410000</td>\n",
       "      <td>7420</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      price  area  bedrooms  bathrooms  stories mainroad guestroom basement  \\\n",
       "0  13300000  7420         4          2        3      yes        no       no   \n",
       "1  12250000  8960         4          4        4      yes        no       no   \n",
       "2  12250000  9960         3          2        2      yes        no      yes   \n",
       "3  12215000  7500         4          2        2      yes        no      yes   \n",
       "4  11410000  7420         4          1        2      yes       yes      yes   \n",
       "\n",
       "  hotwaterheating airconditioning  parking prefarea furnishingstatus  \n",
       "0              no             yes        2      yes        furnished  \n",
       "1              no             yes        3       no        furnished  \n",
       "2              no              no        2      yes   semi-furnished  \n",
       "3              no             yes        3      yes        furnished  \n",
       "4              no             yes        2       no        furnished  "
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see the first 5 rows \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "furnishingstatus\n",
       "semi-furnished    227\n",
       "unfurnished       178\n",
       "furnished         140\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['furnishingstatus'].value_counts()\n",
    "\n",
    "# usually we are only interested if the house\n",
    "# is furnished AT ALL or completely unfurnished\n",
    "# what if we combine semi-furnished with furnished?\n",
    "# (there's no way knowing if this works, it's more about trying creative\n",
    "# approaches in order to simplify/optimize the dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE HAVE SOME BOOLEAN CATEGORIES\n",
    "# => change them to 0 and 1\n",
    "\n",
    "# this just converts the value of column to 0 or 1\n",
    "# factorize in pandas works too, but only one column at a time\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "variables = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']\n",
    "encoder = LabelEncoder()\n",
    "df[variables] = df[variables].apply(encoder.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>stories</th>\n",
       "      <th>mainroad</th>\n",
       "      <th>guestroom</th>\n",
       "      <th>basement</th>\n",
       "      <th>hotwaterheating</th>\n",
       "      <th>airconditioning</th>\n",
       "      <th>parking</th>\n",
       "      <th>prefarea</th>\n",
       "      <th>furnishingstatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13300000</td>\n",
       "      <td>7420</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12250000</td>\n",
       "      <td>8960</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12250000</td>\n",
       "      <td>9960</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>semi-furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12215000</td>\n",
       "      <td>7500</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11410000</td>\n",
       "      <td>7420</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      price  area  bedrooms  bathrooms  stories  mainroad  guestroom  \\\n",
       "0  13300000  7420         4          2        3         1          0   \n",
       "1  12250000  8960         4          4        4         1          0   \n",
       "2  12250000  9960         3          2        2         1          0   \n",
       "3  12215000  7500         4          2        2         1          0   \n",
       "4  11410000  7420         4          1        2         1          1   \n",
       "\n",
       "   basement  hotwaterheating  airconditioning  parking  prefarea  \\\n",
       "0         0                0                1        2         1   \n",
       "1         0                0                1        3         0   \n",
       "2         1                0                0        2         1   \n",
       "3         1                0                1        3         1   \n",
       "4         1                0                1        2         0   \n",
       "\n",
       "  furnishingstatus  \n",
       "0        furnished  \n",
       "1        furnished  \n",
       "2   semi-furnished  \n",
       "3        furnished  \n",
       "4        furnished  "
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see how our dataset now changed\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert text categories with multiple choices into multiple variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SINCE WE ARE GOING TO MODIFY THE furnishingstatus from three options\n",
    "# to two options, we no longer need OneHotEncoder, since this is now \n",
    "# goint to be a boolean/binary variable\n",
    "\n",
    "# # this makes multiple columns with the variable (Separate for yes/no)\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# variables = ['furnishingstatus']\n",
    "\n",
    "# # use encoder\n",
    "# encoder = OneHotEncoder(sparse_output=False).set_output(transform=\"pandas\")\n",
    "# one_hot_encoded = encoder.fit_transform(df[variables]).astype(int)\n",
    "# df = pd.concat([df,one_hot_encoded],axis=1).drop(columns=variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a helper function for pandas and replace\n",
    "# the different furnishingstatus values into either 0 or 1\n",
    "def modify_furnishing(row):\n",
    "    if row['furnishingstatus'] == 'unfurnished':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "# create a new boolean/binary furnished status \n",
    "# 0 => not furnished\n",
    "# 1 => either fully or partially furnished\n",
    "df['furnished'] = df.apply(modify_furnishing, axis=1)\n",
    "\n",
    "# drop the original 3-option furnishing status\n",
    "df = df.drop(\"furnishingstatus\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no longer needed, since we don't use OneHotEncoder anymore for this\n",
    "# # we can alwasy remove EXACTLY ONE option per variable when we use OneHotEncoder\n",
    "# df = df.drop(\"furnishingstatus_unfurnished\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>stories</th>\n",
       "      <th>mainroad</th>\n",
       "      <th>guestroom</th>\n",
       "      <th>basement</th>\n",
       "      <th>hotwaterheating</th>\n",
       "      <th>airconditioning</th>\n",
       "      <th>parking</th>\n",
       "      <th>prefarea</th>\n",
       "      <th>furnished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13300000</td>\n",
       "      <td>7420</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12250000</td>\n",
       "      <td>8960</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12250000</td>\n",
       "      <td>9960</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12215000</td>\n",
       "      <td>7500</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11410000</td>\n",
       "      <td>7420</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10850000</td>\n",
       "      <td>7500</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10150000</td>\n",
       "      <td>8580</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10150000</td>\n",
       "      <td>16200</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9870000</td>\n",
       "      <td>8100</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9800000</td>\n",
       "      <td>5750</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      price   area  bedrooms  bathrooms  stories  mainroad  guestroom  \\\n",
       "0  13300000   7420         4          2        3         1          0   \n",
       "1  12250000   8960         4          4        4         1          0   \n",
       "2  12250000   9960         3          2        2         1          0   \n",
       "3  12215000   7500         4          2        2         1          0   \n",
       "4  11410000   7420         4          1        2         1          1   \n",
       "5  10850000   7500         3          3        1         1          0   \n",
       "6  10150000   8580         4          3        4         1          0   \n",
       "7  10150000  16200         5          3        2         1          0   \n",
       "8   9870000   8100         4          1        2         1          1   \n",
       "9   9800000   5750         3          2        4         1          1   \n",
       "\n",
       "   basement  hotwaterheating  airconditioning  parking  prefarea  furnished  \n",
       "0         0                0                1        2         1          1  \n",
       "1         0                0                1        3         0          1  \n",
       "2         1                0                0        2         1          1  \n",
       "3         1                0                1        3         1          1  \n",
       "4         1                0                1        2         0          1  \n",
       "5         1                0                1        2         1          1  \n",
       "6         0                0                1        2         1          1  \n",
       "7         0                0                0        0         0          0  \n",
       "8         1                0                1        2         1          1  \n",
       "9         0                0                1        1         1          0  "
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see what happened to our dataset\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usually the most straight-forward / quick&dirty -solution is to use\n",
    "# scipy and z-score\n",
    "\n",
    "# version 1, SciPy, extreme outliers that go under -3 or over +3 in normal distribution\n",
    "from scipy import stats\n",
    "df = df[(np.abs(stats.zscore(df)) < 3).all(axis=1)]\n",
    "\n",
    "# see also the quantile -based removal in Moodle, you have more control\n",
    "# over what is removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can also get as fancy you wish with outlier detection and removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n"
     ]
    }
   ],
   "source": [
    "# let's try a fancier example for outlier removal\n",
    "# this code is originally from:\n",
    "# https://stackoverflow.com/questions/69248118/detect-outliers-across-all-columns-of-pandas-dataframe\n",
    "\n",
    "def find_outliers(col):\n",
    "    q1 = col.quantile(.15)\n",
    "    q3 = col.quantile(.85)\n",
    "    IQR = q3 - q1\n",
    "    ll = q1 - (1.5*IQR)\n",
    "    ul = q3 + (1.5*IQR)\n",
    "    upper_outliers = col[col > ul].index.tolist()\n",
    "    lower_outliers = col[col < ll].index.tolist()\n",
    "    bad_indices = list(set(upper_outliers + lower_outliers))\n",
    "    return(bad_indices)\n",
    "\n",
    "# get indexes of all outliers into a list\n",
    "bad_indexes = []\n",
    "for col in df.columns:\n",
    "    if df[col].dtype in [\"int64\",\"float64\"]:\n",
    "        bad_indexes.append(find_outliers(df[col]))\n",
    "\n",
    "\n",
    "# modify the list so that we can drop these rows from the DataFrame\n",
    "\n",
    "bad_indexes = set(list(np.concatenate(bad_indexes).flat))\n",
    "\n",
    "print(len(bad_indexes))\n",
    "\n",
    "# drop the outliers\n",
    "df = df.drop(bad_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# it seems many of the more expensive houses were dropped off\n",
    "# we still have slightly skewed distribution due to expensive houses\n",
    "df['price'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X/y -split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform X/y -split\n",
    "# if you  have more than one independent variable, list them all here\n",
    "# leave out the target variable! (dependent variable)\n",
    "\n",
    "# this is a nice and common trick => everything EXCEPT target variable => support variable\n",
    "X = df.drop(\"price\", axis=1)\n",
    "\n",
    "# have only the target variable here (dependent variable)\n",
    "y = df[\"price\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the different tools to figure out which variables are important and which are not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>stories</th>\n",
       "      <th>mainroad</th>\n",
       "      <th>guestroom</th>\n",
       "      <th>basement</th>\n",
       "      <th>hotwaterheating</th>\n",
       "      <th>airconditioning</th>\n",
       "      <th>parking</th>\n",
       "      <th>prefarea</th>\n",
       "      <th>furnished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area</th>\n",
       "      <td>0.49</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bedrooms</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bathrooms</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stories</th>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mainroad</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guestroom</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>basement</th>\n",
       "      <td>0.27</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hotwaterheating</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airconditioning</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parking</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.13</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prefarea</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>furnished</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 price  area  bedrooms  bathrooms  stories  mainroad  \\\n",
       "price             1.00  0.49      0.37       0.43     0.20       NaN   \n",
       "area              0.49  1.00      0.11       0.13    -0.10       NaN   \n",
       "bedrooms          0.37  0.11      1.00       0.32     0.45       NaN   \n",
       "bathrooms         0.43  0.13      0.32       1.00     0.16       NaN   \n",
       "stories           0.20 -0.10      0.45       0.16     1.00       NaN   \n",
       "mainroad           NaN   NaN       NaN        NaN      NaN       NaN   \n",
       "guestroom         0.33  0.19      0.12       0.13    -0.00       NaN   \n",
       "basement          0.27  0.09      0.16       0.15    -0.12       NaN   \n",
       "hotwaterheating    NaN   NaN       NaN        NaN      NaN       NaN   \n",
       "airconditioning   0.46  0.21      0.19       0.18     0.14       NaN   \n",
       "parking           0.26  0.29      0.09       0.10    -0.13       NaN   \n",
       "prefarea          0.37  0.23      0.16       0.09     0.05       NaN   \n",
       "furnished         0.31  0.16      0.15       0.15     0.06       NaN   \n",
       "\n",
       "                 guestroom  basement  hotwaterheating  airconditioning  \\\n",
       "price                 0.33      0.27              NaN             0.46   \n",
       "area                  0.19      0.09              NaN             0.21   \n",
       "bedrooms              0.12      0.16              NaN             0.19   \n",
       "bathrooms             0.13      0.15              NaN             0.18   \n",
       "stories              -0.00     -0.12              NaN             0.14   \n",
       "mainroad               NaN       NaN              NaN              NaN   \n",
       "guestroom             1.00      0.43              NaN             0.13   \n",
       "basement              0.43      1.00              NaN             0.09   \n",
       "hotwaterheating        NaN       NaN              NaN              NaN   \n",
       "airconditioning       0.13      0.09              NaN             1.00   \n",
       "parking               0.05      0.06              NaN             0.13   \n",
       "prefarea              0.20      0.25              NaN             0.09   \n",
       "furnished             0.13      0.11              NaN             0.10   \n",
       "\n",
       "                 parking  prefarea  furnished  \n",
       "price               0.26      0.37       0.31  \n",
       "area                0.29      0.23       0.16  \n",
       "bedrooms            0.09      0.16       0.15  \n",
       "bathrooms           0.10      0.09       0.15  \n",
       "stories            -0.13      0.05       0.06  \n",
       "mainroad             NaN       NaN        NaN  \n",
       "guestroom           0.05      0.20       0.13  \n",
       "basement            0.06      0.25       0.11  \n",
       "hotwaterheating      NaN       NaN        NaN  \n",
       "airconditioning     0.13      0.09       0.10  \n",
       "parking             1.00      0.07       0.14  \n",
       "prefarea            0.07      1.00       0.08  \n",
       "furnished           0.14      0.08       1.00  "
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the most basic tool of them all => correlations\n",
    "correlations = df.corr(numeric_only=True)\n",
    "correlations\n",
    "\n",
    "# based on this, pretty much everything correlates with price\n",
    "# so we don't have unusable variables\n",
    "# for some reason, mainroad and hotwaterheating show as NaN (not a number)\n",
    "# probably not enough data or variance to show a correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pip install skfeature-chappers\n",
    "from skfeature.function.similarity_based import fisher_score\n",
    "\n",
    "# get the fisher's score rankings \n",
    "ranks = fisher_score.fisher_score(X.values, y.values)\n",
    "\n",
    "# create a pandas DataFrame for easier interpretation\n",
    "feat_importances = pd.Series(ranks, X.columns)\n",
    "feat_importances.plot(kind='barh')\n",
    "\n",
    "# how to interpret -> low score means the effect of this field is not large in the dataset\n",
    "# => typically means other columns in the dataset have similar correlations, \n",
    "# therefore making this particular column not so useful since other columns \n",
    "# already fill this role for this correlation\n",
    "\n",
    "# Fisher's score studies the variance of the data -> statistical significance'\n",
    "\n",
    "# based on Fisher's score:\n",
    "# num_rooms is not important at all, and num_people has minor importance in this data\n",
    "# but ave_monthly_income is quite powerful based on Fisher's score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>area</td>\n",
       "      <td>165447.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>parking</td>\n",
       "      <td>199.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>guestroom</td>\n",
       "      <td>176.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>prefarea</td>\n",
       "      <td>152.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>airconditioning</td>\n",
       "      <td>150.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>basement</td>\n",
       "      <td>125.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>furnished</td>\n",
       "      <td>70.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stories</td>\n",
       "      <td>42.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bedrooms</td>\n",
       "      <td>32.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bathrooms</td>\n",
       "      <td>30.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mainroad</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hotwaterheating</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Features     Score\n",
       "0              area 165447.39\n",
       "9           parking    199.95\n",
       "5         guestroom    176.03\n",
       "10         prefarea    152.81\n",
       "8   airconditioning    150.11\n",
       "6          basement    125.20\n",
       "11        furnished     70.32\n",
       "3           stories     42.50\n",
       "1          bedrooms     32.15\n",
       "2         bathrooms     30.15\n",
       "4          mainroad      0.00\n",
       "7   hotwaterheating       NaN"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# convert all continuous variables to integer,\n",
    "# and convert all negative numbers to 0\n",
    "X_cat = X.astype(int)\n",
    "X_cat = X_cat.clip(lower=0)\n",
    "\n",
    "# initialize chi2 and SelectKBest\n",
    "# Note: chi2 -test is a very common test\n",
    "# in statistics and quantitative analysis\n",
    "# basically it studies the data whether variables are related\n",
    "# or independent of each other\n",
    "chi_2_features = SelectKBest(chi2, k=len(X_cat.columns))\n",
    "\n",
    "# fit our data to the SelectKBest\n",
    "best_features = chi_2_features.fit(X_cat,y.astype(int))\n",
    "\n",
    "# use decimal format in table print later\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "# wrap it up, and show the results\n",
    "# the higher the score, the more effect that column has on target variable\n",
    "df_features = pd.DataFrame(best_features.scores_)\n",
    "df_columns = pd.DataFrame(X_cat.columns)\n",
    "f_scores = pd.concat([df_columns,df_features],axis=1)\n",
    "f_scores.columns = ['Features','Score']\n",
    "f_scores.sort_values(by='Score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            feature   VIF\n",
      "0              area  1.24\n",
      "1          bedrooms  1.47\n",
      "2         bathrooms  1.17\n",
      "3           stories  1.41\n",
      "4          mainroad 28.77\n",
      "5         guestroom  1.29\n",
      "6          basement  1.36\n",
      "7   hotwaterheating   NaN\n",
      "8   airconditioning  1.12\n",
      "9           parking  1.14\n",
      "10         prefarea  1.14\n",
      "11        furnished  1.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:1782: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return 1 - self.ssr/self.centered_tss\n"
     ]
    }
   ],
   "source": [
    "# pip install statsmodels\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor \n",
    "\n",
    "# VIF dataframe \n",
    "# VIF = Variance Inflation Factor\n",
    "vif_data = pd.DataFrame() \n",
    "vif_data[\"feature\"] = X.columns \n",
    "  \n",
    "# calculating VIF for each feature \n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) \n",
    "                          for i in range(len(X.columns))] \n",
    "  \n",
    "\n",
    "# variables with high VIF-value \n",
    "# can mean multlicollinearity (variables providing same linear\n",
    "# relationships in the data, potentially confusing the ML algorithm\n",
    "# this might be good info when deciding if some variable needs to be removed\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test/validation -split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Classic ML, we only had train/test -split\n",
    "# in deep learning, we usually use validation-data also, for better\n",
    "# optimization possibilities and better metrics\n",
    "\n",
    "# unfortunately the scikit-learn's train_test_split doesn't support validation\n",
    "# set split in itself.\n",
    "\n",
    "# if you want to split the test set into two for a validation set too, try this trick:\n",
    "\n",
    "# step 1, split the data into 70% (training data) and 30% (temporary data)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# step 2, split the temporary data in HALF (0.5) => 15% test and 15% validation\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a neural network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:142: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m)             │            \u001b[38;5;34m48\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_28 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_29 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)             │           \u001b[38;5;34m792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_30 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_31 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,673</span> (6.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,673\u001b[0m (6.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,649</span> (6.44 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,649\u001b[0m (6.44 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24</span> (96.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m24\u001b[0m (96.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create neural network\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# save the amount of support variables into a helper variable\n",
    "# so we don't have to update the input_shape all the time\n",
    "variable_amount = len(X.columns)\n",
    "\n",
    "# create callbacks and place them into a parameter list\n",
    "# NOTE! if you get PermissionError while training the model,\n",
    "# just try training it again\n",
    "mc = ModelCheckpoint('best_model_regression2_housing.keras', monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "# if you use multiple callbacks (EarlyStoppin, ReduceLROnPlateau etc.)\n",
    "# add them to this same list\n",
    "callback_list = [mc]\n",
    "\n",
    "# Define Sequential neural network model\n",
    "# input shape has to match the amount of SUPPORT VARIABLES\n",
    "# in other words => amount of columns in X \n",
    "\n",
    "# Tip: have at least the same number of nodes as in the input shape\n",
    "\n",
    "# since we have 13 support variables this time => 16 nodes in first layer\n",
    "\n",
    "# output layer in regression is always 1 node without activation function\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.BatchNormalization(input_shape=(variable_amount,)),\n",
    "        layers.Dense(32, activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=0.1, l2=0.1)),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(24, activation=\"relu\"),\n",
    "        layers.Dense(16, activation=\"relu\"),\n",
    "        layers.Dense(1)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# select the optimizer and loss function\n",
    "# you can try rmsprop also as optimizer, or stochastic gradient descent\n",
    "# model.compile(optimizer='adam', loss='mse')\n",
    "# model.compile(optimizer=keras.optimizers.RMSprop(0.00085), loss=keras.losses.Huber())\n",
    "# model.compile(optimizer=keras.optimizers.SGD(0.001), loss=\"mse\")\n",
    "# an example where we alter the learning rate of Adam-optimizer\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.00125), loss='mse')\n",
    "\n",
    "# common tips on how to change neural network structure if your metrics are not good:\n",
    "\n",
    "# make wider (or narrower) layers (for example, 64 or 128 nodes)\n",
    "# make a longer or shorter network (add or remove layers)\n",
    "# use Dropout -layers (e.g. layers.Dropout(0.1))\n",
    "\n",
    "# remember: there's no process or mathematical formula\n",
    "# in order to figure out the optimal neural network structure\n",
    "# it's mostly all about trial and error => EXPERIMENTATION!\n",
    "\n",
    "# remember to have enough \"decision-space\" for your data!\n",
    "# it's highly unlikely a dataset with 20 different variables is going\n",
    "# to work well with only 8 nodes in each layer etc.\n",
    "\n",
    "# print out the summary of your model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 23560544321536.0000 - val_loss: 20417590853632.0000\n",
      "Epoch 2/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 23560538030080.0000 - val_loss: 20417580367872.0000\n",
      "Epoch 3/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 23560533835776.0000 - val_loss: 20417565687808.0000\n",
      "Epoch 4/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 23560527544320.0000 - val_loss: 20417551007744.0000\n",
      "Epoch 5/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 23560512864256.0000 - val_loss: 20417530036224.0000\n",
      "Epoch 6/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 23560496087040.0000 - val_loss: 20417496481792.0000\n",
      "Epoch 7/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 23560468824064.0000 - val_loss: 20417448247296.0000\n",
      "Epoch 8/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 23560428978176.0000 - val_loss: 20417383235584.0000\n",
      "Epoch 9/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 23560370257920.0000 - val_loss: 20417288863744.0000\n",
      "Epoch 10/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 23560290566144.0000 - val_loss: 20417156743168.0000\n",
      "Epoch 11/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 23560160542720.0000 - val_loss: 20416972193792.0000\n",
      "Epoch 12/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 23559982284800.0000 - val_loss: 20416714244096.0000\n",
      "Epoch 13/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 23559736918016.0000 - val_loss: 20416368214016.0000\n",
      "Epoch 14/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 23559363624960.0000 - val_loss: 20415881674752.0000\n",
      "Epoch 15/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 23558925320192.0000 - val_loss: 20415225266176.0000\n",
      "Epoch 16/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 23558208094208.0000 - val_loss: 20414361239552.0000\n",
      "Epoch 17/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 23557362941952.0000 - val_loss: 20413220388864.0000\n",
      "Epoch 18/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 23556148690944.0000 - val_loss: 20411725119488.0000\n",
      "Epoch 19/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 23554494038016.0000 - val_loss: 20409753796608.0000\n",
      "Epoch 20/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 23552275251200.0000 - val_loss: 20407220436992.0000\n",
      "Epoch 21/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 23549643325440.0000 - val_loss: 20404032765952.0000\n",
      "Epoch 22/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 23546067681280.0000 - val_loss: 20400006234112.0000\n",
      "Epoch 23/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 23540814315520.0000 - val_loss: 20394973069312.0000\n",
      "Epoch 24/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 23535581921280.0000 - val_loss: 20388685807616.0000\n",
      "Epoch 25/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 23528130740224.0000 - val_loss: 20381119283200.0000\n",
      "Epoch 26/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 23518628544512.0000 - val_loss: 20371841482752.0000\n",
      "Epoch 27/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 23508077772800.0000 - val_loss: 20360810463232.0000\n",
      "Epoch 28/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 23494226083840.0000 - val_loss: 20347229306880.0000\n",
      "Epoch 29/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 23476213645312.0000 - val_loss: 20331320311808.0000\n",
      "Epoch 30/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 23458442379264.0000 - val_loss: 20312445943808.0000\n",
      "Epoch 31/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 23434704715776.0000 - val_loss: 20290054651904.0000\n",
      "Epoch 32/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 23407672426496.0000 - val_loss: 20264194670592.0000\n",
      "Epoch 33/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 23374417887232.0000 - val_loss: 20234008264704.0000\n",
      "Epoch 34/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 23336218263552.0000 - val_loss: 20198956466176.0000\n",
      "Epoch 35/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 23292238888960.0000 - val_loss: 20158466752512.0000\n",
      "Epoch 36/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 23240068038656.0000 - val_loss: 20112207773696.0000\n",
      "Epoch 37/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 23181777698816.0000 - val_loss: 20059070136320.0000\n",
      "Epoch 38/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 23113332948992.0000 - val_loss: 19998441472000.0000\n",
      "Epoch 39/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 23039001493504.0000 - val_loss: 19930112065536.0000\n",
      "Epoch 40/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 22947362242560.0000 - val_loss: 19852322406400.0000\n",
      "Epoch 41/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 22847485378560.0000 - val_loss: 19766513238016.0000\n",
      "Epoch 42/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 22735740731392.0000 - val_loss: 19669983428608.0000\n",
      "Epoch 43/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 22634762862592.0000 - val_loss: 19562433085440.0000\n",
      "Epoch 44/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 22486330638336.0000 - val_loss: 19444594114560.0000\n",
      "Epoch 45/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 22342765903872.0000 - val_loss: 19312567910400.0000\n",
      "Epoch 46/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 22161693605888.0000 - val_loss: 19167289802752.0000\n",
      "Epoch 47/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 21981864919040.0000 - val_loss: 19007025446912.0000\n",
      "Epoch 48/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 21774152499200.0000 - val_loss: 18831399452672.0000\n",
      "Epoch 49/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 21553987190784.0000 - val_loss: 18642146164736.0000\n",
      "Epoch 50/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 21314798616576.0000 - val_loss: 18431160090624.0000\n",
      "Epoch 51/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 21048124768256.0000 - val_loss: 18210042675200.0000\n",
      "Epoch 52/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 20798161027072.0000 - val_loss: 17968603856896.0000\n",
      "Epoch 53/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 20481570766848.0000 - val_loss: 17709683179520.0000\n",
      "Epoch 54/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 20153957875712.0000 - val_loss: 17429504720896.0000\n",
      "Epoch 55/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 19785416966144.0000 - val_loss: 17133103742976.0000\n",
      "Epoch 56/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 19402611228672.0000 - val_loss: 16812058083328.0000\n",
      "Epoch 57/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 19052772720640.0000 - val_loss: 16472620400640.0000\n",
      "Epoch 58/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 18556949364736.0000 - val_loss: 16113808179200.0000\n",
      "Epoch 59/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 18232910020608.0000 - val_loss: 15733264220160.0000\n",
      "Epoch 60/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 17745146019840.0000 - val_loss: 15339261788160.0000\n",
      "Epoch 61/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 17272570642432.0000 - val_loss: 14925622673408.0000\n",
      "Epoch 62/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 16699193556992.0000 - val_loss: 14479341387776.0000\n",
      "Epoch 63/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 16085784985600.0000 - val_loss: 14025181102080.0000\n",
      "Epoch 64/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 15667871875072.0000 - val_loss: 13546158030848.0000\n",
      "Epoch 65/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 14956059688960.0000 - val_loss: 13055400345600.0000\n",
      "Epoch 66/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 14418768297984.0000 - val_loss: 12548082499584.0000\n",
      "Epoch 67/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 13843377946624.0000 - val_loss: 12027519041536.0000\n",
      "Epoch 68/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 13161262481408.0000 - val_loss: 11502305148928.0000\n",
      "Epoch 69/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 12606418976768.0000 - val_loss: 10968322015232.0000\n",
      "Epoch 70/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 12100707549184.0000 - val_loss: 10420227145728.0000\n",
      "Epoch 71/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 11272122793984.0000 - val_loss: 9862892224512.0000\n",
      "Epoch 72/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 10677333786624.0000 - val_loss: 9313426866176.0000\n",
      "Epoch 73/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 10033386487808.0000 - val_loss: 8750011252736.0000\n",
      "Epoch 74/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9291418304512.0000 - val_loss: 8203730944000.0000\n",
      "Epoch 75/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8707288072192.0000 - val_loss: 7665614848000.0000\n",
      "Epoch 76/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 8084114112512.0000 - val_loss: 7137846624256.0000\n",
      "Epoch 77/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7509843116032.0000 - val_loss: 6614598287360.0000\n",
      "Epoch 78/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 6839690854400.0000 - val_loss: 6116242096128.0000\n",
      "Epoch 79/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 6239338627072.0000 - val_loss: 5617585487872.0000\n",
      "Epoch 80/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5758916755456.0000 - val_loss: 5155558785024.0000\n",
      "Epoch 81/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5336006656000.0000 - val_loss: 4712002748416.0000\n",
      "Epoch 82/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4816793239552.0000 - val_loss: 4302089748480.0000\n",
      "Epoch 83/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4408564252672.0000 - val_loss: 3925472706560.0000\n",
      "Epoch 84/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4206403780608.0000 - val_loss: 3558826835968.0000\n",
      "Epoch 85/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3607530569728.0000 - val_loss: 3227781693440.0000\n",
      "Epoch 86/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3370036756480.0000 - val_loss: 2916272570368.0000\n",
      "Epoch 87/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2957082361856.0000 - val_loss: 2643912294400.0000\n",
      "Epoch 88/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2725195022336.0000 - val_loss: 2400400965632.0000\n",
      "Epoch 89/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 2624906067968.0000 - val_loss: 2178989817856.0000\n",
      "Epoch 90/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2488406900736.0000 - val_loss: 1990189383680.0000\n",
      "Epoch 91/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2099750502400.0000 - val_loss: 1821251862528.0000\n",
      "Epoch 92/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1994177642496.0000 - val_loss: 1678013497344.0000\n",
      "Epoch 93/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1836099960832.0000 - val_loss: 1545356312576.0000\n",
      "Epoch 94/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1749544861696.0000 - val_loss: 1446685179904.0000\n",
      "Epoch 95/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1834991484928.0000 - val_loss: 1353576087552.0000\n",
      "Epoch 96/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1719133143040.0000 - val_loss: 1279757254656.0000\n",
      "Epoch 97/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1647689203712.0000 - val_loss: 1219506995200.0000\n",
      "Epoch 98/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1598066655232.0000 - val_loss: 1159732264960.0000\n",
      "Epoch 99/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1439224299520.0000 - val_loss: 1117231775744.0000\n",
      "Epoch 100/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1348443701248.0000 - val_loss: 1081352847360.0000\n",
      "Epoch 101/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1477874548736.0000 - val_loss: 1049881411584.0000\n",
      "Epoch 102/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1373440049152.0000 - val_loss: 1025292566528.0000\n",
      "Epoch 103/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1379224780800.0000 - val_loss: 1004036096000.0000\n",
      "Epoch 104/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1443592273920.0000 - val_loss: 984101945344.0000\n",
      "Epoch 105/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1346884337664.0000 - val_loss: 969906323456.0000\n",
      "Epoch 106/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1391140929536.0000 - val_loss: 957636083712.0000\n",
      "Epoch 107/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1265588240384.0000 - val_loss: 946227773440.0000\n",
      "Epoch 108/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1290291249152.0000 - val_loss: 937200713728.0000\n",
      "Epoch 109/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1364936753152.0000 - val_loss: 926976442368.0000\n",
      "Epoch 110/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1393433903104.0000 - val_loss: 919113891840.0000\n",
      "Epoch 111/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1388245811200.0000 - val_loss: 915122552832.0000\n",
      "Epoch 112/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1131366318080.0000 - val_loss: 910539882496.0000\n",
      "Epoch 113/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1256512159744.0000 - val_loss: 906063052800.0000\n",
      "Epoch 114/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1220974346240.0000 - val_loss: 901175574528.0000\n",
      "Epoch 115/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1328755769344.0000 - val_loss: 897061486592.0000\n",
      "Epoch 116/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1304505745408.0000 - val_loss: 891940110336.0000\n",
      "Epoch 117/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1317110022144.0000 - val_loss: 891870117888.0000\n",
      "Epoch 118/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1361025171456.0000 - val_loss: 889705857024.0000\n",
      "Epoch 119/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1241208193024.0000 - val_loss: 888088035328.0000\n",
      "Epoch 120/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1376313147392.0000 - val_loss: 887421140992.0000\n",
      "Epoch 121/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1295344730112.0000 - val_loss: 885604810752.0000\n",
      "Epoch 122/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1395090915328.0000 - val_loss: 884519272448.0000\n",
      "Epoch 123/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1187226583040.0000 - val_loss: 884547452928.0000\n",
      "Epoch 124/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1236071612416.0000 - val_loss: 883977486336.0000\n",
      "Epoch 125/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1303168942080.0000 - val_loss: 883672612864.0000\n",
      "Epoch 126/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1215338381312.0000 - val_loss: 880365600768.0000\n",
      "Epoch 127/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1162883366912.0000 - val_loss: 880517971968.0000\n",
      "Epoch 128/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1254585532416.0000 - val_loss: 876387368960.0000\n",
      "Epoch 129/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1192032075776.0000 - val_loss: 874487808000.0000\n",
      "Epoch 130/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1110288367616.0000 - val_loss: 872506785792.0000\n",
      "Epoch 131/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1118782357504.0000 - val_loss: 871246594048.0000\n",
      "Epoch 132/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1199043510272.0000 - val_loss: 870711558144.0000\n",
      "Epoch 133/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1126489391104.0000 - val_loss: 870118588416.0000\n",
      "Epoch 134/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1220962680832.0000 - val_loss: 868478484480.0000\n",
      "Epoch 135/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1164994281472.0000 - val_loss: 868881924096.0000\n",
      "Epoch 136/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1155766812672.0000 - val_loss: 871663599616.0000\n",
      "Epoch 137/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1182268391424.0000 - val_loss: 871656652800.0000\n",
      "Epoch 138/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1194670948352.0000 - val_loss: 870160793600.0000\n",
      "Epoch 139/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1212598059008.0000 - val_loss: 869078597632.0000\n",
      "Epoch 140/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1308818407424.0000 - val_loss: 869558190080.0000\n",
      "Epoch 141/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1144802508800.0000 - val_loss: 867592634368.0000\n",
      "Epoch 142/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1193632858112.0000 - val_loss: 866802532352.0000\n",
      "Epoch 143/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1287829454848.0000 - val_loss: 868402724864.0000\n",
      "Epoch 144/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1214281154560.0000 - val_loss: 868148772864.0000\n",
      "Epoch 145/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1201166221312.0000 - val_loss: 869486755840.0000\n",
      "Epoch 146/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1285962989568.0000 - val_loss: 866229813248.0000\n",
      "Epoch 147/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1239830757376.0000 - val_loss: 863975178240.0000\n",
      "Epoch 148/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1182386618368.0000 - val_loss: 864614088704.0000\n",
      "Epoch 149/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1184051888128.0000 - val_loss: 866477932544.0000\n",
      "Epoch 150/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1076035125248.0000 - val_loss: 866370256896.0000\n",
      "Epoch 151/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1192455569408.0000 - val_loss: 865535590400.0000\n",
      "Epoch 152/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1335374512128.0000 - val_loss: 862809030656.0000\n",
      "Epoch 153/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1177524764672.0000 - val_loss: 862577295360.0000\n",
      "Epoch 154/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1150033461248.0000 - val_loss: 863887687680.0000\n",
      "Epoch 155/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1202408783872.0000 - val_loss: 863513608192.0000\n",
      "Epoch 156/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1263540633600.0000 - val_loss: 864693387264.0000\n",
      "Epoch 157/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1208688836608.0000 - val_loss: 864536756224.0000\n",
      "Epoch 158/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1104536141824.0000 - val_loss: 865413562368.0000\n",
      "Epoch 159/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1162281877504.0000 - val_loss: 866716483584.0000\n",
      "Epoch 160/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1215177424896.0000 - val_loss: 867259449344.0000\n",
      "Epoch 161/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1092592992256.0000 - val_loss: 865479426048.0000\n",
      "Epoch 162/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1139343622144.0000 - val_loss: 865392525312.0000\n",
      "Epoch 163/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1102077231104.0000 - val_loss: 865016020992.0000\n",
      "Epoch 164/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1119466029056.0000 - val_loss: 864335298560.0000\n",
      "Epoch 165/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1131077566464.0000 - val_loss: 863848300544.0000\n",
      "Epoch 166/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1057622851584.0000 - val_loss: 864552812544.0000\n",
      "Epoch 167/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1069228228608.0000 - val_loss: 866130132992.0000\n",
      "Epoch 168/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1183389974528.0000 - val_loss: 865860845568.0000\n",
      "Epoch 169/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1253112283136.0000 - val_loss: 867912253440.0000\n",
      "Epoch 170/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1193941401600.0000 - val_loss: 867757457408.0000\n",
      "Epoch 171/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1044005781504.0000 - val_loss: 865693073408.0000\n",
      "Epoch 172/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1189541838848.0000 - val_loss: 867059630080.0000\n",
      "Epoch 173/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1211943747584.0000 - val_loss: 866106408960.0000\n",
      "Epoch 174/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1080153800704.0000 - val_loss: 864826753024.0000\n",
      "Epoch 175/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1098245210112.0000 - val_loss: 863798624256.0000\n",
      "Epoch 176/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1074760450048.0000 - val_loss: 865043021824.0000\n",
      "Epoch 177/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1108980531200.0000 - val_loss: 865116160000.0000\n",
      "Epoch 178/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1147420672000.0000 - val_loss: 866696953856.0000\n",
      "Epoch 179/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1115298988032.0000 - val_loss: 866604417024.0000\n",
      "Epoch 180/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1257122037760.0000 - val_loss: 867567468544.0000\n",
      "Epoch 181/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1149075849216.0000 - val_loss: 869770199040.0000\n",
      "Epoch 182/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1048027070464.0000 - val_loss: 871710588928.0000\n",
      "Epoch 183/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 980937539584.0000 - val_loss: 870499287040.0000\n",
      "Epoch 184/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1130671112192.0000 - val_loss: 872284553216.0000\n",
      "Epoch 185/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1116707880960.0000 - val_loss: 871203733504.0000\n",
      "Epoch 186/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1052879618048.0000 - val_loss: 870032146432.0000\n",
      "Epoch 187/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1145280921600.0000 - val_loss: 870268600320.0000\n",
      "Epoch 188/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1119342166016.0000 - val_loss: 868258283520.0000\n",
      "Epoch 189/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1228720177152.0000 - val_loss: 869229854720.0000\n",
      "Epoch 190/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1132692504576.0000 - val_loss: 870559514624.0000\n",
      "Epoch 191/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1149530669056.0000 - val_loss: 872361689088.0000\n",
      "Epoch 192/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1098520068096.0000 - val_loss: 873254879232.0000\n",
      "Epoch 193/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1021987061760.0000 - val_loss: 872473952256.0000\n",
      "Epoch 194/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1132386058240.0000 - val_loss: 870324109312.0000\n",
      "Epoch 195/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1119415959552.0000 - val_loss: 871997112320.0000\n",
      "Epoch 196/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1070287683584.0000 - val_loss: 870502891520.0000\n",
      "Epoch 197/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1214651564032.0000 - val_loss: 871321894912.0000\n",
      "Epoch 198/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1174169059328.0000 - val_loss: 871438417920.0000\n",
      "Epoch 199/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1186449981440.0000 - val_loss: 872192475136.0000\n",
      "Epoch 200/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 994915385344.0000 - val_loss: 872150990848.0000\n",
      "Epoch 201/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1195849809920.0000 - val_loss: 872954134528.0000\n",
      "Epoch 202/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1066564517888.0000 - val_loss: 873092218880.0000\n",
      "Epoch 203/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1176222040064.0000 - val_loss: 873407643648.0000\n",
      "Epoch 204/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1185754251264.0000 - val_loss: 874480533504.0000\n",
      "Epoch 205/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1215043076096.0000 - val_loss: 872504819712.0000\n",
      "Epoch 206/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1216338722816.0000 - val_loss: 873512566784.0000\n",
      "Epoch 207/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1059436625920.0000 - val_loss: 873890250752.0000\n",
      "Epoch 208/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1239631921152.0000 - val_loss: 874301816832.0000\n",
      "Epoch 209/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1078111764480.0000 - val_loss: 873928785920.0000\n",
      "Epoch 210/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1247008915456.0000 - val_loss: 874008936448.0000\n",
      "Epoch 211/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1136318480384.0000 - val_loss: 876321243136.0000\n",
      "Epoch 212/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1110851452928.0000 - val_loss: 878353580032.0000\n",
      "Epoch 213/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1062477037568.0000 - val_loss: 879072247808.0000\n",
      "Epoch 214/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 999645380608.0000 - val_loss: 877199491072.0000\n",
      "Epoch 215/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1186822881280.0000 - val_loss: 878933901312.0000\n",
      "Epoch 216/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1154204172288.0000 - val_loss: 878644953088.0000\n",
      "Epoch 217/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1109253685248.0000 - val_loss: 875728732160.0000\n",
      "Epoch 218/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1067337121792.0000 - val_loss: 876547735552.0000\n",
      "Epoch 219/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1059740123136.0000 - val_loss: 879363620864.0000\n",
      "Epoch 220/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1115255078912.0000 - val_loss: 879733702656.0000\n",
      "Epoch 221/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1176090705920.0000 - val_loss: 878949302272.0000\n",
      "Epoch 222/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1085977460736.0000 - val_loss: 881020960768.0000\n",
      "Epoch 223/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 947360628736.0000 - val_loss: 878637416448.0000\n",
      "Epoch 224/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1154947874816.0000 - val_loss: 879585787904.0000\n",
      "Epoch 225/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1172056834048.0000 - val_loss: 880329818112.0000\n",
      "Epoch 226/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1099722784768.0000 - val_loss: 882240520192.0000\n",
      "Epoch 227/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1119165087744.0000 - val_loss: 883465715712.0000\n",
      "Epoch 228/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1127468498944.0000 - val_loss: 880613523456.0000\n",
      "Epoch 229/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1127061913600.0000 - val_loss: 879131033600.0000\n",
      "Epoch 230/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1096885010432.0000 - val_loss: 879841640448.0000\n",
      "Epoch 231/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1088939491328.0000 - val_loss: 880660643840.0000\n",
      "Epoch 232/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1203005685760.0000 - val_loss: 879466840064.0000\n",
      "Epoch 233/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1188992909312.0000 - val_loss: 877820379136.0000\n",
      "Epoch 234/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1220658593792.0000 - val_loss: 881208852480.0000\n",
      "Epoch 235/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1079893884928.0000 - val_loss: 879599484928.0000\n",
      "Epoch 236/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1132979290112.0000 - val_loss: 878819540992.0000\n",
      "Epoch 237/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1078123626496.0000 - val_loss: 878942552064.0000\n",
      "Epoch 238/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1120767311872.0000 - val_loss: 880420126720.0000\n",
      "Epoch 239/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1163187191808.0000 - val_loss: 880869113856.0000\n",
      "Epoch 240/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1076167245824.0000 - val_loss: 883344408576.0000\n",
      "Epoch 241/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1089162706944.0000 - val_loss: 884563116032.0000\n",
      "Epoch 242/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1071606661120.0000 - val_loss: 887217389568.0000\n",
      "Epoch 243/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1151886557184.0000 - val_loss: 887191502848.0000\n",
      "Epoch 244/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1195904729088.0000 - val_loss: 886360702976.0000\n",
      "Epoch 245/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1167208480768.0000 - val_loss: 886164291584.0000\n",
      "Epoch 246/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1085769121792.0000 - val_loss: 883584401408.0000\n",
      "Epoch 247/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1132424200192.0000 - val_loss: 884719616000.0000\n",
      "Epoch 248/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1205742469120.0000 - val_loss: 884076118016.0000\n",
      "Epoch 249/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1094881509376.0000 - val_loss: 883153567744.0000\n",
      "Epoch 250/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1117844013056.0000 - val_loss: 880432840704.0000\n",
      "Epoch 251/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1106822823936.0000 - val_loss: 879515729920.0000\n",
      "Epoch 252/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1058621489152.0000 - val_loss: 878626865152.0000\n",
      "Epoch 253/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1103905816576.0000 - val_loss: 880636264448.0000\n",
      "Epoch 254/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1039305211904.0000 - val_loss: 886131064832.0000\n",
      "Epoch 255/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1025250099200.0000 - val_loss: 887076356096.0000\n",
      "Epoch 256/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 988714106880.0000 - val_loss: 887030349824.0000\n",
      "Epoch 257/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1102923169792.0000 - val_loss: 884633370624.0000\n",
      "Epoch 258/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1128289009664.0000 - val_loss: 884016218112.0000\n",
      "Epoch 259/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1095666565120.0000 - val_loss: 883476463616.0000\n",
      "Epoch 260/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1087877742592.0000 - val_loss: 884171407360.0000\n",
      "Epoch 261/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1183105155072.0000 - val_loss: 887190323200.0000\n",
      "Epoch 262/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1124108075008.0000 - val_loss: 886751428608.0000\n",
      "Epoch 263/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1186671362048.0000 - val_loss: 884446789632.0000\n",
      "Epoch 264/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1142030204928.0000 - val_loss: 887010557952.0000\n",
      "Epoch 265/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1206552887296.0000 - val_loss: 885853061120.0000\n",
      "Epoch 266/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1157855576064.0000 - val_loss: 887026286592.0000\n",
      "Epoch 267/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1105693114368.0000 - val_loss: 889324371968.0000\n",
      "Epoch 268/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1098335125504.0000 - val_loss: 889220497408.0000\n",
      "Epoch 269/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1102112489472.0000 - val_loss: 888959270912.0000\n",
      "Epoch 270/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1166487715840.0000 - val_loss: 890476756992.0000\n",
      "Epoch 271/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1054280843264.0000 - val_loss: 889985564672.0000\n",
      "Epoch 272/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1107537035264.0000 - val_loss: 889469140992.0000\n",
      "Epoch 273/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1137243979776.0000 - val_loss: 888710561792.0000\n",
      "Epoch 274/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1106705907712.0000 - val_loss: 890522632192.0000\n",
      "Epoch 275/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1030389235712.0000 - val_loss: 890582204416.0000\n",
      "Epoch 276/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1052777185280.0000 - val_loss: 888990662656.0000\n",
      "Epoch 277/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1002504060928.0000 - val_loss: 887462690816.0000\n",
      "Epoch 278/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1154498035712.0000 - val_loss: 885090877440.0000\n",
      "Epoch 279/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1181425467392.0000 - val_loss: 884529889280.0000\n",
      "Epoch 280/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1101730938880.0000 - val_loss: 886620618752.0000\n",
      "Epoch 281/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1074833195008.0000 - val_loss: 887496966144.0000\n",
      "Epoch 282/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1120411713536.0000 - val_loss: 889414483968.0000\n",
      "Epoch 283/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 986009305088.0000 - val_loss: 889048530944.0000\n",
      "Epoch 284/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1097687957504.0000 - val_loss: 891184807936.0000\n",
      "Epoch 285/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1094412599296.0000 - val_loss: 891618328576.0000\n",
      "Epoch 286/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1069379878912.0000 - val_loss: 894149132288.0000\n",
      "Epoch 287/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 989777821696.0000 - val_loss: 891124842496.0000\n",
      "Epoch 288/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1177402605568.0000 - val_loss: 888654397440.0000\n",
      "Epoch 289/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1084416458752.0000 - val_loss: 888452349952.0000\n",
      "Epoch 290/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1009749524480.0000 - val_loss: 890292535296.0000\n",
      "Epoch 291/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1076617609216.0000 - val_loss: 891708833792.0000\n",
      "Epoch 292/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1111373643776.0000 - val_loss: 890773110784.0000\n",
      "Epoch 293/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1044937834496.0000 - val_loss: 889255886848.0000\n",
      "Epoch 294/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1044463091712.0000 - val_loss: 888096882688.0000\n",
      "Epoch 295/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1110147334144.0000 - val_loss: 889446858752.0000\n",
      "Epoch 296/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1219041558528.0000 - val_loss: 887821369344.0000\n",
      "Epoch 297/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1102380138496.0000 - val_loss: 886808248320.0000\n",
      "Epoch 298/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1110133178368.0000 - val_loss: 887404101632.0000\n",
      "Epoch 299/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1103946055680.0000 - val_loss: 891391574016.0000\n",
      "Epoch 300/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1046037463040.0000 - val_loss: 895043239936.0000\n",
      "Epoch 301/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1018430095360.0000 - val_loss: 895022268416.0000\n",
      "Epoch 302/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1100587859968.0000 - val_loss: 894594711552.0000\n",
      "Epoch 303/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1118680645632.0000 - val_loss: 893400186880.0000\n",
      "Epoch 304/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1119505481728.0000 - val_loss: 892860170240.0000\n",
      "Epoch 305/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1252632166400.0000 - val_loss: 891506524160.0000\n",
      "Epoch 306/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1109738651648.0000 - val_loss: 893058613248.0000\n",
      "Epoch 307/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1098507812864.0000 - val_loss: 894514036736.0000\n",
      "Epoch 308/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1094306103296.0000 - val_loss: 896275316736.0000\n",
      "Epoch 309/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1160749645824.0000 - val_loss: 892304359424.0000\n",
      "Epoch 310/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1206413688832.0000 - val_loss: 894029922304.0000\n",
      "Epoch 311/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 932558471168.0000 - val_loss: 893103833088.0000\n",
      "Epoch 312/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1127934590976.0000 - val_loss: 893450649600.0000\n",
      "Epoch 313/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1105388765184.0000 - val_loss: 892121972736.0000\n",
      "Epoch 314/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1095762771968.0000 - val_loss: 894919114752.0000\n",
      "Epoch 315/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1189834915840.0000 - val_loss: 895395758080.0000\n",
      "Epoch 316/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1138323226624.0000 - val_loss: 892730343424.0000\n",
      "Epoch 317/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 984544903168.0000 - val_loss: 895181324288.0000\n",
      "Epoch 318/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1242349568000.0000 - val_loss: 897976172544.0000\n",
      "Epoch 319/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1101856112640.0000 - val_loss: 897727594496.0000\n",
      "Epoch 320/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1163929976832.0000 - val_loss: 896835059712.0000\n",
      "Epoch 321/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1096742010880.0000 - val_loss: 899204710400.0000\n",
      "Epoch 322/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1070574272512.0000 - val_loss: 895276744704.0000\n",
      "Epoch 323/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1171461505024.0000 - val_loss: 895481872384.0000\n",
      "Epoch 324/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1113688244224.0000 - val_loss: 897315700736.0000\n",
      "Epoch 325/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1107558924288.0000 - val_loss: 896688259072.0000\n",
      "Epoch 326/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1117177511936.0000 - val_loss: 896487784448.0000\n",
      "Epoch 327/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1042690539520.0000 - val_loss: 893617176576.0000\n",
      "Epoch 328/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1113223462912.0000 - val_loss: 892583936000.0000\n",
      "Epoch 329/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1164970164224.0000 - val_loss: 893701914624.0000\n",
      "Epoch 330/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1185100201984.0000 - val_loss: 894301634560.0000\n",
      "Epoch 331/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1225450192896.0000 - val_loss: 892084617216.0000\n",
      "Epoch 332/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1041420386304.0000 - val_loss: 892351938560.0000\n",
      "Epoch 333/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1199411429376.0000 - val_loss: 892879962112.0000\n",
      "Epoch 334/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1110223355904.0000 - val_loss: 895573295104.0000\n",
      "Epoch 335/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1166437777408.0000 - val_loss: 896903282688.0000\n",
      "Epoch 336/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1056082821120.0000 - val_loss: 898959343616.0000\n",
      "Epoch 337/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1068277039104.0000 - val_loss: 897286864896.0000\n",
      "Epoch 338/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1041207132160.0000 - val_loss: 896964493312.0000\n",
      "Epoch 339/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1125368070144.0000 - val_loss: 897038811136.0000\n",
      "Epoch 340/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1194423615488.0000 - val_loss: 895757320192.0000\n",
      "Epoch 341/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1064335966208.0000 - val_loss: 894925930496.0000\n",
      "Epoch 342/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1083870216192.0000 - val_loss: 892881862656.0000\n",
      "Epoch 343/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1094782418944.0000 - val_loss: 893605773312.0000\n",
      "Epoch 344/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1126662537216.0000 - val_loss: 893270753280.0000\n",
      "Epoch 345/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1127893172224.0000 - val_loss: 893359554560.0000\n",
      "Epoch 346/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1170002018304.0000 - val_loss: 892850012160.0000\n",
      "Epoch 347/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1100564267008.0000 - val_loss: 894243373056.0000\n",
      "Epoch 348/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1229217333248.0000 - val_loss: 895072010240.0000\n",
      "Epoch 349/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1086910234624.0000 - val_loss: 894906073088.0000\n",
      "Epoch 350/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1029468258304.0000 - val_loss: 895025283072.0000\n",
      "Epoch 351/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1090118746112.0000 - val_loss: 897539244032.0000\n",
      "Epoch 352/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1083574779904.0000 - val_loss: 897504903168.0000\n",
      "Epoch 353/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1094842384384.0000 - val_loss: 897823539200.0000\n",
      "Epoch 354/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1142796976128.0000 - val_loss: 900599382016.0000\n",
      "Epoch 355/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1122323922944.0000 - val_loss: 899647537152.0000\n",
      "Epoch 356/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1049326780416.0000 - val_loss: 898208497664.0000\n",
      "Epoch 357/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1076949483520.0000 - val_loss: 900149542912.0000\n",
      "Epoch 358/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1082691354624.0000 - val_loss: 899963551744.0000\n",
      "Epoch 359/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1230724530176.0000 - val_loss: 899088777216.0000\n",
      "Epoch 360/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1090140045312.0000 - val_loss: 901044568064.0000\n",
      "Epoch 361/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1153858666496.0000 - val_loss: 903068844032.0000\n",
      "Epoch 362/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1151816564736.0000 - val_loss: 904349483008.0000\n",
      "Epoch 363/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1211786199040.0000 - val_loss: 905094430720.0000\n",
      "Epoch 364/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1022545756160.0000 - val_loss: 903742947328.0000\n",
      "Epoch 365/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1155391029248.0000 - val_loss: 903770406912.0000\n",
      "Epoch 366/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1206099247104.0000 - val_loss: 899846111232.0000\n",
      "Epoch 367/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1101737754624.0000 - val_loss: 897826947072.0000\n",
      "Epoch 368/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1212703571968.0000 - val_loss: 900477288448.0000\n",
      "Epoch 369/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1116356739072.0000 - val_loss: 899511287808.0000\n",
      "Epoch 370/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1093674270720.0000 - val_loss: 899321233408.0000\n",
      "Epoch 371/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1080379047936.0000 - val_loss: 902635126784.0000\n",
      "Epoch 372/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1055826116608.0000 - val_loss: 899835101184.0000\n",
      "Epoch 373/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1082717372416.0000 - val_loss: 898063400960.0000\n",
      "Epoch 374/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1280510132224.0000 - val_loss: 899486384128.0000\n",
      "Epoch 375/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1119586615296.0000 - val_loss: 901271912448.0000\n",
      "Epoch 376/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1165618315264.0000 - val_loss: 901046992896.0000\n",
      "Epoch 377/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1120132661248.0000 - val_loss: 901781782528.0000\n",
      "Epoch 378/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1108201177088.0000 - val_loss: 898082340864.0000\n",
      "Epoch 379/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1040634937344.0000 - val_loss: 897777205248.0000\n",
      "Epoch 380/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1172381499392.0000 - val_loss: 898179989504.0000\n",
      "Epoch 381/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1129628172288.0000 - val_loss: 898884501504.0000\n",
      "Epoch 382/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1110197927936.0000 - val_loss: 901922029568.0000\n",
      "Epoch 383/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1071765061632.0000 - val_loss: 901781913600.0000\n",
      "Epoch 384/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1152776011776.0000 - val_loss: 903703560192.0000\n",
      "Epoch 385/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1140322992128.0000 - val_loss: 903815954432.0000\n",
      "Epoch 386/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1074141986816.0000 - val_loss: 902663241728.0000\n",
      "Epoch 387/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1061481021440.0000 - val_loss: 899828809728.0000\n",
      "Epoch 388/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1064724201472.0000 - val_loss: 900374200320.0000\n",
      "Epoch 389/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1044620247040.0000 - val_loss: 900330553344.0000\n",
      "Epoch 390/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1008066560000.0000 - val_loss: 899380609024.0000\n",
      "Epoch 391/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1088166166528.0000 - val_loss: 899862626304.0000\n",
      "Epoch 392/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1117701144576.0000 - val_loss: 899921215488.0000\n",
      "Epoch 393/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 991063179264.0000 - val_loss: 902105726976.0000\n",
      "Epoch 394/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1179253866496.0000 - val_loss: 905601417216.0000\n",
      "Epoch 395/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1151929548800.0000 - val_loss: 905899802624.0000\n",
      "Epoch 396/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1050064125952.0000 - val_loss: 901877399552.0000\n",
      "Epoch 397/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1158429278208.0000 - val_loss: 899459121152.0000\n",
      "Epoch 398/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1230946697216.0000 - val_loss: 900568317952.0000\n",
      "Epoch 399/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1114500366336.0000 - val_loss: 898922053632.0000\n",
      "Epoch 400/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1020914827264.0000 - val_loss: 900948819968.0000\n",
      "Epoch 401/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1060761960448.0000 - val_loss: 901664997376.0000\n",
      "Epoch 402/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1074571706368.0000 - val_loss: 902158876672.0000\n",
      "Epoch 403/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1218029027328.0000 - val_loss: 898373910528.0000\n",
      "Epoch 404/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1094164021248.0000 - val_loss: 899216113664.0000\n",
      "Epoch 405/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1116240871424.0000 - val_loss: 897874132992.0000\n",
      "Epoch 406/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1237448916992.0000 - val_loss: 900518379520.0000\n",
      "Epoch 407/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1077077016576.0000 - val_loss: 897290797056.0000\n",
      "Epoch 408/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1106455691264.0000 - val_loss: 897344143360.0000\n",
      "Epoch 409/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 990129160192.0000 - val_loss: 899647602688.0000\n",
      "Epoch 410/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1107371491328.0000 - val_loss: 900744871936.0000\n",
      "Epoch 411/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1183822118912.0000 - val_loss: 900148035584.0000\n",
      "Epoch 412/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1109971173376.0000 - val_loss: 903164788736.0000\n",
      "Epoch 413/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1105063575552.0000 - val_loss: 901787287552.0000\n",
      "Epoch 414/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1085660528640.0000 - val_loss: 900236640256.0000\n",
      "Epoch 415/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1075230277632.0000 - val_loss: 899964141568.0000\n",
      "Epoch 416/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1110242492416.0000 - val_loss: 898317090816.0000\n",
      "Epoch 417/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1066290053120.0000 - val_loss: 897633550336.0000\n",
      "Epoch 418/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1088170950656.0000 - val_loss: 896351404032.0000\n",
      "Epoch 419/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1161973989376.0000 - val_loss: 898349334528.0000\n",
      "Epoch 420/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1120924467200.0000 - val_loss: 897044643840.0000\n",
      "Epoch 421/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1159798194176.0000 - val_loss: 901623644160.0000\n",
      "Epoch 422/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1097668493312.0000 - val_loss: 899339124736.0000\n",
      "Epoch 423/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1229275267072.0000 - val_loss: 899274113024.0000\n",
      "Epoch 424/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1202800427008.0000 - val_loss: 901561057280.0000\n",
      "Epoch 425/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1073976705024.0000 - val_loss: 901339807744.0000\n",
      "Epoch 426/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1061287559168.0000 - val_loss: 897554448384.0000\n",
      "Epoch 427/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1280320602112.0000 - val_loss: 897938096128.0000\n",
      "Epoch 428/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1090338947072.0000 - val_loss: 898261385216.0000\n",
      "Epoch 429/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1059598499840.0000 - val_loss: 898956066816.0000\n",
      "Epoch 430/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1010701303808.0000 - val_loss: 898036662272.0000\n",
      "Epoch 431/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1048913772544.0000 - val_loss: 897507000320.0000\n",
      "Epoch 432/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1033940369408.0000 - val_loss: 898361065472.0000\n",
      "Epoch 433/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1098228563968.0000 - val_loss: 899205234688.0000\n",
      "Epoch 434/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1127221166080.0000 - val_loss: 899865313280.0000\n",
      "Epoch 435/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1071473819648.0000 - val_loss: 899242983424.0000\n",
      "Epoch 436/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1095227867136.0000 - val_loss: 900705550336.0000\n",
      "Epoch 437/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 968952512512.0000 - val_loss: 902042615808.0000\n",
      "Epoch 438/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1179504345088.0000 - val_loss: 902414073856.0000\n",
      "Epoch 439/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1050450132992.0000 - val_loss: 899659726848.0000\n",
      "Epoch 440/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1054196105216.0000 - val_loss: 901451939840.0000\n",
      "Epoch 441/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1148241969152.0000 - val_loss: 897982201856.0000\n",
      "Epoch 442/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1145189957632.0000 - val_loss: 899503357952.0000\n",
      "Epoch 443/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1087439699968.0000 - val_loss: 899935698944.0000\n",
      "Epoch 444/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1110598352896.0000 - val_loss: 900570611712.0000\n",
      "Epoch 445/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1183228624896.0000 - val_loss: 899855679488.0000\n",
      "Epoch 446/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1002657284096.0000 - val_loss: 899010330624.0000\n",
      "Epoch 447/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1123722067968.0000 - val_loss: 897356136448.0000\n",
      "Epoch 448/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1087982075904.0000 - val_loss: 897644822528.0000\n",
      "Epoch 449/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1134825963520.0000 - val_loss: 896859897856.0000\n",
      "Epoch 450/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1173913731072.0000 - val_loss: 896686686208.0000\n",
      "Epoch 451/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1038111735808.0000 - val_loss: 897152057344.0000\n",
      "Epoch 452/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1113762824192.0000 - val_loss: 896971046912.0000\n",
      "Epoch 453/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1028126670848.0000 - val_loss: 898589196288.0000\n",
      "Epoch 454/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1055081758720.0000 - val_loss: 904655405056.0000\n",
      "Epoch 455/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1129149366272.0000 - val_loss: 907258626048.0000\n",
      "Epoch 456/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1087933382656.0000 - val_loss: 908713918464.0000\n",
      "Epoch 457/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1142836428800.0000 - val_loss: 904772386816.0000\n",
      "Epoch 458/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1121471954944.0000 - val_loss: 904057061376.0000\n",
      "Epoch 459/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1185710997504.0000 - val_loss: 905432662016.0000\n",
      "Epoch 460/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1015078977536.0000 - val_loss: 905201844224.0000\n",
      "Epoch 461/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1077644099584.0000 - val_loss: 903817396224.0000\n",
      "Epoch 462/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1043180748800.0000 - val_loss: 901504630784.0000\n",
      "Epoch 463/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1185623703552.0000 - val_loss: 900405854208.0000\n",
      "Epoch 464/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1107375423488.0000 - val_loss: 900551081984.0000\n",
      "Epoch 465/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1107813597184.0000 - val_loss: 901669191680.0000\n",
      "Epoch 466/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1065024356352.0000 - val_loss: 902763315200.0000\n",
      "Epoch 467/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1247646580736.0000 - val_loss: 904528527360.0000\n",
      "Epoch 468/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1080676843520.0000 - val_loss: 906912071680.0000\n",
      "Epoch 469/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1093776441344.0000 - val_loss: 904957984768.0000\n",
      "Epoch 470/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1254118391808.0000 - val_loss: 906590420992.0000\n",
      "Epoch 471/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1073487937536.0000 - val_loss: 904083734528.0000\n",
      "Epoch 472/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1169984061440.0000 - val_loss: 905169403904.0000\n",
      "Epoch 473/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1045271412736.0000 - val_loss: 904999206912.0000\n",
      "Epoch 474/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1101280575488.0000 - val_loss: 905550364672.0000\n",
      "Epoch 475/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1063697055744.0000 - val_loss: 905749463040.0000\n",
      "Epoch 476/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1117796433920.0000 - val_loss: 903964590080.0000\n",
      "Epoch 477/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1219401744384.0000 - val_loss: 903497711616.0000\n",
      "Epoch 478/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1171598868480.0000 - val_loss: 905882435584.0000\n",
      "Epoch 479/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1120370950144.0000 - val_loss: 910586216448.0000\n",
      "Epoch 480/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1164881166336.0000 - val_loss: 912599023616.0000\n",
      "Epoch 481/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1103727427584.0000 - val_loss: 907347951616.0000\n",
      "Epoch 482/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1250460696576.0000 - val_loss: 903355170816.0000\n",
      "Epoch 483/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1147203616768.0000 - val_loss: 901430312960.0000\n",
      "Epoch 484/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 973722812416.0000 - val_loss: 902397427712.0000\n",
      "Epoch 485/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1065227517952.0000 - val_loss: 903908360192.0000\n",
      "Epoch 486/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1074020155392.0000 - val_loss: 906447945728.0000\n",
      "Epoch 487/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1043963641856.0000 - val_loss: 905035644928.0000\n",
      "Epoch 488/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1065992323072.0000 - val_loss: 905702211584.0000\n",
      "Epoch 489/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1125015486464.0000 - val_loss: 905664331776.0000\n",
      "Epoch 490/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1123607248896.0000 - val_loss: 903474184192.0000\n",
      "Epoch 491/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1100811206656.0000 - val_loss: 905695526912.0000\n",
      "Epoch 492/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1143795744768.0000 - val_loss: 905232121856.0000\n",
      "Epoch 493/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1095359266816.0000 - val_loss: 904780972032.0000\n",
      "Epoch 494/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1172687028224.0000 - val_loss: 905981984768.0000\n",
      "Epoch 495/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1075933741056.0000 - val_loss: 907000741888.0000\n",
      "Epoch 496/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1119846531072.0000 - val_loss: 909953990656.0000\n",
      "Epoch 497/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1177364463616.0000 - val_loss: 909558087680.0000\n",
      "Epoch 498/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1158782386176.0000 - val_loss: 907219107840.0000\n",
      "Epoch 499/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1062388826112.0000 - val_loss: 907219435520.0000\n",
      "Epoch 500/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1092535648256.0000 - val_loss: 908215255040.0000\n",
      "Epoch 501/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1120140918784.0000 - val_loss: 906435100672.0000\n",
      "Epoch 502/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1143869538304.0000 - val_loss: 908512591872.0000\n",
      "Epoch 503/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1091814555648.0000 - val_loss: 907632771072.0000\n",
      "Epoch 504/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 983600529408.0000 - val_loss: 906620960768.0000\n",
      "Epoch 505/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1085969727488.0000 - val_loss: 905643950080.0000\n",
      "Epoch 506/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1036073172992.0000 - val_loss: 906137174016.0000\n",
      "Epoch 507/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1096930754560.0000 - val_loss: 906741678080.0000\n",
      "Epoch 508/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 976467460096.0000 - val_loss: 906450173952.0000\n",
      "Epoch 509/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1072136978432.0000 - val_loss: 904140226560.0000\n",
      "Epoch 510/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1071143190528.0000 - val_loss: 903837253632.0000\n",
      "Epoch 511/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1082778845184.0000 - val_loss: 901618728960.0000\n",
      "Epoch 512/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1056707379200.0000 - val_loss: 902466895872.0000\n",
      "Epoch 513/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 974260994048.0000 - val_loss: 903746748416.0000\n",
      "Epoch 514/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1055949848576.0000 - val_loss: 904460959744.0000\n",
      "Epoch 515/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1048540086272.0000 - val_loss: 906023075840.0000\n",
      "Epoch 516/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1119979175936.0000 - val_loss: 907174674432.0000\n",
      "Epoch 517/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1064353005568.0000 - val_loss: 909200261120.0000\n",
      "Epoch 518/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1079105945600.0000 - val_loss: 908509249536.0000\n",
      "Epoch 519/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1047167434752.0000 - val_loss: 906166337536.0000\n",
      "Epoch 520/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 998523404288.0000 - val_loss: 905141485568.0000\n",
      "Epoch 521/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1197497384960.0000 - val_loss: 904826191872.0000\n",
      "Epoch 522/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 965654937600.0000 - val_loss: 903499939840.0000\n",
      "Epoch 523/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1113153601536.0000 - val_loss: 901675286528.0000\n",
      "Epoch 524/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1069170425856.0000 - val_loss: 902118113280.0000\n",
      "Epoch 525/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1015047979008.0000 - val_loss: 904680570880.0000\n",
      "Epoch 526/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 977445715968.0000 - val_loss: 905334292480.0000\n",
      "Epoch 527/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1156334092288.0000 - val_loss: 906666770432.0000\n",
      "Epoch 528/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1131016093696.0000 - val_loss: 911919939584.0000\n",
      "Epoch 529/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1053888675840.0000 - val_loss: 909533511680.0000\n",
      "Epoch 530/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1144805654528.0000 - val_loss: 904891858944.0000\n",
      "Epoch 531/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1229400047616.0000 - val_loss: 907977293824.0000\n",
      "Epoch 532/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1174190161920.0000 - val_loss: 908381782016.0000\n",
      "Epoch 533/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1058983051264.0000 - val_loss: 910149025792.0000\n",
      "Epoch 534/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1087177883648.0000 - val_loss: 908236750848.0000\n",
      "Epoch 535/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1028829544448.0000 - val_loss: 903652245504.0000\n",
      "Epoch 536/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1117512531968.0000 - val_loss: 903180582912.0000\n",
      "Epoch 537/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1200489496576.0000 - val_loss: 903053901824.0000\n",
      "Epoch 538/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1035594629120.0000 - val_loss: 904229814272.0000\n",
      "Epoch 539/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 938535944192.0000 - val_loss: 903181172736.0000\n",
      "Epoch 540/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1037712228352.0000 - val_loss: 902952255488.0000\n",
      "Epoch 541/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1115836121088.0000 - val_loss: 905621733376.0000\n",
      "Epoch 542/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1176797184000.0000 - val_loss: 904833466368.0000\n",
      "Epoch 543/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1164327911424.0000 - val_loss: 904818524160.0000\n",
      "Epoch 544/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1094165463040.0000 - val_loss: 906605625344.0000\n",
      "Epoch 545/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1057603256320.0000 - val_loss: 906089988096.0000\n",
      "Epoch 546/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1066094231552.0000 - val_loss: 911331426304.0000\n",
      "Epoch 547/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1123635560448.0000 - val_loss: 911421669376.0000\n",
      "Epoch 548/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1198634565632.0000 - val_loss: 912223830016.0000\n",
      "Epoch 549/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1176775032832.0000 - val_loss: 910144110592.0000\n",
      "Epoch 550/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1000213446656.0000 - val_loss: 906907287552.0000\n",
      "Epoch 551/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1199469101056.0000 - val_loss: 903450984448.0000\n",
      "Epoch 552/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 969677144064.0000 - val_loss: 903375945728.0000\n",
      "Epoch 553/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1124284104704.0000 - val_loss: 903220232192.0000\n",
      "Epoch 554/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1152481099776.0000 - val_loss: 901030805504.0000\n",
      "Epoch 555/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1139255803904.0000 - val_loss: 902327828480.0000\n",
      "Epoch 556/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1133418250240.0000 - val_loss: 905180282880.0000\n",
      "Epoch 557/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1069253197824.0000 - val_loss: 906453909504.0000\n",
      "Epoch 558/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1048605360128.0000 - val_loss: 905349890048.0000\n",
      "Epoch 559/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1204893646848.0000 - val_loss: 904600813568.0000\n",
      "Epoch 560/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1075776323584.0000 - val_loss: 902982074368.0000\n",
      "Epoch 561/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1208799985664.0000 - val_loss: 904014987264.0000\n",
      "Epoch 562/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1136373923840.0000 - val_loss: 904685748224.0000\n",
      "Epoch 563/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 995299753984.0000 - val_loss: 905220063232.0000\n",
      "Epoch 564/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1106737233920.0000 - val_loss: 905828630528.0000\n",
      "Epoch 565/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1047611572224.0000 - val_loss: 904820424704.0000\n",
      "Epoch 566/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1178319847424.0000 - val_loss: 902724845568.0000\n",
      "Epoch 567/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1081576390656.0000 - val_loss: 901660409856.0000\n",
      "Epoch 568/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1170415681536.0000 - val_loss: 902722813952.0000\n",
      "Epoch 569/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1047834198016.0000 - val_loss: 904833531904.0000\n",
      "Epoch 570/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1092501700608.0000 - val_loss: 904648523776.0000\n",
      "Epoch 571/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1175767744512.0000 - val_loss: 906210181120.0000\n",
      "Epoch 572/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1149778001920.0000 - val_loss: 907350835200.0000\n",
      "Epoch 573/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1174203662336.0000 - val_loss: 906187898880.0000\n",
      "Epoch 574/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1120950681600.0000 - val_loss: 905227862016.0000\n",
      "Epoch 575/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1102713323520.0000 - val_loss: 905506258944.0000\n",
      "Epoch 576/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1098790731776.0000 - val_loss: 909968998400.0000\n",
      "Epoch 577/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1116212166656.0000 - val_loss: 910875557888.0000\n",
      "Epoch 578/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1097653420032.0000 - val_loss: 904833269760.0000\n",
      "Epoch 579/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1143658643456.0000 - val_loss: 906639310848.0000\n",
      "Epoch 580/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1272456806400.0000 - val_loss: 908513312768.0000\n",
      "Epoch 581/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1070921416704.0000 - val_loss: 911440609280.0000\n",
      "Epoch 582/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1065829859328.0000 - val_loss: 910715912192.0000\n",
      "Epoch 583/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1141194227712.0000 - val_loss: 908470255616.0000\n",
      "Epoch 584/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1140648181760.0000 - val_loss: 908974292992.0000\n",
      "Epoch 585/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1033743040512.0000 - val_loss: 908675645440.0000\n",
      "Epoch 586/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1056369016832.0000 - val_loss: 908205621248.0000\n",
      "Epoch 587/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1079106928640.0000 - val_loss: 905169862656.0000\n",
      "Epoch 588/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1128124121088.0000 - val_loss: 904499822592.0000\n",
      "Epoch 589/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1220098129920.0000 - val_loss: 905035513856.0000\n",
      "Epoch 590/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1170747949056.0000 - val_loss: 902052380672.0000\n",
      "Epoch 591/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1135614099456.0000 - val_loss: 902473646080.0000\n",
      "Epoch 592/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1133652606976.0000 - val_loss: 904428650496.0000\n",
      "Epoch 593/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1068702629888.0000 - val_loss: 904595308544.0000\n",
      "Epoch 594/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1122361540608.0000 - val_loss: 906529079296.0000\n",
      "Epoch 595/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1157503909888.0000 - val_loss: 904654946304.0000\n",
      "Epoch 596/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1106864373760.0000 - val_loss: 903657684992.0000\n",
      "Epoch 597/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1156778164224.0000 - val_loss: 905943056384.0000\n",
      "Epoch 598/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1168306864128.0000 - val_loss: 904436842496.0000\n",
      "Epoch 599/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1089106083840.0000 - val_loss: 905464446976.0000\n",
      "Epoch 600/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1055095914496.0000 - val_loss: 905534767104.0000\n",
      "Epoch 601/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1107285245952.0000 - val_loss: 903405240320.0000\n",
      "Epoch 602/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1061534695424.0000 - val_loss: 902815547392.0000\n",
      "Epoch 603/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1080681889792.0000 - val_loss: 901583208448.0000\n",
      "Epoch 604/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1130479353856.0000 - val_loss: 903644577792.0000\n",
      "Epoch 605/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1044948910080.0000 - val_loss: 905191686144.0000\n",
      "Epoch 606/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1106277564416.0000 - val_loss: 905489154048.0000\n",
      "Epoch 607/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1145204899840.0000 - val_loss: 906572267520.0000\n",
      "Epoch 608/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1069588873216.0000 - val_loss: 904247246848.0000\n",
      "Epoch 609/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1083963932672.0000 - val_loss: 904269004800.0000\n",
      "Epoch 610/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1032269987840.0000 - val_loss: 903158038528.0000\n",
      "Epoch 611/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1134255407104.0000 - val_loss: 902245580800.0000\n",
      "Epoch 612/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1001679159296.0000 - val_loss: 903425294336.0000\n",
      "Epoch 613/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1059764895744.0000 - val_loss: 904036155392.0000\n",
      "Epoch 614/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 987215233024.0000 - val_loss: 903990411264.0000\n",
      "Epoch 615/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1020801384448.0000 - val_loss: 903947485184.0000\n",
      "Epoch 616/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1136300392448.0000 - val_loss: 903635009536.0000\n",
      "Epoch 617/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1104522772480.0000 - val_loss: 902687555584.0000\n",
      "Epoch 618/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1129954410496.0000 - val_loss: 907273830400.0000\n",
      "Epoch 619/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1075181715456.0000 - val_loss: 907851333632.0000\n",
      "Epoch 620/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1070283489280.0000 - val_loss: 906026090496.0000\n",
      "Epoch 621/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1037288407040.0000 - val_loss: 905115271168.0000\n",
      "Epoch 622/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1178899054592.0000 - val_loss: 903678590976.0000\n",
      "Epoch 623/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1149292642304.0000 - val_loss: 902259605504.0000\n",
      "Epoch 624/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1115163197440.0000 - val_loss: 901209587712.0000\n",
      "Epoch 625/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 980040417280.0000 - val_loss: 899656318976.0000\n",
      "Epoch 626/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1276356984832.0000 - val_loss: 900485808128.0000\n",
      "Epoch 627/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1095498268672.0000 - val_loss: 899910991872.0000\n",
      "Epoch 628/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1174418751488.0000 - val_loss: 901362417664.0000\n",
      "Epoch 629/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1060476616704.0000 - val_loss: 904387690496.0000\n",
      "Epoch 630/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1079219060736.0000 - val_loss: 907412635648.0000\n",
      "Epoch 631/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1073058873344.0000 - val_loss: 910251917312.0000\n",
      "Epoch 632/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1110984753152.0000 - val_loss: 908835356672.0000\n",
      "Epoch 633/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1053368254464.0000 - val_loss: 906701635584.0000\n",
      "Epoch 634/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1077892677632.0000 - val_loss: 906136125440.0000\n",
      "Epoch 635/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1112753307648.0000 - val_loss: 909683064832.0000\n",
      "Epoch 636/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1164423725056.0000 - val_loss: 908790267904.0000\n",
      "Epoch 637/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 987369504768.0000 - val_loss: 905612886016.0000\n",
      "Epoch 638/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1106500124672.0000 - val_loss: 907309678592.0000\n",
      "Epoch 639/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1127571390464.0000 - val_loss: 908069109760.0000\n",
      "Epoch 640/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1133084409856.0000 - val_loss: 907761483776.0000\n",
      "Epoch 641/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1060427988992.0000 - val_loss: 906834870272.0000\n",
      "Epoch 642/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1015615455232.0000 - val_loss: 907912347648.0000\n",
      "Epoch 643/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1190373490688.0000 - val_loss: 907422466048.0000\n",
      "Epoch 644/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1123639099392.0000 - val_loss: 903369916416.0000\n",
      "Epoch 645/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1095255261184.0000 - val_loss: 903675772928.0000\n",
      "Epoch 646/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1120969162752.0000 - val_loss: 902096617472.0000\n",
      "Epoch 647/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 949295906816.0000 - val_loss: 903911833600.0000\n",
      "Epoch 648/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1052996272128.0000 - val_loss: 908416581632.0000\n",
      "Epoch 649/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1173828534272.0000 - val_loss: 911553986560.0000\n",
      "Epoch 650/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1122141732864.0000 - val_loss: 907118313472.0000\n",
      "Epoch 651/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1062370082816.0000 - val_loss: 905126019072.0000\n",
      "Epoch 652/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1106328420352.0000 - val_loss: 903049117696.0000\n",
      "Epoch 653/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1065097363456.0000 - val_loss: 902420561920.0000\n",
      "Epoch 654/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1085749592064.0000 - val_loss: 904205565952.0000\n",
      "Epoch 655/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1180958851072.0000 - val_loss: 902679166976.0000\n",
      "Epoch 656/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1048255266816.0000 - val_loss: 904070365184.0000\n",
      "Epoch 657/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1135590637568.0000 - val_loss: 906202513408.0000\n",
      "Epoch 658/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1123273146368.0000 - val_loss: 910429192192.0000\n",
      "Epoch 659/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1079928160256.0000 - val_loss: 908716081152.0000\n",
      "Epoch 660/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1117421961216.0000 - val_loss: 909020889088.0000\n",
      "Epoch 661/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1089177976832.0000 - val_loss: 907921326080.0000\n",
      "Epoch 662/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1148888678400.0000 - val_loss: 909366329344.0000\n",
      "Epoch 663/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1150165975040.0000 - val_loss: 907357585408.0000\n",
      "Epoch 664/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1115070660608.0000 - val_loss: 907978801152.0000\n",
      "Epoch 665/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1013409906688.0000 - val_loss: 902277693440.0000\n",
      "Epoch 666/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1162658316288.0000 - val_loss: 900574674944.0000\n",
      "Epoch 667/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1120163725312.0000 - val_loss: 898640969728.0000\n",
      "Epoch 668/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1037146652672.0000 - val_loss: 900796973056.0000\n",
      "Epoch 669/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1036355371008.0000 - val_loss: 901640290304.0000\n",
      "Epoch 670/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1309985079296.0000 - val_loss: 902818430976.0000\n",
      "Epoch 671/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1117317103616.0000 - val_loss: 901970001920.0000\n",
      "Epoch 672/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1035094130688.0000 - val_loss: 902676873216.0000\n",
      "Epoch 673/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1170139119616.0000 - val_loss: 903905935360.0000\n",
      "Epoch 674/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1128549842944.0000 - val_loss: 908710379520.0000\n",
      "Epoch 675/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1011815874560.0000 - val_loss: 908415008768.0000\n",
      "Epoch 676/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1045802582016.0000 - val_loss: 908276400128.0000\n",
      "Epoch 677/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1145483821056.0000 - val_loss: 908398034944.0000\n",
      "Epoch 678/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1111030497280.0000 - val_loss: 907744837632.0000\n",
      "Epoch 679/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1109423030272.0000 - val_loss: 908980453376.0000\n",
      "Epoch 680/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1188121804800.0000 - val_loss: 907788550144.0000\n",
      "Epoch 681/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1084720939008.0000 - val_loss: 911118041088.0000\n",
      "Epoch 682/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1099898814464.0000 - val_loss: 912276586496.0000\n",
      "Epoch 683/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1137678745600.0000 - val_loss: 913392205824.0000\n",
      "Epoch 684/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 990096916480.0000 - val_loss: 910617214976.0000\n",
      "Epoch 685/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1023178047488.0000 - val_loss: 907277828096.0000\n",
      "Epoch 686/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1042023645184.0000 - val_loss: 907358961664.0000\n",
      "Epoch 687/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1022718771200.0000 - val_loss: 910187429888.0000\n",
      "Epoch 688/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1038921433088.0000 - val_loss: 909487439872.0000\n",
      "Epoch 689/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1174111125504.0000 - val_loss: 911156838400.0000\n",
      "Epoch 690/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1014620553216.0000 - val_loss: 910113767424.0000\n",
      "Epoch 691/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1148774907904.0000 - val_loss: 911925182464.0000\n",
      "Epoch 692/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1017929269248.0000 - val_loss: 906868817920.0000\n",
      "Epoch 693/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1138870583296.0000 - val_loss: 907027349504.0000\n",
      "Epoch 694/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1003441422336.0000 - val_loss: 908954566656.0000\n",
      "Epoch 695/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1074357862400.0000 - val_loss: 908902203392.0000\n",
      "Epoch 696/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1213859233792.0000 - val_loss: 910415233024.0000\n",
      "Epoch 697/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1086767628288.0000 - val_loss: 910286585856.0000\n",
      "Epoch 698/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1058289025024.0000 - val_loss: 911234695168.0000\n",
      "Epoch 699/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1154787835904.0000 - val_loss: 911720513536.0000\n",
      "Epoch 700/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1112388009984.0000 - val_loss: 911936847872.0000\n",
      "Epoch 701/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1069994737664.0000 - val_loss: 907289296896.0000\n",
      "Epoch 702/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1181368188928.0000 - val_loss: 905196404736.0000\n",
      "Epoch 703/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1066783473664.0000 - val_loss: 905103802368.0000\n",
      "Epoch 704/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1037150126080.0000 - val_loss: 903098859520.0000\n",
      "Epoch 705/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1114610728960.0000 - val_loss: 903559970816.0000\n",
      "Epoch 706/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1157130354688.0000 - val_loss: 901060034560.0000\n",
      "Epoch 707/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1148998516736.0000 - val_loss: 903316701184.0000\n",
      "Epoch 708/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1183372148736.0000 - val_loss: 907924340736.0000\n",
      "Epoch 709/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1090729017344.0000 - val_loss: 909287227392.0000\n",
      "Epoch 710/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1100111937536.0000 - val_loss: 906555686912.0000\n",
      "Epoch 711/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1047467982848.0000 - val_loss: 903115243520.0000\n",
      "Epoch 712/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1158080495616.0000 - val_loss: 901621153792.0000\n",
      "Epoch 713/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1168609378304.0000 - val_loss: 902020333568.0000\n",
      "Epoch 714/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1152642973696.0000 - val_loss: 905822404608.0000\n",
      "Epoch 715/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1090225307648.0000 - val_loss: 903724138496.0000\n",
      "Epoch 716/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1030288965632.0000 - val_loss: 903726301184.0000\n",
      "Epoch 717/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1123059367936.0000 - val_loss: 901866651648.0000\n",
      "Epoch 718/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1153175126016.0000 - val_loss: 903028867072.0000\n",
      "Epoch 719/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1168123756544.0000 - val_loss: 903054884864.0000\n",
      "Epoch 720/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1235763331072.0000 - val_loss: 903946502144.0000\n",
      "Epoch 721/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1088978026496.0000 - val_loss: 904138326016.0000\n",
      "Epoch 722/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1158085214208.0000 - val_loss: 904496807936.0000\n",
      "Epoch 723/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 997975851008.0000 - val_loss: 903916486656.0000\n",
      "Epoch 724/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1150591434752.0000 - val_loss: 904782675968.0000\n",
      "Epoch 725/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1077610938368.0000 - val_loss: 903754219520.0000\n",
      "Epoch 726/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1036338593792.0000 - val_loss: 901281611776.0000\n",
      "Epoch 727/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1062015795200.0000 - val_loss: 901282398208.0000\n",
      "Epoch 728/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1090171174912.0000 - val_loss: 902707740672.0000\n",
      "Epoch 729/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1121720598528.0000 - val_loss: 903366443008.0000\n",
      "Epoch 730/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1107338199040.0000 - val_loss: 909610188800.0000\n",
      "Epoch 731/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1106122768384.0000 - val_loss: 910238744576.0000\n",
      "Epoch 732/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1073914052608.0000 - val_loss: 911101198336.0000\n",
      "Epoch 733/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1254763266048.0000 - val_loss: 906058924032.0000\n",
      "Epoch 734/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1166514061312.0000 - val_loss: 907631984640.0000\n",
      "Epoch 735/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1155970891776.0000 - val_loss: 907958616064.0000\n",
      "Epoch 736/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1067171053568.0000 - val_loss: 904668708864.0000\n",
      "Epoch 737/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1021528506368.0000 - val_loss: 907591942144.0000\n",
      "Epoch 738/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1115355217920.0000 - val_loss: 914087149568.0000\n",
      "Epoch 739/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1126779453440.0000 - val_loss: 910947123200.0000\n",
      "Epoch 740/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1043328532480.0000 - val_loss: 909791199232.0000\n",
      "Epoch 741/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1108555595776.0000 - val_loss: 907755847680.0000\n",
      "Epoch 742/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1124524359680.0000 - val_loss: 904826650624.0000\n",
      "Epoch 743/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1040747462656.0000 - val_loss: 905971105792.0000\n",
      "Epoch 744/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1108743290880.0000 - val_loss: 906944643072.0000\n",
      "Epoch 745/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1209541853184.0000 - val_loss: 907121524736.0000\n",
      "Epoch 746/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1241067159552.0000 - val_loss: 905059303424.0000\n",
      "Epoch 747/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1035815092224.0000 - val_loss: 906698620928.0000\n",
      "Epoch 748/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1041886216192.0000 - val_loss: 906403577856.0000\n",
      "Epoch 749/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1078827679744.0000 - val_loss: 909776191488.0000\n",
      "Epoch 750/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1194594533376.0000 - val_loss: 912339173376.0000\n",
      "Epoch 751/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1125716328448.0000 - val_loss: 910453243904.0000\n",
      "Epoch 752/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1068531384320.0000 - val_loss: 907628052480.0000\n",
      "Epoch 753/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1192242053120.0000 - val_loss: 904744206336.0000\n",
      "Epoch 754/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1073365581824.0000 - val_loss: 903364542464.0000\n",
      "Epoch 755/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1161563865088.0000 - val_loss: 903991328768.0000\n",
      "Epoch 756/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1074485460992.0000 - val_loss: 904819113984.0000\n",
      "Epoch 757/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1170531549184.0000 - val_loss: 907759779840.0000\n",
      "Epoch 758/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1085369679872.0000 - val_loss: 907064639488.0000\n",
      "Epoch 759/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1018757054464.0000 - val_loss: 908184453120.0000\n",
      "Epoch 760/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1089941471232.0000 - val_loss: 906757603328.0000\n",
      "Epoch 761/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1116296183808.0000 - val_loss: 908900630528.0000\n",
      "Epoch 762/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1156273537024.0000 - val_loss: 905898622976.0000\n",
      "Epoch 763/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1127344898048.0000 - val_loss: 904963686400.0000\n",
      "Epoch 764/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1131811045376.0000 - val_loss: 907001004032.0000\n",
      "Epoch 765/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1110271066112.0000 - val_loss: 908922519552.0000\n",
      "Epoch 766/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1233608638464.0000 - val_loss: 909795721216.0000\n",
      "Epoch 767/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1167484780544.0000 - val_loss: 910255063040.0000\n",
      "Epoch 768/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 972388761600.0000 - val_loss: 907326914560.0000\n",
      "Epoch 769/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1097399205888.0000 - val_loss: 904942190592.0000\n",
      "Epoch 770/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1189541838848.0000 - val_loss: 907090395136.0000\n",
      "Epoch 771/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1109861859328.0000 - val_loss: 908446466048.0000\n",
      "Epoch 772/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1099836817408.0000 - val_loss: 906675159040.0000\n",
      "Epoch 773/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1085805166592.0000 - val_loss: 905885253632.0000\n",
      "Epoch 774/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1006291517440.0000 - val_loss: 904787722240.0000\n",
      "Epoch 775/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1198496415744.0000 - val_loss: 905388097536.0000\n",
      "Epoch 776/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1131722440704.0000 - val_loss: 906865672192.0000\n",
      "Epoch 777/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1144507072512.0000 - val_loss: 909216907264.0000\n",
      "Epoch 778/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1172744568832.0000 - val_loss: 910265090048.0000\n",
      "Epoch 779/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1081468452864.0000 - val_loss: 907615272960.0000\n",
      "Epoch 780/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1037516734464.0000 - val_loss: 906178265088.0000\n",
      "Epoch 781/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1001049817088.0000 - val_loss: 907846877184.0000\n",
      "Epoch 782/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1010498469888.0000 - val_loss: 908545032192.0000\n",
      "Epoch 783/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1108426489856.0000 - val_loss: 905575923712.0000\n",
      "Epoch 784/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1177000476672.0000 - val_loss: 905297723392.0000\n",
      "Epoch 785/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1204949221376.0000 - val_loss: 906777919488.0000\n",
      "Epoch 786/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1254470320128.0000 - val_loss: 913825857536.0000\n",
      "Epoch 787/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1081433653248.0000 - val_loss: 910068482048.0000\n",
      "Epoch 788/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1066864279552.0000 - val_loss: 906087497728.0000\n",
      "Epoch 789/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1220522016768.0000 - val_loss: 903867858944.0000\n",
      "Epoch 790/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1172606550016.0000 - val_loss: 901166202880.0000\n",
      "Epoch 791/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1136605659136.0000 - val_loss: 902821380096.0000\n",
      "Epoch 792/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1165589872640.0000 - val_loss: 903523663872.0000\n",
      "Epoch 793/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1008721461248.0000 - val_loss: 907146821632.0000\n",
      "Epoch 794/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1156967038976.0000 - val_loss: 906502799360.0000\n",
      "Epoch 795/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1157669060608.0000 - val_loss: 906379264000.0000\n",
      "Epoch 796/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1118230806528.0000 - val_loss: 907310530560.0000\n",
      "Epoch 797/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1006672609280.0000 - val_loss: 909243383808.0000\n",
      "Epoch 798/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1079817273344.0000 - val_loss: 907122245632.0000\n",
      "Epoch 799/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1148085862400.0000 - val_loss: 907904614400.0000\n",
      "Epoch 800/800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1150257725440.0000 - val_loss: 909950844928.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x143823f7e90>"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train/fit neural network with validation data\n",
    "# see the instructions on the train/test -split above on how to split the data correctly\n",
    "model.fit(x=X_train, y=y_train, epochs=800, validation_data=(X_val, y_val), callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance and error metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's use pandas for this (easy code)\n",
    "# try to look if the model is actually training \n",
    "# => the error is going downwards\n",
    "# if using validation data, you get two lines\n",
    "# in this case, see if the lines follow a similar trend \n",
    "# (they don't always overlap with complex data, the trend is more important)\n",
    "loss_df = pd.DataFrame(model.history.history)\n",
    "loss_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload the best version of the model from history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(\"best_model_regression2_housing.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data evaluation:\n",
      "1093779914752.0\n",
      "\n",
      "Train data evaluation:\n",
      "959677726720.0\n"
     ]
    }
   ],
   "source": [
    "# compare the final model loss/evaluation values\n",
    "print(\"Test data evaluation:\")\n",
    "print(model.evaluate(X_test, y_test, verbose=0))\n",
    "print(\"\\nTrain data evaluation:\")\n",
    "print(model.evaluate(X_train, y_train, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test True Y</th>\n",
       "      <th>Model Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5880000</td>\n",
       "      <td>4952615.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8645000</td>\n",
       "      <td>5766962.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4585000</td>\n",
       "      <td>3844479.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4095000</td>\n",
       "      <td>4328981.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7000000</td>\n",
       "      <td>6432570.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3605000</td>\n",
       "      <td>3614698.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3234000</td>\n",
       "      <td>3618120.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4165000</td>\n",
       "      <td>4012890.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5229000</td>\n",
       "      <td>5839471.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4403000</td>\n",
       "      <td>4028929.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1890000</td>\n",
       "      <td>2927067.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6083000</td>\n",
       "      <td>5615922.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5600000</td>\n",
       "      <td>4980056.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7875000</td>\n",
       "      <td>5588459.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8400000</td>\n",
       "      <td>4452528.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3703000</td>\n",
       "      <td>3134540.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5950000</td>\n",
       "      <td>5403890.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3220000</td>\n",
       "      <td>3462374.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2450000</td>\n",
       "      <td>3186783.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3850000</td>\n",
       "      <td>4350133.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6930000</td>\n",
       "      <td>6405759.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3710000</td>\n",
       "      <td>2851850.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5110000</td>\n",
       "      <td>5128901.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5565000</td>\n",
       "      <td>6117925.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3150000</td>\n",
       "      <td>2773257.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3500000</td>\n",
       "      <td>4397484.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5075000</td>\n",
       "      <td>4949848.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4830000</td>\n",
       "      <td>4941192.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5110000</td>\n",
       "      <td>5469665.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4550000</td>\n",
       "      <td>3224562.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3430000</td>\n",
       "      <td>4539877.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4830000</td>\n",
       "      <td>3789609.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>6230000</td>\n",
       "      <td>5115317.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>5040000</td>\n",
       "      <td>5988624.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>6860000</td>\n",
       "      <td>4667191.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5110000</td>\n",
       "      <td>4881414.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4235000</td>\n",
       "      <td>4821382.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>5600000</td>\n",
       "      <td>5618044.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4375000</td>\n",
       "      <td>3368144.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>6615000</td>\n",
       "      <td>7132082.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3920000</td>\n",
       "      <td>6224715.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4270000</td>\n",
       "      <td>3516222.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>3836000</td>\n",
       "      <td>3257875.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>3773000</td>\n",
       "      <td>4613406.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2345000</td>\n",
       "      <td>2601552.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4473000</td>\n",
       "      <td>4351218.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2940000</td>\n",
       "      <td>4002259.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>5950000</td>\n",
       "      <td>5885720.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>6650000</td>\n",
       "      <td>6352863.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>4900000</td>\n",
       "      <td>3564187.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2310000</td>\n",
       "      <td>2509327.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>4270000</td>\n",
       "      <td>3975598.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>6790000</td>\n",
       "      <td>5912053.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2730000</td>\n",
       "      <td>3674377.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>4900000</td>\n",
       "      <td>4032100.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2485000</td>\n",
       "      <td>3468389.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1890000</td>\n",
       "      <td>2603356.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>5250000</td>\n",
       "      <td>6021400.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>4200000</td>\n",
       "      <td>4479999.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Test True Y  Model Predictions\n",
       "0       5880000         4952615.00\n",
       "1       8645000         5766962.00\n",
       "2       4585000         3844479.25\n",
       "3       4095000         4328981.00\n",
       "4       7000000         6432570.00\n",
       "5       3605000         3614698.50\n",
       "6       3234000         3618120.00\n",
       "7       4165000         4012890.50\n",
       "8       5229000         5839471.00\n",
       "9       4403000         4028929.50\n",
       "10      1890000         2927067.00\n",
       "11      6083000         5615922.00\n",
       "12      5600000         4980056.00\n",
       "13      7875000         5588459.00\n",
       "14      8400000         4452528.00\n",
       "15      3703000         3134540.50\n",
       "16      5950000         5403890.00\n",
       "17      3220000         3462374.00\n",
       "18      2450000         3186783.00\n",
       "19      3850000         4350133.50\n",
       "20      6930000         6405759.00\n",
       "21      3710000         2851850.50\n",
       "22      5110000         5128901.00\n",
       "23      5565000         6117925.00\n",
       "24      3150000         2773257.25\n",
       "25      3500000         4397484.00\n",
       "26      5075000         4949848.00\n",
       "27      4830000         4941192.00\n",
       "28      5110000         5469665.00\n",
       "29      4550000         3224562.00\n",
       "30      3430000         4539877.00\n",
       "31      4830000         3789609.75\n",
       "32      6230000         5115317.50\n",
       "33      5040000         5988624.00\n",
       "34      6860000         4667191.50\n",
       "35      5110000         4881414.50\n",
       "36      4235000         4821382.00\n",
       "37      5600000         5618044.00\n",
       "38      4375000         3368144.50\n",
       "39      6615000         7132082.00\n",
       "40      3920000         6224715.50\n",
       "41      4270000         3516222.00\n",
       "42      3836000         3257875.00\n",
       "43      3773000         4613406.00\n",
       "44      2345000         2601552.50\n",
       "45      4473000         4351218.00\n",
       "46      2940000         4002259.75\n",
       "47      5950000         5885720.00\n",
       "48      6650000         6352863.50\n",
       "49      4900000         3564187.00\n",
       "50      2310000         2509327.25\n",
       "51      4270000         3975598.00\n",
       "52      6790000         5912053.50\n",
       "53      2730000         3674377.00\n",
       "54      4900000         4032100.75\n",
       "55      2485000         3468389.00\n",
       "56      1890000         2603356.75\n",
       "57      5250000         6021400.00\n",
       "58      4200000         4479999.00"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get test predictions\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# reshape the data for easier comparison table\n",
    "test_predictions = pd.Series(test_predictions.reshape(len(y_test),))\n",
    "pred_df = pd.DataFrame(np.asarray(y_test), columns=['Test True Y'])\n",
    "pred_df = pd.concat([pred_df, test_predictions], axis=1)\n",
    "pred_df.columns = ['Test True Y', 'Model Predictions']\n",
    "\n",
    "# print the comparison table - true values vs. model predicted values\n",
    "# we can nicely see here how far off our model is in some cases\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Test True Y', ylabel='Model Predictions'>"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# these values follow a linear diagonal line = good predictions\n",
    "# we basically compare the predicted values \n",
    "# to true test values and see the differences\n",
    "sns.scatterplot(x='Test True Y', y='Model Predictions', data=pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE\n",
      "767626.31 $\n",
      "\n",
      "MSE\n",
      "1093779849216.0 $^2\n",
      "\n",
      "RMSE:\n",
      "1045839.3 $\n",
      "\n",
      "R-squared:\n",
      "0.54\n",
      "\n",
      "Explained variance score:\n",
      "0.55\n"
     ]
    }
   ],
   "source": [
    "# MAE - Mean average error\n",
    "print(\"MAE\")\n",
    "print(round(metrics.mean_absolute_error(y_test, test_predictions), 2), \"$\")\n",
    "\n",
    "# MSE - Mean square error\n",
    "print(\"\\nMSE\")\n",
    "print(round(metrics.mean_squared_error(y_test, test_predictions), 2), \"$^2\")\n",
    "\n",
    "# RMSE - Root mean square error\n",
    "print('\\nRMSE:')\n",
    "print(round(np.sqrt(metrics.mean_squared_error(y_test, test_predictions)), 2), \"$\")\n",
    "\n",
    "# R-squared. 0 = the model descibes the dataset poorly\n",
    "# 1 = model describes the dataset perfectly\n",
    "print('\\nR-squared:')\n",
    "print(round(metrics.r2_score(y_test, test_predictions), 2))\n",
    "\n",
    "# Explained Variance Score => 0 = the model descibes the dataset poorly\n",
    "# 1 = model describes the dataset perfectly\n",
    "# high variance score = model is a good fit for the data \n",
    "# low variance score = model is not a good fit for the data\n",
    "# the higher the score, the model is more able to explain the variation in the data\n",
    "# if score is low, we might need more and better data\n",
    "print(\"\\nExplained variance score:\")\n",
    "print(round(metrics.explained_variance_score(y_test, test_predictions), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tuomas.valtanen\\AppData\\Local\\Temp\\ipykernel_2648\\3124900743.py:5: UserWarning: \n",
      "\n",
      "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
      "\n",
      "Please adapt your code to use either `displot` (a figure-level function with\n",
      "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "\n",
      "For a guide to updating your code to use the new functions, please see\n",
      "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
      "\n",
      "  sns.distplot((y_test - test_predictions))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# if the prediction distribution are far from normal distribution\n",
    "# then the model is not probably good enough\n",
    "# distplot is deprecating in future pandas-version\n",
    "# unfortunately, there's no exact alternative to do this plot at the moment\n",
    "sns.distplot((y_test - test_predictions))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying the model in practice with new imaginary house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>stories</th>\n",
       "      <th>mainroad</th>\n",
       "      <th>guestroom</th>\n",
       "      <th>basement</th>\n",
       "      <th>hotwaterheating</th>\n",
       "      <th>airconditioning</th>\n",
       "      <th>parking</th>\n",
       "      <th>prefarea</th>\n",
       "      <th>furnished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9870000</td>\n",
       "      <td>8100</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9310000</td>\n",
       "      <td>6550</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9240000</td>\n",
       "      <td>7800</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      price  area  bedrooms  bathrooms  stories  mainroad  guestroom  \\\n",
       "8   9870000  8100         4          1        2         1          1   \n",
       "12  9310000  6550         4          2        2         1          0   \n",
       "14  9240000  7800         3          2        2         1          0   \n",
       "\n",
       "    basement  hotwaterheating  airconditioning  parking  prefarea  furnished  \n",
       "8          1                0                1        2         1          1  \n",
       "12         0                0                1        1         1          1  \n",
       "14         0                0                0        0         1          1  "
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just to see what kind of values are usually in the dataset\n",
    "# so we can test with the tester_row\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: fix the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try with some new imaginary data\n",
    "# this example uses the student performance index score dataset\n",
    "# modify this as needed regarding your own dataset\n",
    "tester_row = {\n",
    "    'area': 7420, \n",
    "    'bedrooms': 4, \n",
    "    'bathrooms': 2, \n",
    "    'stories': 3, \n",
    "    'mainroad': 1,\n",
    "    'guestroom': 0, \n",
    "    'basement': 0, \n",
    "    'hotwaterheating': 0, \n",
    "    'airconditioning': 1,\n",
    "    'parking': 2,\n",
    "    'prefarea': 1,\n",
    "    'furnishingstatus_furnished': 1,\n",
    "    'furnishingstatus_semi-furnished:': 0\n",
    "}\n",
    "\n",
    "# convert to pandas-format\n",
    "tester_row = pd.DataFrame([tester_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node sequential_7_1/batch_normalization_3_1/batchnorm/mul_1 defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n\n  File \"C:\\Users\\tuomas.valtanen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 645, in run_forever\n\n  File \"C:\\Users\\tuomas.valtanen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 1999, in _run_once\n\n  File \"C:\\Users\\tuomas.valtanen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\events.py\", line 88, in _run\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 519, in dispatch_queue\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 508, in process_one\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 368, in execute_request\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 455, in do_execute\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 577, in run_cell\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3116, in run_cell\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3171, in _run_cell\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3394, in run_cell_async\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3639, in run_ast_nodes\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3699, in run_code\n\n  File \"C:\\Users\\tuomas.valtanen\\AppData\\Local\\Temp\\ipykernel_2648\\4002590234.py\", line 2, in <module>\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 566, in predict\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 260, in one_step_on_data_distributed\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 250, in one_step_on_data\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 105, in predict_step\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\layers\\layer.py\", line 941, in __call__\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\ops\\operation.py\", line 59, in __call__\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\models\\sequential.py\", line 220, in call\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\models\\functional.py\", line 183, in call\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\ops\\function.py\", line 206, in _run_through_graph\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\models\\functional.py\", line 644, in call\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\layers\\layer.py\", line 941, in __call__\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\ops\\operation.py\", line 59, in __call__\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py\", line 277, in call\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\ops\\nn.py\", line 2267, in batch_normalization\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py\", line 879, in batch_normalization\n\nIncompatible shapes: [1,13] vs. [12]\n\t [[{{node sequential_7_1/batch_normalization_3_1/batchnorm/mul_1}}]] [Op:__inference_one_step_on_data_distributed_488854]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgumentError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[324]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# get the prediction from the model and print out the result\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m result = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtester_row\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEstimated house price with this example:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[32m     54\u001b[39m                                       inputs, attrs, num_outputs)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mInvalidArgumentError\u001b[39m: Graph execution error:\n\nDetected at node sequential_7_1/batch_normalization_3_1/batchnorm/mul_1 defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n\n  File \"C:\\Users\\tuomas.valtanen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 645, in run_forever\n\n  File \"C:\\Users\\tuomas.valtanen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 1999, in _run_once\n\n  File \"C:\\Users\\tuomas.valtanen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\events.py\", line 88, in _run\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 519, in dispatch_queue\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 508, in process_one\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 368, in execute_request\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 455, in do_execute\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 577, in run_cell\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3116, in run_cell\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3171, in _run_cell\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3394, in run_cell_async\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3639, in run_ast_nodes\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3699, in run_code\n\n  File \"C:\\Users\\tuomas.valtanen\\AppData\\Local\\Temp\\ipykernel_2648\\4002590234.py\", line 2, in <module>\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 566, in predict\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 260, in one_step_on_data_distributed\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 250, in one_step_on_data\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 105, in predict_step\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\layers\\layer.py\", line 941, in __call__\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\ops\\operation.py\", line 59, in __call__\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\models\\sequential.py\", line 220, in call\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\models\\functional.py\", line 183, in call\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\ops\\function.py\", line 206, in _run_through_graph\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\models\\functional.py\", line 644, in call\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\layers\\layer.py\", line 941, in __call__\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\ops\\operation.py\", line 59, in __call__\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py\", line 277, in call\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\ops\\nn.py\", line 2267, in batch_normalization\n\n  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py\", line 879, in batch_normalization\n\nIncompatible shapes: [1,13] vs. [12]\n\t [[{{node sequential_7_1/batch_normalization_3_1/batchnorm/mul_1}}]] [Op:__inference_one_step_on_data_distributed_488854]"
     ]
    }
   ],
   "source": [
    "# get the prediction from the model and print out the result\n",
    "result = model.predict(tester_row)[0]\n",
    "\n",
    "print()\n",
    "print(f\"Estimated house price with this example:\")\n",
    "print(f\"$ {round(float(result[0]), 2)}\")\n",
    "print(\"----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model above undershoots the estimation\n",
    "# should be 13.3 million => prediction is 6.45 million"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
