{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network, experimentation tool, version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# activation functions\n",
    "# ReLu is very simple, it filters out all negative values\n",
    "# this is a powerful activation function in reality\n",
    "def activation_ReLu(number):\n",
    "    if number > 0:\n",
    "        return number\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# we also need a derivated version of ReLu\n",
    "# otherwise same as original, but instead of original value, return 1 instead\n",
    "def activation_ReLu_partial_derivative(number):\n",
    "    if number > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lock down the randomness in order to get same results everytime\n",
    "# you can change or disable this if you want\n",
    "np.random.seed(123)\n",
    "\n",
    "def generate_train_data():\n",
    "    result = []\n",
    "\n",
    "    # create 100 numbers\n",
    "    for x in range(100):\n",
    "        n1 = np.random.randint(0, 5)\n",
    "        n2 = np.random.randint(3, 7)\n",
    "\n",
    "        # formula for the target variable: x1 ^^ 2 + x2 + (random integer between 0-5)\n",
    "        # the only point of this is to have some kind of logic in the data\n",
    "        n3 = n1 ** 2 + n2 + np.random.randint(0, 5)\n",
    "        n3 = int(n3)\n",
    "\n",
    "        result.append([n1, n2, n3])\n",
    "\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>27.900</td>\n",
       "      <td>16884.92400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1725.55230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>33.000</td>\n",
       "      <td>4449.46200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>22.705</td>\n",
       "      <td>21984.47061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>28.880</td>\n",
       "      <td>3866.85520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2767</th>\n",
       "      <td>47</td>\n",
       "      <td>45.320</td>\n",
       "      <td>8569.86180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2768</th>\n",
       "      <td>21</td>\n",
       "      <td>34.600</td>\n",
       "      <td>2020.17700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2769</th>\n",
       "      <td>19</td>\n",
       "      <td>26.030</td>\n",
       "      <td>16450.89470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770</th>\n",
       "      <td>23</td>\n",
       "      <td>18.715</td>\n",
       "      <td>21595.38229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2771</th>\n",
       "      <td>54</td>\n",
       "      <td>31.600</td>\n",
       "      <td>9850.43200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2772 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age     bmi      charges\n",
       "0      19  27.900  16884.92400\n",
       "1      18  33.770   1725.55230\n",
       "2      28  33.000   4449.46200\n",
       "3      33  22.705  21984.47061\n",
       "4      32  28.880   3866.85520\n",
       "...   ...     ...          ...\n",
       "2767   47  45.320   8569.86180\n",
       "2768   21  34.600   2020.17700\n",
       "2769   19  26.030  16450.89470\n",
       "2770   23  18.715  21595.38229\n",
       "2771   54  31.600   9850.43200\n",
       "\n",
       "[2772 rows x 3 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use generated training data from our helper function\n",
    "# data = generate_train_data()\n",
    "# df = pd.DataFrame(data, columns=[\"x1\", \"x2\", \"y\"])\n",
    "df = pd.read_csv(\"medical_insurance.csv\")\n",
    "df = df[[\"age\", \"bmi\", \"charges\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 9205055328680660.0\n",
      "Epoch: 2, loss: 148764216.23168606\n",
      "Epoch: 3, loss: 148764216.23134056\n",
      "Epoch: 4, loss: 148764216.23134056\n",
      "Epoch: 5, loss: 148764216.23134056\n",
      "Epoch: 6, loss: 148764216.23134056\n",
      "Epoch: 7, loss: 148764216.23134056\n",
      "Epoch: 8, loss: 148764216.23134056\n",
      "Epoch: 9, loss: 148764216.23134056\n",
      "Epoch: 10, loss: 148764216.23134056\n",
      "Epoch: 11, loss: 148764216.23134056\n",
      "Epoch: 12, loss: 148764216.23134056\n",
      "Epoch: 13, loss: 148764216.23134056\n",
      "Epoch: 14, loss: 148764216.23134056\n",
      "Epoch: 15, loss: 148764216.23134056\n",
      "Epoch: 16, loss: 148764216.23134056\n",
      "Epoch: 17, loss: 148764216.23134056\n",
      "Epoch: 18, loss: 148764216.23134056\n",
      "Epoch: 19, loss: 148764216.23134056\n",
      "Epoch: 20, loss: 148764216.23134056\n",
      "Epoch: 21, loss: 148764216.23134056\n",
      "Epoch: 22, loss: 148764216.23134056\n",
      "Epoch: 23, loss: 148764216.23134056\n",
      "Epoch: 24, loss: 148764216.23134056\n",
      "Epoch: 25, loss: 148764216.23134056\n",
      "Epoch: 26, loss: 148764216.23134056\n",
      "Epoch: 27, loss: 148764216.23134056\n",
      "Epoch: 28, loss: 148764216.23134056\n",
      "Epoch: 29, loss: 148764216.23134056\n",
      "Epoch: 30, loss: 148764216.23134056\n",
      "Epoch: 31, loss: 148764216.23134056\n",
      "Epoch: 32, loss: 148764216.23134056\n",
      "Epoch: 33, loss: 148764216.23134056\n",
      "Epoch: 34, loss: 148764216.23134056\n",
      "Epoch: 35, loss: 148764216.23134056\n",
      "Epoch: 36, loss: 148764216.23134056\n",
      "Epoch: 37, loss: 148764216.23134056\n",
      "Epoch: 38, loss: 148764216.23134056\n",
      "Epoch: 39, loss: 148764216.23134056\n",
      "Epoch: 40, loss: 148764216.23134056\n",
      "Epoch: 41, loss: 148764216.23134056\n",
      "Epoch: 42, loss: 148764216.23134056\n",
      "Epoch: 43, loss: 148764216.23134056\n",
      "Epoch: 44, loss: 148764216.23134056\n",
      "Epoch: 45, loss: 148764216.23134056\n",
      "Epoch: 46, loss: 148764216.23134056\n",
      "Epoch: 47, loss: 148764216.23134056\n",
      "Epoch: 48, loss: 148764216.23134056\n",
      "Epoch: 49, loss: 148764216.23134056\n",
      "Epoch: 50, loss: 148764216.23134056\n",
      "Epoch: 51, loss: 148764216.23134056\n",
      "Epoch: 52, loss: 148764216.23134056\n",
      "Epoch: 53, loss: 148764216.23134056\n",
      "Epoch: 54, loss: 148764216.23134056\n",
      "Epoch: 55, loss: 148764216.23134056\n",
      "Epoch: 56, loss: 148764216.23134056\n",
      "Epoch: 57, loss: 148764216.23134056\n",
      "Epoch: 58, loss: 148764216.23134056\n",
      "Epoch: 59, loss: 148764216.23134056\n",
      "Epoch: 60, loss: 148764216.23134056\n",
      "Epoch: 61, loss: 148764216.23134056\n",
      "Epoch: 62, loss: 148764216.23134056\n",
      "Epoch: 63, loss: 148764216.23134056\n",
      "Epoch: 64, loss: 148764216.23134056\n",
      "Epoch: 65, loss: 148764216.23134056\n",
      "Epoch: 66, loss: 148764216.23134056\n",
      "Epoch: 67, loss: 148764216.23134056\n",
      "Epoch: 68, loss: 148764216.23134056\n",
      "Epoch: 69, loss: 148764216.23134056\n",
      "Epoch: 70, loss: 148764216.23134056\n",
      "Epoch: 71, loss: 148764216.23134056\n",
      "Epoch: 72, loss: 148764216.23134056\n",
      "Epoch: 73, loss: 148764216.23134056\n",
      "Epoch: 74, loss: 148764216.23134056\n",
      "Epoch: 75, loss: 148764216.23134056\n",
      "Epoch: 76, loss: 148764216.23134056\n",
      "Epoch: 77, loss: 148764216.23134056\n",
      "Epoch: 78, loss: 148764216.23134056\n",
      "Epoch: 79, loss: 148764216.23134056\n",
      "Epoch: 80, loss: 148764216.23134056\n",
      "Epoch: 81, loss: 148764216.23134056\n",
      "Epoch: 82, loss: 148764216.23134056\n",
      "Epoch: 83, loss: 148764216.23134056\n",
      "Epoch: 84, loss: 148764216.23134056\n",
      "Epoch: 85, loss: 148764216.23134056\n",
      "Epoch: 86, loss: 148764216.23134056\n",
      "Epoch: 87, loss: 148764216.23134056\n",
      "Epoch: 88, loss: 148764216.23134056\n",
      "Epoch: 89, loss: 148764216.23134056\n",
      "Epoch: 90, loss: 148764216.23134056\n",
      "Epoch: 91, loss: 148764216.23134056\n",
      "Epoch: 92, loss: 148764216.23134056\n",
      "Epoch: 93, loss: 148764216.23134056\n",
      "Epoch: 94, loss: 148764216.23134056\n",
      "Epoch: 95, loss: 148764216.23134056\n",
      "Epoch: 96, loss: 148764216.23134056\n",
      "Epoch: 97, loss: 148764216.23134056\n",
      "Epoch: 98, loss: 148764216.23134056\n",
      "Epoch: 99, loss: 148764216.23134056\n",
      "Epoch: 100, loss: 148764216.23134056\n",
      "Epoch: 101, loss: 148764216.23134056\n",
      "Epoch: 102, loss: 148764216.23134056\n",
      "Epoch: 103, loss: 148764216.23134056\n",
      "Epoch: 104, loss: 148764216.23134056\n",
      "Epoch: 105, loss: 148764216.23134056\n",
      "Epoch: 106, loss: 148764216.23134056\n",
      "Epoch: 107, loss: 148764216.23134056\n",
      "Epoch: 108, loss: 148764216.23134056\n",
      "Epoch: 109, loss: 148764216.23134056\n",
      "Epoch: 110, loss: 148764216.23134056\n",
      "Epoch: 111, loss: 148764216.23134056\n",
      "Epoch: 112, loss: 148764216.23134056\n",
      "Epoch: 113, loss: 148764216.23134056\n",
      "Epoch: 114, loss: 148764216.23134056\n",
      "Epoch: 115, loss: 148764216.23134056\n",
      "Epoch: 116, loss: 148764216.23134056\n",
      "Epoch: 117, loss: 148764216.23134056\n",
      "Epoch: 118, loss: 148764216.23134056\n",
      "Epoch: 119, loss: 148764216.23134056\n",
      "Epoch: 120, loss: 148764216.23134056\n",
      "Epoch: 121, loss: 148764216.23134056\n",
      "Epoch: 122, loss: 148764216.23134056\n",
      "Epoch: 123, loss: 148764216.23134056\n",
      "Epoch: 124, loss: 148764216.23134056\n",
      "Epoch: 125, loss: 148764216.23134056\n",
      "Epoch: 126, loss: 148764216.23134056\n",
      "Epoch: 127, loss: 148764216.23134056\n",
      "Epoch: 128, loss: 148764216.23134056\n",
      "Epoch: 129, loss: 148764216.23134056\n",
      "Epoch: 130, loss: 148764216.23134056\n",
      "Epoch: 131, loss: 148764216.23134056\n",
      "Epoch: 132, loss: 148764216.23134056\n",
      "Epoch: 133, loss: 148764216.23134056\n",
      "Epoch: 134, loss: 148764216.23134056\n",
      "Epoch: 135, loss: 148764216.23134056\n",
      "Epoch: 136, loss: 148764216.23134056\n",
      "Epoch: 137, loss: 148764216.23134056\n",
      "Epoch: 138, loss: 148764216.23134056\n",
      "Epoch: 139, loss: 148764216.23134056\n",
      "Epoch: 140, loss: 148764216.23134056\n",
      "Epoch: 141, loss: 148764216.23134056\n",
      "Epoch: 142, loss: 148764216.23134056\n",
      "Epoch: 143, loss: 148764216.23134056\n",
      "Epoch: 144, loss: 148764216.23134056\n",
      "Epoch: 145, loss: 148764216.23134056\n",
      "Epoch: 146, loss: 148764216.23134056\n",
      "Epoch: 147, loss: 148764216.23134056\n",
      "Epoch: 148, loss: 148764216.23134056\n",
      "Epoch: 149, loss: 148764216.23134056\n",
      "Epoch: 150, loss: 148764216.23134056\n",
      "Epoch: 151, loss: 148764216.23134056\n",
      "Epoch: 152, loss: 148764216.23134056\n",
      "Epoch: 153, loss: 148764216.23134056\n",
      "Epoch: 154, loss: 148764216.23134056\n",
      "Epoch: 155, loss: 148764216.23134056\n",
      "Epoch: 156, loss: 148764216.23134056\n",
      "Epoch: 157, loss: 148764216.23134056\n",
      "Epoch: 158, loss: 148764216.23134056\n",
      "Epoch: 159, loss: 148764216.23134056\n",
      "Epoch: 160, loss: 148764216.23134056\n",
      "Epoch: 161, loss: 148764216.23134056\n",
      "Epoch: 162, loss: 148764216.23134056\n",
      "Epoch: 163, loss: 148764216.23134056\n",
      "Epoch: 164, loss: 148764216.23134056\n",
      "Epoch: 165, loss: 148764216.23134056\n",
      "Epoch: 166, loss: 148764216.23134056\n",
      "Epoch: 167, loss: 148764216.23134056\n",
      "Epoch: 168, loss: 148764216.23134056\n",
      "Epoch: 169, loss: 148764216.23134056\n",
      "Epoch: 170, loss: 148764216.23134056\n",
      "Epoch: 171, loss: 148764216.23134056\n",
      "Epoch: 172, loss: 148764216.23134056\n",
      "Epoch: 173, loss: 148764216.23134056\n",
      "Epoch: 174, loss: 148764216.23134056\n",
      "Epoch: 175, loss: 148764216.23134056\n",
      "Epoch: 176, loss: 148764216.23134056\n",
      "Epoch: 177, loss: 148764216.23134056\n",
      "Epoch: 178, loss: 148764216.23134056\n",
      "Epoch: 179, loss: 148764216.23134056\n",
      "Epoch: 180, loss: 148764216.23134056\n",
      "Epoch: 181, loss: 148764216.23134056\n",
      "Epoch: 182, loss: 148764216.23134056\n",
      "Epoch: 183, loss: 148764216.23134056\n",
      "Epoch: 184, loss: 148764216.23134056\n",
      "Epoch: 185, loss: 148764216.23134056\n",
      "Epoch: 186, loss: 148764216.23134056\n",
      "Epoch: 187, loss: 148764216.23134056\n",
      "Epoch: 188, loss: 148764216.23134056\n",
      "Epoch: 189, loss: 148764216.23134056\n",
      "Epoch: 190, loss: 148764216.23134056\n",
      "Epoch: 191, loss: 148764216.23134056\n",
      "Epoch: 192, loss: 148764216.23134056\n",
      "Epoch: 193, loss: 148764216.23134056\n",
      "Epoch: 194, loss: 148764216.23134056\n",
      "Epoch: 195, loss: 148764216.23134056\n",
      "Epoch: 196, loss: 148764216.23134056\n",
      "Epoch: 197, loss: 148764216.23134056\n",
      "Epoch: 198, loss: 148764216.23134056\n",
      "Epoch: 199, loss: 148764216.23134056\n",
      "Epoch: 200, loss: 148764216.23134056\n",
      "Epoch: 201, loss: 148764216.23134056\n",
      "Epoch: 202, loss: 148764216.23134056\n",
      "Epoch: 203, loss: 148764216.23134056\n",
      "Epoch: 204, loss: 148764216.23134056\n",
      "Epoch: 205, loss: 148764216.23134056\n",
      "Epoch: 206, loss: 148764216.23134056\n",
      "Epoch: 207, loss: 148764216.23134056\n",
      "Epoch: 208, loss: 148764216.23134056\n",
      "Epoch: 209, loss: 148764216.23134056\n",
      "Epoch: 210, loss: 148764216.23134056\n",
      "Epoch: 211, loss: 148764216.23134056\n",
      "Epoch: 212, loss: 148764216.23134056\n",
      "Epoch: 213, loss: 148764216.23134056\n",
      "Epoch: 214, loss: 148764216.23134056\n",
      "Epoch: 215, loss: 148764216.23134056\n",
      "Epoch: 216, loss: 148764216.23134056\n",
      "Epoch: 217, loss: 148764216.23134056\n",
      "Epoch: 218, loss: 148764216.23134056\n",
      "Epoch: 219, loss: 148764216.23134056\n",
      "Epoch: 220, loss: 148764216.23134056\n",
      "Epoch: 221, loss: 148764216.23134056\n",
      "Epoch: 222, loss: 148764216.23134056\n",
      "Epoch: 223, loss: 148764216.23134056\n",
      "Epoch: 224, loss: 148764216.23134056\n",
      "Epoch: 225, loss: 148764216.23134056\n",
      "Epoch: 226, loss: 148764216.23134056\n",
      "Epoch: 227, loss: 148764216.23134056\n",
      "Epoch: 228, loss: 148764216.23134056\n",
      "Epoch: 229, loss: 148764216.23134056\n",
      "Epoch: 230, loss: 148764216.23134056\n",
      "Epoch: 231, loss: 148764216.23134056\n",
      "Epoch: 232, loss: 148764216.23134056\n",
      "Epoch: 233, loss: 148764216.23134056\n",
      "Epoch: 234, loss: 148764216.23134056\n",
      "Epoch: 235, loss: 148764216.23134056\n",
      "Epoch: 236, loss: 148764216.23134056\n",
      "Epoch: 237, loss: 148764216.23134056\n",
      "Epoch: 238, loss: 148764216.23134056\n",
      "Epoch: 239, loss: 148764216.23134056\n",
      "Epoch: 240, loss: 148764216.23134056\n",
      "Epoch: 241, loss: 148764216.23134056\n",
      "Epoch: 242, loss: 148764216.23134056\n",
      "Epoch: 243, loss: 148764216.23134056\n",
      "Epoch: 244, loss: 148764216.23134056\n",
      "Epoch: 245, loss: 148764216.23134056\n",
      "Epoch: 246, loss: 148764216.23134056\n",
      "Epoch: 247, loss: 148764216.23134056\n",
      "Epoch: 248, loss: 148764216.23134056\n",
      "Epoch: 249, loss: 148764216.23134056\n",
      "Epoch: 250, loss: 148764216.23134056\n",
      "Epoch: 251, loss: 148764216.23134056\n",
      "Epoch: 252, loss: 148764216.23134056\n",
      "Epoch: 253, loss: 148764216.23134056\n",
      "Epoch: 254, loss: 148764216.23134056\n",
      "Epoch: 255, loss: 148764216.23134056\n",
      "Epoch: 256, loss: 148764216.23134056\n",
      "Epoch: 257, loss: 148764216.23134056\n",
      "Epoch: 258, loss: 148764216.23134056\n",
      "Epoch: 259, loss: 148764216.23134056\n",
      "Epoch: 260, loss: 148764216.23134056\n",
      "Epoch: 261, loss: 148764216.23134056\n",
      "Epoch: 262, loss: 148764216.23134056\n",
      "Epoch: 263, loss: 148764216.23134056\n",
      "Epoch: 264, loss: 148764216.23134056\n",
      "Epoch: 265, loss: 148764216.23134056\n",
      "Epoch: 266, loss: 148764216.23134056\n",
      "Epoch: 267, loss: 148764216.23134056\n",
      "Epoch: 268, loss: 148764216.23134056\n",
      "Epoch: 269, loss: 148764216.23134056\n",
      "Epoch: 270, loss: 148764216.23134056\n",
      "Epoch: 271, loss: 148764216.23134056\n",
      "Epoch: 272, loss: 148764216.23134056\n",
      "Epoch: 273, loss: 148764216.23134056\n",
      "Epoch: 274, loss: 148764216.23134056\n",
      "Epoch: 275, loss: 148764216.23134056\n",
      "Epoch: 276, loss: 148764216.23134056\n",
      "Epoch: 277, loss: 148764216.23134056\n",
      "Epoch: 278, loss: 148764216.23134056\n",
      "Epoch: 279, loss: 148764216.23134056\n",
      "Epoch: 280, loss: 148764216.23134056\n",
      "Epoch: 281, loss: 148764216.23134056\n",
      "Epoch: 282, loss: 148764216.23134056\n",
      "Epoch: 283, loss: 148764216.23134056\n",
      "Epoch: 284, loss: 148764216.23134056\n",
      "Epoch: 285, loss: 148764216.23134056\n",
      "Epoch: 286, loss: 148764216.23134056\n",
      "Epoch: 287, loss: 148764216.23134056\n",
      "Epoch: 288, loss: 148764216.23134056\n",
      "Epoch: 289, loss: 148764216.23134056\n",
      "Epoch: 290, loss: 148764216.23134056\n",
      "Epoch: 291, loss: 148764216.23134056\n",
      "Epoch: 292, loss: 148764216.23134056\n",
      "Epoch: 293, loss: 148764216.23134056\n",
      "Epoch: 294, loss: 148764216.23134056\n",
      "Epoch: 295, loss: 148764216.23134056\n",
      "Epoch: 296, loss: 148764216.23134056\n",
      "Epoch: 297, loss: 148764216.23134056\n",
      "Epoch: 298, loss: 148764216.23134056\n",
      "Epoch: 299, loss: 148764216.23134056\n",
      "Epoch: 300, loss: 148764216.23134056\n"
     ]
    }
   ],
   "source": [
    "# initialize weights and biases\n",
    "# in Keras/TensorFlow/PyTorch etc. these are usually randomized in the beginning\n",
    "w1 = 1\n",
    "w2 = 0.5\n",
    "w3 = 1\n",
    "w4 = -0.5\n",
    "w5 = 1\n",
    "w6 = 1\n",
    "bias1 = 0.5\n",
    "bias2 = 0\n",
    "bias3 = 0.5\n",
    "\n",
    "# just for comparison after the training\n",
    "original_w1 = w1\n",
    "original_w2 = w2\n",
    "original_w3 = w3\n",
    "original_w4 = w4\n",
    "original_w5 = w5\n",
    "original_w6 = w6\n",
    "original_b1 = bias1\n",
    "original_b2 = bias2\n",
    "original_b3 = bias3\n",
    "\n",
    "\n",
    "# DataFrame values as a list\n",
    "data = list(df.values)\n",
    "\n",
    "# use min/max -scaling to make values in the range 0-1\n",
    "# IDEA: try scaling the features and the target separately\n",
    "# does this help with the model, since currently\n",
    "# most of the predictions revolve around the average charges\n",
    "data = (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "# learning rate\n",
    "LR = 0.005\n",
    "epochs = 300\n",
    "\n",
    "# let's initalize a list for loss visualizations\n",
    "loss_points = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # the previous version only measured the loss value\n",
    "    # of the last calculation done in the code (node 3)\n",
    "    # it's probably better to measure the average loss for each epoch\n",
    "    epoch_losses = []\n",
    "\n",
    "    for row in data:\n",
    "        # this is where we do Forward pass + backpropagation\n",
    "        input1 = row[0]\n",
    "        input2 = row[1]\n",
    "        true_value = row[2]\n",
    "\n",
    "        # NODE 1 OUTPUT\n",
    "        node_1_output = input1 * w1 + input2 * w3 + bias1\n",
    "        node_1_output = activation_ReLu(node_1_output)\n",
    "        node_1_output\n",
    "\n",
    "        # NODE 2 OUTPUT\n",
    "        node_2_output = input1 * w2 + input2 * w4 + bias2\n",
    "        node_2_output = activation_ReLu(node_2_output)\n",
    "        node_2_output\n",
    "\n",
    "        # NODE 3 OUTPUT\n",
    "        # we can just use Node 1 and 2 outputs, since they\n",
    "        # already contain the previous weights in their result\n",
    "        node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "        node_3_output = activation_ReLu(node_3_output)\n",
    "        node_3_output\n",
    "\n",
    "        # LOSS FUNCTION - we are going to use MSE -> mean squared error\n",
    "        # MSE formula for LOSS => (predicted_value - true_value) ^ 2\n",
    "        predicted_value = node_3_output\n",
    "        loss = (predicted_value - true_value) ** 2\n",
    "\n",
    "        # add current loss into epoch losses -list\n",
    "        epoch_losses.append(loss)\n",
    "        \n",
    "        # BACKPROPAGATION - LAST LAYER FIRST\n",
    "        # solving the partial derivative of the loss function with respect to w5\n",
    "        deriv_L_w5 = 2 * node_1_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w5 = w5 - LR * deriv_L_w5\n",
    "\n",
    "        deriv_L_w6 = 2 * node_2_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w6 = w6 - LR * deriv_L_w6\n",
    "\n",
    "        deriv_L_b3 = 2 * 1 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_b3 = bias3 - LR * deriv_L_b3\n",
    "\n",
    "        # BACKPROPAGATION - THE FIRST LAYER\n",
    "        # FROM THIS POINT ONWARD WE HAVE TO USE THE MORE COMPLEX VERSION\n",
    "        # OF UPDATING THE VALUES => CHAIN RULE\n",
    "\n",
    "        # weight 1\n",
    "        deriv_L_w1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input1\n",
    "        deriv_L_w1 = deriv_L_w1_left * deriv_L_w1_right\n",
    "        new_w1 = w1 - LR * deriv_L_w1\n",
    "\n",
    "        deriv_L_w2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w2_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input1\n",
    "        deriv_L_w2 = deriv_L_w2_left * deriv_L_w2_right\n",
    "        new_w2 = w2 - LR * deriv_L_w2\n",
    "\n",
    "        deriv_L_w3_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w3_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input2\n",
    "        deriv_L_w3 = deriv_L_w3_left * deriv_L_w3_right\n",
    "        new_w3 = w3 - LR * deriv_L_w3\n",
    "\n",
    "        deriv_L_w4_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w4_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input2\n",
    "        deriv_L_w4 = deriv_L_w4_left * deriv_L_w4_right\n",
    "        new_w4 = w4 - LR * deriv_L_w4\n",
    "\n",
    "        deriv_L_b1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * 1\n",
    "        deriv_L_b1 = deriv_L_b1_left * deriv_L_b1_right\n",
    "        new_b1 = bias1 - LR * deriv_L_b1\n",
    "\n",
    "        deriv_L_b2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b2_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * 1\n",
    "        deriv_L_b2 = deriv_L_b2_left * deriv_L_b2_right\n",
    "        new_b2 = bias2 - LR * deriv_L_b2\n",
    "\n",
    "        # ALL DONE! FINALLY UPDATE THE EXISTING WEIGHTS\n",
    "        w1 = new_w1\n",
    "        w2 = new_w2\n",
    "        w3 = new_w3\n",
    "        w4 = new_w4\n",
    "        w5 = new_w5\n",
    "        w6 = new_w6\n",
    "        bias1 = new_b1\n",
    "        bias2 = new_b2\n",
    "        bias3 = new_b3\n",
    "\n",
    "    # calculate average epoch-wise loss and add it to loss points\n",
    "    average_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "\n",
    "    # place the overall epoch loss into the loss_points list\n",
    "    loss_points.append(average_loss)\n",
    "    print(f\"Epoch: {epoch + 1}, loss: {average_loss}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_points)\n",
    "# plt.ylim(-1, 5)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL WEIGHTS AND BIASES\n",
      "w1: 1\n",
      "w2: 0.5\n",
      "w3: 1\n",
      "w4: -0.5\n",
      "w5: 1\n",
      "w6: 1\n",
      "b1: 0.5\n",
      "b2: 0\n",
      "b3: 0.5\n",
      "\n",
      "\n",
      "#################################\n",
      "\n",
      "\n",
      "NEW WEIGHTS AND BIASES\n",
      "w1: -19918723076977.11\n",
      "w2: -1825671632640.4531\n",
      "w3: -17976647578523.234\n",
      "w4: -1647668650010.5334\n",
      "w5: -18968641620393.29\n",
      "w6: -18966647402928.86\n",
      "b1: -622460096161.9165\n",
      "b2: -57052238526.880714\n",
      "b3: 13275.382589240497\n"
     ]
    }
   ],
   "source": [
    "print(\"ORIGINAL WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {original_w1}\")\n",
    "print(f\"w2: {original_w2}\")\n",
    "print(f\"w3: {original_w3}\")\n",
    "print(f\"w4: {original_w4}\")\n",
    "print(f\"w5: {original_w5}\")\n",
    "print(f\"w6: {original_w6}\")\n",
    "print(f\"b1: {original_b1}\")\n",
    "print(f\"b2: {original_b2}\")\n",
    "print(f\"b3: {original_b3}\")\n",
    "\n",
    "print(\"\\n\\n#################################\\n\\n\")\n",
    "\n",
    "print(\"NEW WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {new_w1}\")\n",
    "print(f\"w2: {new_w2}\")\n",
    "print(f\"w3: {new_w3}\")\n",
    "print(f\"w4: {new_w4}\")\n",
    "print(f\"w5: {new_w5}\")\n",
    "print(f\"w6: {new_w6}\")\n",
    "print(f\"b1: {new_b1}\")\n",
    "print(f\"b2: {new_b2}\")\n",
    "print(f\"b3: {new_b3}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction function, just doing the forward pass\n",
    "# again (but only that)\n",
    "def predict(x1, x2):\n",
    "    input1 = x1\n",
    "    input2 = x2\n",
    "\n",
    "    # NODE 1 OUTPUT\n",
    "    node_1_output = input1 * w1 + input2 * w3 + bias1\n",
    "    node_1_output = activation_ReLu(node_1_output)\n",
    "\n",
    "    # NODE 2 OUTPUT\n",
    "    node_2_output = input1 * w2 + input2 * w4 + bias2\n",
    "    node_2_output = activation_ReLu(node_2_output)\n",
    "\n",
    "    # NODE 3 OUTPUT\n",
    "    # we can just use Node 1 and 2 outputs, since they\n",
    "    # already contain the previous weights in their result\n",
    "    node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "    node_3_output = activation_ReLu(node_3_output)\n",
    "    return node_3_output\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>27.900</td>\n",
       "      <td>16884.92400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1725.55230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>33.000</td>\n",
       "      <td>4449.46200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>22.705</td>\n",
       "      <td>21984.47061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>28.880</td>\n",
       "      <td>3866.85520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2767</th>\n",
       "      <td>47</td>\n",
       "      <td>45.320</td>\n",
       "      <td>8569.86180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2768</th>\n",
       "      <td>21</td>\n",
       "      <td>34.600</td>\n",
       "      <td>2020.17700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2769</th>\n",
       "      <td>19</td>\n",
       "      <td>26.030</td>\n",
       "      <td>16450.89470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770</th>\n",
       "      <td>23</td>\n",
       "      <td>18.715</td>\n",
       "      <td>21595.38229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2771</th>\n",
       "      <td>54</td>\n",
       "      <td>31.600</td>\n",
       "      <td>9850.43200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2772 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age     bmi      charges\n",
       "0      19  27.900  16884.92400\n",
       "1      18  33.770   1725.55230\n",
       "2      28  33.000   4449.46200\n",
       "3      33  22.705  21984.47061\n",
       "4      32  28.880   3866.85520\n",
       "...   ...     ...          ...\n",
       "2767   47  45.320   8569.86180\n",
       "2768   21  34.600   2020.17700\n",
       "2769   19  26.030  16450.89470\n",
       "2770   23  18.715  21595.38229\n",
       "2771   54  31.600   9850.43200\n",
       "\n",
       "[2772 rows x 3 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age           54.00000\n",
       "bmi           47.41000\n",
       "charges    63770.42801\n",
       "Name: 543, dtype: float64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[543]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.set_printoptions(precision=12, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   54.     ,    47.41   , 63770.42801])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scaled values\n",
    "data[543]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(36190.79656997774)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['charges'].max() * 0.567516914961"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(13275.382589240497)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try using the model with our prediction function\n",
    "# the value tends to be same as final bias 3\n",
    "# so if node1 and node2 outputs are small => more or less bias3\n",
    "result = predict(0.000596664064, 0.00049329876)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(846576829.7123685)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['charges'].max() * result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2772.000000</td>\n",
       "      <td>2772.000000</td>\n",
       "      <td>2772.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>39.109668</td>\n",
       "      <td>30.701349</td>\n",
       "      <td>13261.369959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.081459</td>\n",
       "      <td>6.129449</td>\n",
       "      <td>12151.768945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>15.960000</td>\n",
       "      <td>1121.873900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>26.000000</td>\n",
       "      <td>26.220000</td>\n",
       "      <td>4687.797000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>30.447500</td>\n",
       "      <td>9333.014350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>51.000000</td>\n",
       "      <td>34.770000</td>\n",
       "      <td>16577.779500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>64.000000</td>\n",
       "      <td>53.130000</td>\n",
       "      <td>63770.428010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               age          bmi       charges\n",
       "count  2772.000000  2772.000000   2772.000000\n",
       "mean     39.109668    30.701349  13261.369959\n",
       "std      14.081459     6.129449  12151.768945\n",
       "min      18.000000    15.960000   1121.873900\n",
       "25%      26.000000    26.220000   4687.797000\n",
       "50%      39.000000    30.447500   9333.014350\n",
       "75%      51.000000    34.770000  16577.779500\n",
       "max      64.000000    53.130000  63770.428010"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
