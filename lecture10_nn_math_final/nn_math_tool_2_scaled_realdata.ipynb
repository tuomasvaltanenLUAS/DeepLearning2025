{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network, experimentation tool, version 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDIT: the scaling is now done separately for independent support variables and the target. The model works a bit better now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# activation functions\n",
    "# ReLu is very simple, it filters out all negative values\n",
    "# this is a powerful activation function in reality\n",
    "def activation_ReLu(number):\n",
    "    if number > 0:\n",
    "        return number\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# we also need a derivated version of ReLu\n",
    "# otherwise same as original, but instead of original value, return 1 instead\n",
    "def activation_ReLu_partial_derivative(number):\n",
    "    if number > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lock down the randomness in order to get same results everytime\n",
    "# you can change or disable this if you want\n",
    "np.random.seed(123)\n",
    "\n",
    "def generate_train_data():\n",
    "    result = []\n",
    "\n",
    "    # create 100 numbers\n",
    "    for x in range(100):\n",
    "        n1 = np.random.randint(0, 5)\n",
    "        n2 = np.random.randint(3, 7)\n",
    "\n",
    "        # formula for the target variable: x1 ^^ 2 + x2 + (random integer between 0-5)\n",
    "        # the only point of this is to have some kind of logic in the data\n",
    "        n3 = n1 ** 2 + n2 + np.random.randint(0, 5)\n",
    "        n3 = int(n3)\n",
    "\n",
    "        result.append([n1, n2, n3])\n",
    "\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>27.900</td>\n",
       "      <td>16884.92400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1725.55230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>33.000</td>\n",
       "      <td>4449.46200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>22.705</td>\n",
       "      <td>21984.47061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>28.880</td>\n",
       "      <td>3866.85520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2767</th>\n",
       "      <td>47</td>\n",
       "      <td>45.320</td>\n",
       "      <td>8569.86180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2768</th>\n",
       "      <td>21</td>\n",
       "      <td>34.600</td>\n",
       "      <td>2020.17700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2769</th>\n",
       "      <td>19</td>\n",
       "      <td>26.030</td>\n",
       "      <td>16450.89470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770</th>\n",
       "      <td>23</td>\n",
       "      <td>18.715</td>\n",
       "      <td>21595.38229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2771</th>\n",
       "      <td>54</td>\n",
       "      <td>31.600</td>\n",
       "      <td>9850.43200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2772 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age     bmi      charges\n",
       "0      19  27.900  16884.92400\n",
       "1      18  33.770   1725.55230\n",
       "2      28  33.000   4449.46200\n",
       "3      33  22.705  21984.47061\n",
       "4      32  28.880   3866.85520\n",
       "...   ...     ...          ...\n",
       "2767   47  45.320   8569.86180\n",
       "2768   21  34.600   2020.17700\n",
       "2769   19  26.030  16450.89470\n",
       "2770   23  18.715  21595.38229\n",
       "2771   54  31.600   9850.43200\n",
       "\n",
       "[2772 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use generated training data from our helper function\n",
    "# data = generate_train_data()\n",
    "# df = pd.DataFrame(data, columns=[\"x1\", \"x2\", \"y\"])\n",
    "df = pd.read_csv(\"medical_insurance.csv\")\n",
    "df = df[[\"age\", \"bmi\", \"charges\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 0.05027569715117\n",
      "Epoch: 2, loss: 0.03359019710766125\n",
      "Epoch: 3, loss: 0.033568948716146234\n",
      "Epoch: 4, loss: 0.033565053677841857\n",
      "Epoch: 5, loss: 0.03356279467925406\n",
      "Epoch: 6, loss: 0.03355813340490166\n",
      "Epoch: 7, loss: 0.03355497301457527\n",
      "Epoch: 8, loss: 0.033550098691783846\n",
      "Epoch: 9, loss: 0.03354692811246641\n",
      "Epoch: 10, loss: 0.0335449353817778\n",
      "Epoch: 11, loss: 0.03354306104595482\n",
      "Epoch: 12, loss: 0.033541245953323855\n",
      "Epoch: 13, loss: 0.03353950522687489\n",
      "Epoch: 14, loss: 0.03353787270434304\n",
      "Epoch: 15, loss: 0.03353627186314619\n",
      "Epoch: 16, loss: 0.03353470038073381\n",
      "Epoch: 17, loss: 0.03353315710469112\n",
      "Epoch: 18, loss: 0.03353164098996176\n",
      "Epoch: 19, loss: 0.033530151072460726\n",
      "Epoch: 20, loss: 0.03352868645785967\n",
      "Epoch: 21, loss: 0.03352724631444429\n",
      "Epoch: 22, loss: 0.033525829867343794\n",
      "Epoch: 23, loss: 0.03352443639344974\n",
      "Epoch: 24, loss: 0.03352306521683449\n",
      "Epoch: 25, loss: 0.03352171570459257\n",
      "Epoch: 26, loss: 0.03352038726306435\n",
      "Epoch: 27, loss: 0.033519079334405705\n",
      "Epoch: 28, loss: 0.033517804867832526\n",
      "Epoch: 29, loss: 0.033516570712285323\n",
      "Epoch: 30, loss: 0.03351535308370682\n",
      "Epoch: 31, loss: 0.033514151646097355\n",
      "Epoch: 32, loss: 0.033512966144301215\n",
      "Epoch: 33, loss: 0.03351179634449825\n",
      "Epoch: 34, loss: 0.03351064202219442\n",
      "Epoch: 35, loss: 0.033509502959068224\n",
      "Epoch: 36, loss: 0.03350842513472468\n",
      "Epoch: 37, loss: 0.03350737424913123\n",
      "Epoch: 38, loss: 0.033506337334355145\n",
      "Epoch: 39, loss: 0.03350531574090308\n",
      "Epoch: 40, loss: 0.0335043096159463\n",
      "Epoch: 41, loss: 0.033503318826946404\n",
      "Epoch: 42, loss: 0.03350234317831929\n",
      "Epoch: 43, loss: 0.03350138246208319\n",
      "Epoch: 44, loss: 0.03350049644488993\n",
      "Epoch: 45, loss: 0.03349919729956523\n",
      "Epoch: 46, loss: 0.03349792647562776\n",
      "Epoch: 47, loss: 0.03349667790943928\n",
      "Epoch: 48, loss: 0.03349544997815884\n",
      "Epoch: 49, loss: 0.03349427918075875\n",
      "Epoch: 50, loss: 0.03349315919478659\n",
      "Epoch: 51, loss: 0.033492115022535365\n",
      "Epoch: 52, loss: 0.03349111611356947\n",
      "Epoch: 53, loss: 0.033490137667968836\n",
      "Epoch: 54, loss: 0.0334891938040164\n",
      "Epoch: 55, loss: 0.033488295016810034\n",
      "Epoch: 56, loss: 0.033487409580691796\n",
      "Epoch: 57, loss: 0.03348654422706547\n",
      "Epoch: 58, loss: 0.03348585081396099\n",
      "Epoch: 59, loss: 0.03348520603160876\n",
      "Epoch: 60, loss: 0.033484571154237516\n",
      "Epoch: 61, loss: 0.033483951850891897\n",
      "Epoch: 62, loss: 0.033483349165299735\n",
      "Epoch: 63, loss: 0.033482763016983955\n",
      "Epoch: 64, loss: 0.033482193072766384\n",
      "Epoch: 65, loss: 0.03348163894742522\n",
      "Epoch: 66, loss: 0.03348110024978876\n",
      "Epoch: 67, loss: 0.03348059236850032\n",
      "Epoch: 68, loss: 0.03348018161462032\n",
      "Epoch: 69, loss: 0.033479821438137033\n",
      "Epoch: 70, loss: 0.03347952433115828\n",
      "Epoch: 71, loss: 0.0334792408958611\n",
      "Epoch: 72, loss: 0.033478999692206315\n",
      "Epoch: 73, loss: 0.03347880299407202\n",
      "Epoch: 74, loss: 0.03347860850284811\n",
      "Epoch: 75, loss: 0.033478418420665304\n",
      "Epoch: 76, loss: 0.03347823311832633\n",
      "Epoch: 77, loss: 0.03347805245275873\n",
      "Epoch: 78, loss: 0.033477907711003486\n",
      "Epoch: 79, loss: 0.03347776349319179\n",
      "Epoch: 80, loss: 0.03347764091817885\n",
      "Epoch: 81, loss: 0.03347754040597938\n",
      "Epoch: 82, loss: 0.0334774242378966\n",
      "Epoch: 83, loss: 0.03347730774982477\n",
      "Epoch: 84, loss: 0.033477192114008174\n",
      "Epoch: 85, loss: 0.03347707757403638\n",
      "Epoch: 86, loss: 0.03347696415744052\n",
      "Epoch: 87, loss: 0.03347685184250625\n",
      "Epoch: 88, loss: 0.03347674059729975\n",
      "Epoch: 89, loss: 0.033476630388756465\n",
      "Epoch: 90, loss: 0.03347652118475129\n",
      "Epoch: 91, loss: 0.03347641295452117\n",
      "Epoch: 92, loss: 0.03347626338153334\n",
      "Epoch: 93, loss: 0.03347615832635235\n",
      "Epoch: 94, loss: 0.03347605339437033\n",
      "Epoch: 95, loss: 0.033475948978973345\n",
      "Epoch: 96, loss: 0.03347584516308087\n",
      "Epoch: 97, loss: 0.033475741956006985\n",
      "Epoch: 98, loss: 0.033475639350006754\n",
      "Epoch: 99, loss: 0.03347553659560394\n",
      "Epoch: 100, loss: 0.03347543588986916\n",
      "Epoch: 101, loss: 0.03347538020340905\n",
      "Epoch: 102, loss: 0.03347523521427077\n",
      "Epoch: 103, loss: 0.03347518004728361\n",
      "Epoch: 104, loss: 0.03347503712651521\n",
      "Epoch: 105, loss: 0.033474983472330376\n",
      "Epoch: 106, loss: 0.03347484155451972\n",
      "Epoch: 107, loss: 0.033474789454316936\n",
      "Epoch: 108, loss: 0.033474647324031916\n",
      "Epoch: 109, loss: 0.033474598601890664\n",
      "Epoch: 110, loss: 0.0334745025550224\n",
      "Epoch: 111, loss: 0.033474407994928825\n",
      "Epoch: 112, loss: 0.03347431426111947\n",
      "Epoch: 113, loss: 0.03347422118115096\n",
      "Epoch: 114, loss: 0.03347412870325864\n",
      "Epoch: 115, loss: 0.033474036805882686\n",
      "Epoch: 116, loss: 0.033473945475322474\n",
      "Epoch: 117, loss: 0.03347385470019472\n",
      "Epoch: 118, loss: 0.03347376447004421\n",
      "Epoch: 119, loss: 0.0334736747749844\n",
      "Epoch: 120, loss: 0.03347358560559188\n",
      "Epoch: 121, loss: 0.03347349695286703\n",
      "Epoch: 122, loss: 0.033473408808209766\n",
      "Epoch: 123, loss: 0.03347332116340028\n",
      "Epoch: 124, loss: 0.03347323401058227\n",
      "Epoch: 125, loss: 0.03347314734224611\n",
      "Epoch: 126, loss: 0.033473061151213186\n",
      "Epoch: 127, loss: 0.03347297543062112\n",
      "Epoch: 128, loss: 0.033472890173909225\n",
      "Epoch: 129, loss: 0.03347280537480485\n",
      "Epoch: 130, loss: 0.03347272102731046\n",
      "Epoch: 131, loss: 0.033472637125690353\n",
      "Epoch: 132, loss: 0.03347255366445934\n",
      "Epoch: 133, loss: 0.03347247063837057\n",
      "Epoch: 134, loss: 0.03347238804240533\n",
      "Epoch: 135, loss: 0.03347228722247977\n",
      "Epoch: 136, loss: 0.03347222278438625\n",
      "Epoch: 137, loss: 0.033472123665192935\n",
      "Epoch: 138, loss: 0.03347206374090839\n",
      "Epoch: 139, loss: 0.03347196532385767\n",
      "Epoch: 140, loss: 0.03347188716406194\n",
      "Epoch: 141, loss: 0.03347180711655399\n",
      "Epoch: 142, loss: 0.03347174864684709\n",
      "Epoch: 143, loss: 0.0334716533481289\n",
      "Epoch: 144, loss: 0.03347157789431706\n",
      "Epoch: 145, loss: 0.03347150019473626\n",
      "Epoch: 146, loss: 0.033471424309211885\n",
      "Epoch: 147, loss: 0.03347136860908769\n",
      "Epoch: 148, loss: 0.03347127609193977\n",
      "Epoch: 149, loss: 0.03347120141390686\n",
      "Epoch: 150, loss: 0.0334711278283318\n",
      "Epoch: 151, loss: 0.03347105490908184\n",
      "Epoch: 152, loss: 0.0334709825363468\n",
      "Epoch: 153, loss: 0.03347091067111244\n",
      "Epoch: 154, loss: 0.033470839295947734\n",
      "Epoch: 155, loss: 0.033470768399419745\n",
      "Epoch: 156, loss: 0.03347069797197489\n",
      "Epoch: 157, loss: 0.033470609295507\n",
      "Epoch: 158, loss: 0.03347055800640031\n",
      "Epoch: 159, loss: 0.03347048978271462\n",
      "Epoch: 160, loss: 0.03347041952237834\n",
      "Epoch: 161, loss: 0.03347035295103173\n",
      "Epoch: 162, loss: 0.03347026718081282\n",
      "Epoch: 163, loss: 0.0334702183641924\n",
      "Epoch: 164, loss: 0.03347015044635081\n",
      "Epoch: 165, loss: 0.03347006711459215\n",
      "Epoch: 166, loss: 0.033470020276313885\n",
      "Epoch: 167, loss: 0.03346995370285623\n",
      "Epoch: 168, loss: 0.03346987208183865\n",
      "Epoch: 169, loss: 0.03346982427903597\n",
      "Epoch: 170, loss: 0.03346974385329487\n",
      "Epoch: 171, loss: 0.03346969934580748\n",
      "Epoch: 172, loss: 0.033469616303888186\n",
      "Epoch: 173, loss: 0.033469573812170236\n",
      "Epoch: 174, loss: 0.03346949204898419\n",
      "Epoch: 175, loss: 0.0334694483432823\n",
      "Epoch: 176, loss: 0.033469371257142694\n",
      "Epoch: 177, loss: 0.033469308030562285\n",
      "Epoch: 178, loss: 0.03346926869442329\n",
      "Epoch: 179, loss: 0.03346918928176332\n",
      "Epoch: 180, loss: 0.03346912861300162\n",
      "Epoch: 181, loss: 0.03346909075199035\n",
      "Epoch: 182, loss: 0.033469012855279326\n",
      "Epoch: 183, loss: 0.03346895355753247\n",
      "Epoch: 184, loss: 0.03346889562303233\n",
      "Epoch: 185, loss: 0.0334688576078584\n",
      "Epoch: 186, loss: 0.033468785021006385\n",
      "Epoch: 187, loss: 0.03346872639664021\n",
      "Epoch: 188, loss: 0.033468669862184865\n",
      "Epoch: 189, loss: 0.033468614220768994\n",
      "Epoch: 190, loss: 0.03346855913468962\n",
      "Epoch: 191, loss: 0.033468504503829676\n",
      "Epoch: 192, loss: 0.03346845029411671\n",
      "Epoch: 193, loss: 0.03346839649005807\n",
      "Epoch: 194, loss: 0.03346834308157584\n",
      "Epoch: 195, loss: 0.033468290060344304\n",
      "Epoch: 196, loss: 0.033468237418760545\n",
      "Epoch: 197, loss: 0.03346818514964781\n",
      "Epoch: 198, loss: 0.033468133246162456\n",
      "Epoch: 199, loss: 0.033468081701758905\n",
      "Epoch: 200, loss: 0.033468030510169264\n",
      "Epoch: 201, loss: 0.033467979665390515\n",
      "Epoch: 202, loss: 0.03346792916167062\n",
      "Epoch: 203, loss: 0.0334678789934985\n",
      "Epoch: 204, loss: 0.03346782915559108\n",
      "Epoch: 205, loss: 0.033467779642884485\n",
      "Epoch: 206, loss: 0.03346773045052356\n",
      "Epoch: 207, loss: 0.03346768157385104\n",
      "Epoch: 208, loss: 0.03346763300840077\n",
      "Epoch: 209, loss: 0.03346758474988658\n",
      "Epoch: 210, loss: 0.0334675367941955\n",
      "Epoch: 211, loss: 0.03346748913737867\n",
      "Epoch: 212, loss: 0.03346744177564458\n",
      "Epoch: 213, loss: 0.03346739470535103\n",
      "Epoch: 214, loss: 0.03346734792299849\n",
      "Epoch: 215, loss: 0.033467301425223334\n",
      "Epoch: 216, loss: 0.03346725520879125\n",
      "Epoch: 217, loss: 0.03346720927059141\n",
      "Epoch: 218, loss: 0.033467163607630475\n",
      "Epoch: 219, loss: 0.03346711821702684\n",
      "Epoch: 220, loss: 0.033467073096005634\n",
      "Epoch: 221, loss: 0.03346702824189357\n",
      "Epoch: 222, loss: 0.03346698365211345\n",
      "Epoch: 223, loss: 0.03346693932418065\n",
      "Epoch: 224, loss: 0.03346689525569775\n",
      "Epoch: 225, loss: 0.03346685144435083\n",
      "Epoch: 226, loss: 0.033466807887904626\n",
      "Epoch: 227, loss: 0.033466764584200094\n",
      "Epoch: 228, loss: 0.033466721531149234\n",
      "Epoch: 229, loss: 0.033466678726732604\n",
      "Epoch: 230, loss: 0.03346663616899525\n",
      "Epoch: 231, loss: 0.03346659385604437\n",
      "Epoch: 232, loss: 0.03346655178604553\n",
      "Epoch: 233, loss: 0.03346650995721995\n",
      "Epoch: 234, loss: 0.03346646836784183\n",
      "Epoch: 235, loss: 0.03346642701623582\n",
      "Epoch: 236, loss: 0.03346638590077423\n",
      "Epoch: 237, loss: 0.03346634501987493\n",
      "Epoch: 238, loss: 0.033466304371998795\n",
      "Epoch: 239, loss: 0.03346626395564765\n",
      "Epoch: 240, loss: 0.033466223769362514\n",
      "Epoch: 241, loss: 0.033466183811720965\n",
      "Epoch: 242, loss: 0.033466144081336045\n",
      "Epoch: 243, loss: 0.0334661045768537\n",
      "Epoch: 244, loss: 0.03346606529695173\n",
      "Epoch: 245, loss: 0.033466026240337975\n",
      "Epoch: 246, loss: 0.03346598740574844\n",
      "Epoch: 247, loss: 0.03346594879194623\n",
      "Epoch: 248, loss: 0.03346591039772012\n",
      "Epoch: 249, loss: 0.03346587222188348\n",
      "Epoch: 250, loss: 0.03346584422287163\n",
      "Epoch: 251, loss: 0.03346580768302479\n",
      "Epoch: 252, loss: 0.03346576991400458\n",
      "Epoch: 253, loss: 0.03346572309759533\n",
      "Epoch: 254, loss: 0.033465698475403635\n",
      "Epoch: 255, loss: 0.03346566233711639\n",
      "Epoch: 256, loss: 0.03346562690898537\n",
      "Epoch: 257, loss: 0.03346559325451643\n",
      "Epoch: 258, loss: 0.033465557504548146\n",
      "Epoch: 259, loss: 0.03346552276201929\n",
      "Epoch: 260, loss: 0.03346548850712063\n",
      "Epoch: 261, loss: 0.033465454581414866\n",
      "Epoch: 262, loss: 0.03346542093440386\n",
      "Epoch: 263, loss: 0.033465387547939617\n",
      "Epoch: 264, loss: 0.033465354413659894\n",
      "Epoch: 265, loss: 0.033465321526240564\n",
      "Epoch: 266, loss: 0.033465288881374365\n",
      "Epoch: 267, loss: 0.03346525647516021\n",
      "Epoch: 268, loss: 0.033465222914891894\n",
      "Epoch: 269, loss: 0.03346519201397708\n",
      "Epoch: 270, loss: 0.03346516062161331\n",
      "Epoch: 271, loss: 0.03346514043920255\n",
      "Epoch: 272, loss: 0.03346509766131103\n",
      "Epoch: 273, loss: 0.03346506722860011\n",
      "Epoch: 274, loss: 0.03346503520766513\n",
      "Epoch: 275, loss: 0.03346501696586436\n",
      "Epoch: 276, loss: 0.03346497546105549\n",
      "Epoch: 277, loss: 0.033464944586204096\n",
      "Epoch: 278, loss: 0.03346492712494003\n",
      "Epoch: 279, loss: 0.033464886417800536\n",
      "Epoch: 280, loss: 0.0334648561775356\n",
      "Epoch: 281, loss: 0.03346483962267172\n",
      "Epoch: 282, loss: 0.033464799330830394\n",
      "Epoch: 283, loss: 0.03346478098078721\n",
      "Epoch: 284, loss: 0.033464742242115514\n",
      "Epoch: 285, loss: 0.033464713243232296\n",
      "Epoch: 286, loss: 0.03346469829412207\n",
      "Epoch: 287, loss: 0.03346465716157554\n",
      "Epoch: 288, loss: 0.03346464308687912\n",
      "Epoch: 289, loss: 0.03346461381626689\n",
      "Epoch: 290, loss: 0.03346457679159839\n",
      "Epoch: 291, loss: 0.03346456075794998\n",
      "Epoch: 292, loss: 0.033464523444452395\n",
      "Epoch: 293, loss: 0.03346450804983623\n",
      "Epoch: 294, loss: 0.03346448077474058\n",
      "Epoch: 295, loss: 0.03346444489094101\n",
      "Epoch: 296, loss: 0.033464430208162005\n",
      "Epoch: 297, loss: 0.03346440367110919\n",
      "Epoch: 298, loss: 0.03346436827329132\n",
      "Epoch: 299, loss: 0.03346435436676881\n",
      "Epoch: 300, loss: 0.03346432847184227\n"
     ]
    }
   ],
   "source": [
    "# initialize weights and biases\n",
    "# in Keras/TensorFlow/PyTorch etc. these are usually randomized in the beginning\n",
    "w1 = 1\n",
    "w2 = 0.5\n",
    "w3 = 1\n",
    "w4 = -0.5\n",
    "w5 = 1\n",
    "w6 = 1\n",
    "bias1 = 0.5\n",
    "bias2 = 0\n",
    "bias3 = 0.5\n",
    "\n",
    "# just for comparison after the training\n",
    "original_w1 = w1\n",
    "original_w2 = w2\n",
    "original_w3 = w3\n",
    "original_w4 = w4\n",
    "original_w5 = w5\n",
    "original_w6 = w6\n",
    "original_b1 = bias1\n",
    "original_b2 = bias2\n",
    "original_b3 = bias3\n",
    "\n",
    "# DataFrame values as a list\n",
    "# data = list(df.values)\n",
    "data = df.values\n",
    "\n",
    "# use min/max -scaling to make values in the range 0-1\n",
    "\n",
    "# independent support variables => indeces 0 and 1 (X)\n",
    "# target variable => index 2 (y)\n",
    "X = data[:, :2]\n",
    "y = data[:, 2:]\n",
    "\n",
    "# Scale X and y\n",
    "X_scaled = (X - np.min(X, axis=0)) / (np.max(X, axis=0) - np.min(X, axis=0))\n",
    "y_scaled = (y - np.min(y, axis=0)) / (np.max(y, axis=0) - np.min(y, axis=0))\n",
    "\n",
    "# combine back to original data format\n",
    "data = np.hstack((X_scaled, y_scaled))\n",
    "\n",
    "# learning rate\n",
    "LR = 0.005\n",
    "epochs = 300\n",
    "\n",
    "# let's initalize a list for loss visualizations\n",
    "loss_points = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # the previous version only measured the loss value\n",
    "    # of the last calculation done in the code (node 3)\n",
    "    # it's probably better to measure the average loss for each epoch\n",
    "    epoch_losses = []\n",
    "\n",
    "    for row in data:\n",
    "        # this is where we do Forward pass + backpropagation\n",
    "        input1 = row[0]\n",
    "        input2 = row[1]\n",
    "        true_value = row[2]\n",
    "\n",
    "        # NODE 1 OUTPUT\n",
    "        node_1_output = input1 * w1 + input2 * w3 + bias1\n",
    "        node_1_output = activation_ReLu(node_1_output)\n",
    "        node_1_output\n",
    "\n",
    "        # NODE 2 OUTPUT\n",
    "        node_2_output = input1 * w2 + input2 * w4 + bias2\n",
    "        node_2_output = activation_ReLu(node_2_output)\n",
    "        node_2_output\n",
    "\n",
    "        # NODE 3 OUTPUT\n",
    "        # we can just use Node 1 and 2 outputs, since they\n",
    "        # already contain the previous weights in their result\n",
    "        node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "        node_3_output = activation_ReLu(node_3_output)\n",
    "        node_3_output\n",
    "\n",
    "        # LOSS FUNCTION - we are going to use MSE -> mean squared error\n",
    "        # MSE formula for LOSS => (predicted_value - true_value) ^ 2\n",
    "        predicted_value = node_3_output\n",
    "        loss = (predicted_value - true_value) ** 2\n",
    "\n",
    "        # add current loss into epoch losses -list\n",
    "        epoch_losses.append(loss)\n",
    "        \n",
    "        # BACKPROPAGATION - LAST LAYER FIRST\n",
    "        # solving the partial derivative of the loss function with respect to w5\n",
    "        deriv_L_w5 = 2 * node_1_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w5 = w5 - LR * deriv_L_w5\n",
    "\n",
    "        deriv_L_w6 = 2 * node_2_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w6 = w6 - LR * deriv_L_w6\n",
    "\n",
    "        deriv_L_b3 = 2 * 1 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_b3 = bias3 - LR * deriv_L_b3\n",
    "\n",
    "        # BACKPROPAGATION - THE FIRST LAYER\n",
    "        # FROM THIS POINT ONWARD WE HAVE TO USE THE MORE COMPLEX VERSION\n",
    "        # OF UPDATING THE VALUES => CHAIN RULE\n",
    "\n",
    "        # weight 1\n",
    "        deriv_L_w1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input1\n",
    "        deriv_L_w1 = deriv_L_w1_left * deriv_L_w1_right\n",
    "        new_w1 = w1 - LR * deriv_L_w1\n",
    "\n",
    "        # weight 2\n",
    "        deriv_L_w2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w2_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w4 + bias2) * input1\n",
    "        deriv_L_w2 = deriv_L_w2_left * deriv_L_w2_right\n",
    "        new_w2 = w2 - LR * deriv_L_w2\n",
    "\n",
    "        # weight 3\n",
    "        deriv_L_w3_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w3_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input2\n",
    "        deriv_L_w3 = deriv_L_w3_left * deriv_L_w3_right\n",
    "        new_w3 = w3 - LR * deriv_L_w3\n",
    "\n",
    "        # weight 4\n",
    "        deriv_L_w4_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w4_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w4 + bias2) * input2\n",
    "        deriv_L_w4 = deriv_L_w4_left * deriv_L_w4_right\n",
    "        new_w4 = w4 - LR * deriv_L_w4\n",
    "\n",
    "        # bias 1\n",
    "        deriv_L_b1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * 1\n",
    "        deriv_L_b1 = deriv_L_b1_left * deriv_L_b1_right\n",
    "        new_b1 = bias1 - LR * deriv_L_b1\n",
    "\n",
    "        # bias 2\n",
    "        deriv_L_b2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b2_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w4 + bias2) * 1\n",
    "        deriv_L_b2 = deriv_L_b2_left * deriv_L_b2_right\n",
    "        new_b2 = bias2 - LR * deriv_L_b2\n",
    "\n",
    "        # ALL DONE! FINALLY UPDATE THE EXISTING WEIGHTS\n",
    "        w1 = new_w1\n",
    "        w2 = new_w2\n",
    "        w3 = new_w3\n",
    "        w4 = new_w4\n",
    "        w5 = new_w5\n",
    "        w6 = new_w6\n",
    "        bias1 = new_b1\n",
    "        bias2 = new_b2\n",
    "        bias3 = new_b3\n",
    "\n",
    "    # calculate average epoch-wise loss and add it to loss points\n",
    "    average_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "\n",
    "    # place the overall epoch loss into the loss_points list\n",
    "    loss_points.append(average_loss)\n",
    "    print(f\"Epoch: {epoch + 1}, loss: {average_loss}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_points)\n",
    "# plt.ylim(-1, 5)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL WEIGHTS AND BIASES\n",
      "w1: 1\n",
      "w2: 0.5\n",
      "w3: 1\n",
      "w4: -0.5\n",
      "w5: 1\n",
      "w6: 1\n",
      "b1: 0.5\n",
      "b2: 0\n",
      "b3: 0.5\n",
      "\n",
      "\n",
      "#################################\n",
      "\n",
      "\n",
      "NEW WEIGHTS AND BIASES\n",
      "w1: 0.5468548940950568\n",
      "w2: 0.30263781000103873\n",
      "w3: 0.6150842797322122\n",
      "w4: -0.5597046391790864\n",
      "w5: 0.32334708321460054\n",
      "w6: 0.9794174207605538\n",
      "b1: -0.08403769785815485\n",
      "b2: -0.24068822532722903\n",
      "b3: 0.06922317413775518\n"
     ]
    }
   ],
   "source": [
    "print(\"ORIGINAL WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {original_w1}\")\n",
    "print(f\"w2: {original_w2}\")\n",
    "print(f\"w3: {original_w3}\")\n",
    "print(f\"w4: {original_w4}\")\n",
    "print(f\"w5: {original_w5}\")\n",
    "print(f\"w6: {original_w6}\")\n",
    "print(f\"b1: {original_b1}\")\n",
    "print(f\"b2: {original_b2}\")\n",
    "print(f\"b3: {original_b3}\")\n",
    "\n",
    "print(\"\\n\\n#################################\\n\\n\")\n",
    "\n",
    "print(\"NEW WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {new_w1}\")\n",
    "print(f\"w2: {new_w2}\")\n",
    "print(f\"w3: {new_w3}\")\n",
    "print(f\"w4: {new_w4}\")\n",
    "print(f\"w5: {new_w5}\")\n",
    "print(f\"w6: {new_w6}\")\n",
    "print(f\"b1: {new_b1}\")\n",
    "print(f\"b2: {new_b2}\")\n",
    "print(f\"b3: {new_b3}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction function, just doing the forward pass\n",
    "# again (but only that)\n",
    "def predict(x1, x2):\n",
    "    input1 = x1\n",
    "    input2 = x2\n",
    "\n",
    "    # NODE 1 OUTPUT\n",
    "    node_1_output = input1 * w1 + input2 * w3 + bias1\n",
    "    node_1_output = activation_ReLu(node_1_output)\n",
    "\n",
    "    # NODE 2 OUTPUT\n",
    "    node_2_output = input1 * w2 + input2 * w4 + bias2\n",
    "    node_2_output = activation_ReLu(node_2_output)\n",
    "\n",
    "    # NODE 3 OUTPUT\n",
    "    # we can just use Node 1 and 2 outputs, since they\n",
    "    # already contain the previous weights in their result\n",
    "    node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "    node_3_output = activation_ReLu(node_3_output)\n",
    "    return node_3_output\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>27.900</td>\n",
       "      <td>16884.92400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1725.55230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>33.000</td>\n",
       "      <td>4449.46200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>22.705</td>\n",
       "      <td>21984.47061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>28.880</td>\n",
       "      <td>3866.85520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2767</th>\n",
       "      <td>47</td>\n",
       "      <td>45.320</td>\n",
       "      <td>8569.86180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2768</th>\n",
       "      <td>21</td>\n",
       "      <td>34.600</td>\n",
       "      <td>2020.17700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2769</th>\n",
       "      <td>19</td>\n",
       "      <td>26.030</td>\n",
       "      <td>16450.89470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770</th>\n",
       "      <td>23</td>\n",
       "      <td>18.715</td>\n",
       "      <td>21595.38229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2771</th>\n",
       "      <td>54</td>\n",
       "      <td>31.600</td>\n",
       "      <td>9850.43200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2772 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age     bmi      charges\n",
       "0      19  27.900  16884.92400\n",
       "1      18  33.770   1725.55230\n",
       "2      28  33.000   4449.46200\n",
       "3      33  22.705  21984.47061\n",
       "4      32  28.880   3866.85520\n",
       "...   ...     ...          ...\n",
       "2767   47  45.320   8569.86180\n",
       "2768   21  34.600   2020.17700\n",
       "2769   19  26.030  16450.89470\n",
       "2770   23  18.715  21595.38229\n",
       "2771   54  31.600   9850.43200\n",
       "\n",
       "[2772 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age           54.00000\n",
       "bmi           47.41000\n",
       "charges    63770.42801\n",
       "Name: 543, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index 543 is one of the biggest charges in the data\n",
    "df.iloc[543]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.set_printoptions(precision=12, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7826087 , 0.84611246, 1.        ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scaled values\n",
    "data[543]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(63770.42801)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# $ ~63770 \n",
    "df['charges'].max() * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.3487134549839619)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try using the model with our prediction function\n",
    "# the value tends to be same as final bias 3\n",
    "# so if node1 and node2 outputs are small => more or less bias3\n",
    "result = predict(0.7826087 , 0.84611246)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(22237.60627717312)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['charges'].max() * result\n",
    "\n",
    "# estimated $ ~22237 USD, heavily undershoots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2772.000000</td>\n",
       "      <td>2772.000000</td>\n",
       "      <td>2772.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>39.109668</td>\n",
       "      <td>30.701349</td>\n",
       "      <td>13261.369959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.081459</td>\n",
       "      <td>6.129449</td>\n",
       "      <td>12151.768945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>15.960000</td>\n",
       "      <td>1121.873900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>26.000000</td>\n",
       "      <td>26.220000</td>\n",
       "      <td>4687.797000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>30.447500</td>\n",
       "      <td>9333.014350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>51.000000</td>\n",
       "      <td>34.770000</td>\n",
       "      <td>16577.779500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>64.000000</td>\n",
       "      <td>53.130000</td>\n",
       "      <td>63770.428010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               age          bmi       charges\n",
       "count  2772.000000  2772.000000   2772.000000\n",
       "mean     39.109668    30.701349  13261.369959\n",
       "std      14.081459     6.129449  12151.768945\n",
       "min      18.000000    15.960000   1121.873900\n",
       "25%      26.000000    26.220000   4687.797000\n",
       "50%      39.000000    30.447500   9333.014350\n",
       "75%      51.000000    34.770000  16577.779500\n",
       "max      64.000000    53.130000  63770.428010"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just checking if the value revolves around the average...\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
