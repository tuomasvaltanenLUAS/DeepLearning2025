{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network, experimentation tool, version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# activation functions\n",
    "# ReLu is very simple, it filters out all negative values\n",
    "# this is a powerful activation function in reality\n",
    "def activation_ReLu(number):\n",
    "    if number > 0:\n",
    "        return number\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# we also need a derivated version of ReLu\n",
    "# otherwise same as original, but instead of original value, return 1 instead\n",
    "def activation_ReLu_partial_derivative(number):\n",
    "    if number > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lock down the randomness in order to get same results everytime\n",
    "# you can change or disable this if you want\n",
    "np.random.seed(123)\n",
    "\n",
    "def generate_train_data():\n",
    "    result = []\n",
    "\n",
    "    # create 100 numbers\n",
    "    for x in range(100):\n",
    "        n1 = np.random.randint(0, 5)\n",
    "        n2 = np.random.randint(3, 7)\n",
    "\n",
    "        # formula for the target variable: x1 ^^ 2 + x2 + (random integer between 0-5)\n",
    "        # the only point of this is to have some kind of logic in the data\n",
    "        n3 = n1 ** 2 + n2 + np.random.randint(0, 5)\n",
    "        n3 = int(n3)\n",
    "\n",
    "        result.append([n1, n2, n3])\n",
    "\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 84.04477548500036\n",
      "Epoch: 2, loss: 34.67887701638463\n",
      "Epoch: 3, loss: 34.6810770576847\n",
      "Epoch: 4, loss: 34.6810972017027\n",
      "Epoch: 5, loss: 34.681097321215475\n",
      "Epoch: 6, loss: 34.68109732192306\n",
      "Epoch: 7, loss: 34.68109732192725\n",
      "Epoch: 8, loss: 34.681097321927275\n",
      "Epoch: 9, loss: 34.681097321927275\n",
      "Epoch: 10, loss: 34.681097321927275\n",
      "Epoch: 11, loss: 34.681097321927275\n",
      "Epoch: 12, loss: 34.681097321927275\n",
      "Epoch: 13, loss: 34.681097321927275\n",
      "Epoch: 14, loss: 34.681097321927275\n",
      "Epoch: 15, loss: 34.681097321927275\n",
      "Epoch: 16, loss: 34.681097321927275\n",
      "Epoch: 17, loss: 34.681097321927275\n",
      "Epoch: 18, loss: 34.681097321927275\n",
      "Epoch: 19, loss: 34.681097321927275\n",
      "Epoch: 20, loss: 34.681097321927275\n",
      "Epoch: 21, loss: 34.681097321927275\n",
      "Epoch: 22, loss: 34.681097321927275\n",
      "Epoch: 23, loss: 34.681097321927275\n",
      "Epoch: 24, loss: 34.681097321927275\n",
      "Epoch: 25, loss: 34.681097321927275\n",
      "Epoch: 26, loss: 34.681097321927275\n",
      "Epoch: 27, loss: 34.681097321927275\n",
      "Epoch: 28, loss: 34.681097321927275\n",
      "Epoch: 29, loss: 34.681097321927275\n",
      "Epoch: 30, loss: 34.681097321927275\n",
      "Epoch: 31, loss: 34.681097321927275\n",
      "Epoch: 32, loss: 34.681097321927275\n",
      "Epoch: 33, loss: 34.681097321927275\n",
      "Epoch: 34, loss: 34.681097321927275\n",
      "Epoch: 35, loss: 34.681097321927275\n",
      "Epoch: 36, loss: 34.681097321927275\n",
      "Epoch: 37, loss: 34.681097321927275\n",
      "Epoch: 38, loss: 34.681097321927275\n",
      "Epoch: 39, loss: 34.681097321927275\n",
      "Epoch: 40, loss: 34.681097321927275\n",
      "Epoch: 41, loss: 34.681097321927275\n",
      "Epoch: 42, loss: 34.681097321927275\n",
      "Epoch: 43, loss: 34.681097321927275\n",
      "Epoch: 44, loss: 34.681097321927275\n",
      "Epoch: 45, loss: 34.681097321927275\n",
      "Epoch: 46, loss: 34.681097321927275\n",
      "Epoch: 47, loss: 34.681097321927275\n",
      "Epoch: 48, loss: 34.681097321927275\n",
      "Epoch: 49, loss: 34.681097321927275\n",
      "Epoch: 50, loss: 34.681097321927275\n",
      "Epoch: 51, loss: 34.681097321927275\n",
      "Epoch: 52, loss: 34.681097321927275\n",
      "Epoch: 53, loss: 34.681097321927275\n",
      "Epoch: 54, loss: 34.681097321927275\n",
      "Epoch: 55, loss: 34.681097321927275\n",
      "Epoch: 56, loss: 34.681097321927275\n",
      "Epoch: 57, loss: 34.681097321927275\n",
      "Epoch: 58, loss: 34.681097321927275\n",
      "Epoch: 59, loss: 34.681097321927275\n",
      "Epoch: 60, loss: 34.681097321927275\n",
      "Epoch: 61, loss: 34.681097321927275\n",
      "Epoch: 62, loss: 34.681097321927275\n",
      "Epoch: 63, loss: 34.681097321927275\n",
      "Epoch: 64, loss: 34.681097321927275\n",
      "Epoch: 65, loss: 34.681097321927275\n",
      "Epoch: 66, loss: 34.681097321927275\n",
      "Epoch: 67, loss: 34.681097321927275\n",
      "Epoch: 68, loss: 34.681097321927275\n",
      "Epoch: 69, loss: 34.681097321927275\n",
      "Epoch: 70, loss: 34.681097321927275\n",
      "Epoch: 71, loss: 34.681097321927275\n",
      "Epoch: 72, loss: 34.681097321927275\n",
      "Epoch: 73, loss: 34.681097321927275\n",
      "Epoch: 74, loss: 34.681097321927275\n",
      "Epoch: 75, loss: 34.681097321927275\n",
      "Epoch: 76, loss: 34.681097321927275\n",
      "Epoch: 77, loss: 34.681097321927275\n",
      "Epoch: 78, loss: 34.681097321927275\n",
      "Epoch: 79, loss: 34.681097321927275\n",
      "Epoch: 80, loss: 34.681097321927275\n",
      "Epoch: 81, loss: 34.681097321927275\n",
      "Epoch: 82, loss: 34.681097321927275\n",
      "Epoch: 83, loss: 34.681097321927275\n",
      "Epoch: 84, loss: 34.681097321927275\n",
      "Epoch: 85, loss: 34.681097321927275\n",
      "Epoch: 86, loss: 34.681097321927275\n",
      "Epoch: 87, loss: 34.681097321927275\n",
      "Epoch: 88, loss: 34.681097321927275\n",
      "Epoch: 89, loss: 34.681097321927275\n",
      "Epoch: 90, loss: 34.681097321927275\n",
      "Epoch: 91, loss: 34.681097321927275\n",
      "Epoch: 92, loss: 34.681097321927275\n",
      "Epoch: 93, loss: 34.681097321927275\n",
      "Epoch: 94, loss: 34.681097321927275\n",
      "Epoch: 95, loss: 34.681097321927275\n",
      "Epoch: 96, loss: 34.681097321927275\n",
      "Epoch: 97, loss: 34.681097321927275\n",
      "Epoch: 98, loss: 34.681097321927275\n",
      "Epoch: 99, loss: 34.681097321927275\n",
      "Epoch: 100, loss: 34.681097321927275\n",
      "Epoch: 101, loss: 34.681097321927275\n",
      "Epoch: 102, loss: 34.681097321927275\n",
      "Epoch: 103, loss: 34.681097321927275\n",
      "Epoch: 104, loss: 34.681097321927275\n",
      "Epoch: 105, loss: 34.681097321927275\n",
      "Epoch: 106, loss: 34.681097321927275\n",
      "Epoch: 107, loss: 34.681097321927275\n",
      "Epoch: 108, loss: 34.681097321927275\n",
      "Epoch: 109, loss: 34.681097321927275\n",
      "Epoch: 110, loss: 34.681097321927275\n",
      "Epoch: 111, loss: 34.681097321927275\n",
      "Epoch: 112, loss: 34.681097321927275\n",
      "Epoch: 113, loss: 34.681097321927275\n",
      "Epoch: 114, loss: 34.681097321927275\n",
      "Epoch: 115, loss: 34.681097321927275\n",
      "Epoch: 116, loss: 34.681097321927275\n",
      "Epoch: 117, loss: 34.681097321927275\n",
      "Epoch: 118, loss: 34.681097321927275\n",
      "Epoch: 119, loss: 34.681097321927275\n",
      "Epoch: 120, loss: 34.681097321927275\n",
      "Epoch: 121, loss: 34.681097321927275\n",
      "Epoch: 122, loss: 34.681097321927275\n",
      "Epoch: 123, loss: 34.681097321927275\n",
      "Epoch: 124, loss: 34.681097321927275\n",
      "Epoch: 125, loss: 34.681097321927275\n",
      "Epoch: 126, loss: 34.681097321927275\n",
      "Epoch: 127, loss: 34.681097321927275\n",
      "Epoch: 128, loss: 34.681097321927275\n",
      "Epoch: 129, loss: 34.681097321927275\n",
      "Epoch: 130, loss: 34.681097321927275\n",
      "Epoch: 131, loss: 34.681097321927275\n",
      "Epoch: 132, loss: 34.681097321927275\n",
      "Epoch: 133, loss: 34.681097321927275\n",
      "Epoch: 134, loss: 34.681097321927275\n",
      "Epoch: 135, loss: 34.681097321927275\n",
      "Epoch: 136, loss: 34.681097321927275\n",
      "Epoch: 137, loss: 34.681097321927275\n",
      "Epoch: 138, loss: 34.681097321927275\n",
      "Epoch: 139, loss: 34.681097321927275\n",
      "Epoch: 140, loss: 34.681097321927275\n",
      "Epoch: 141, loss: 34.681097321927275\n",
      "Epoch: 142, loss: 34.681097321927275\n",
      "Epoch: 143, loss: 34.681097321927275\n",
      "Epoch: 144, loss: 34.681097321927275\n",
      "Epoch: 145, loss: 34.681097321927275\n",
      "Epoch: 146, loss: 34.681097321927275\n",
      "Epoch: 147, loss: 34.681097321927275\n",
      "Epoch: 148, loss: 34.681097321927275\n",
      "Epoch: 149, loss: 34.681097321927275\n",
      "Epoch: 150, loss: 34.681097321927275\n",
      "Epoch: 151, loss: 34.681097321927275\n",
      "Epoch: 152, loss: 34.681097321927275\n",
      "Epoch: 153, loss: 34.681097321927275\n",
      "Epoch: 154, loss: 34.681097321927275\n",
      "Epoch: 155, loss: 34.681097321927275\n",
      "Epoch: 156, loss: 34.681097321927275\n",
      "Epoch: 157, loss: 34.681097321927275\n",
      "Epoch: 158, loss: 34.681097321927275\n",
      "Epoch: 159, loss: 34.681097321927275\n",
      "Epoch: 160, loss: 34.681097321927275\n",
      "Epoch: 161, loss: 34.681097321927275\n",
      "Epoch: 162, loss: 34.681097321927275\n",
      "Epoch: 163, loss: 34.681097321927275\n",
      "Epoch: 164, loss: 34.681097321927275\n",
      "Epoch: 165, loss: 34.681097321927275\n",
      "Epoch: 166, loss: 34.681097321927275\n",
      "Epoch: 167, loss: 34.681097321927275\n",
      "Epoch: 168, loss: 34.681097321927275\n",
      "Epoch: 169, loss: 34.681097321927275\n",
      "Epoch: 170, loss: 34.681097321927275\n",
      "Epoch: 171, loss: 34.681097321927275\n",
      "Epoch: 172, loss: 34.681097321927275\n",
      "Epoch: 173, loss: 34.681097321927275\n",
      "Epoch: 174, loss: 34.681097321927275\n",
      "Epoch: 175, loss: 34.681097321927275\n",
      "Epoch: 176, loss: 34.681097321927275\n",
      "Epoch: 177, loss: 34.681097321927275\n",
      "Epoch: 178, loss: 34.681097321927275\n",
      "Epoch: 179, loss: 34.681097321927275\n",
      "Epoch: 180, loss: 34.681097321927275\n",
      "Epoch: 181, loss: 34.681097321927275\n",
      "Epoch: 182, loss: 34.681097321927275\n",
      "Epoch: 183, loss: 34.681097321927275\n",
      "Epoch: 184, loss: 34.681097321927275\n",
      "Epoch: 185, loss: 34.681097321927275\n",
      "Epoch: 186, loss: 34.681097321927275\n",
      "Epoch: 187, loss: 34.681097321927275\n",
      "Epoch: 188, loss: 34.681097321927275\n",
      "Epoch: 189, loss: 34.681097321927275\n",
      "Epoch: 190, loss: 34.681097321927275\n",
      "Epoch: 191, loss: 34.681097321927275\n",
      "Epoch: 192, loss: 34.681097321927275\n",
      "Epoch: 193, loss: 34.681097321927275\n",
      "Epoch: 194, loss: 34.681097321927275\n",
      "Epoch: 195, loss: 34.681097321927275\n",
      "Epoch: 196, loss: 34.681097321927275\n",
      "Epoch: 197, loss: 34.681097321927275\n",
      "Epoch: 198, loss: 34.681097321927275\n",
      "Epoch: 199, loss: 34.681097321927275\n",
      "Epoch: 200, loss: 34.681097321927275\n",
      "Epoch: 201, loss: 34.681097321927275\n",
      "Epoch: 202, loss: 34.681097321927275\n",
      "Epoch: 203, loss: 34.681097321927275\n",
      "Epoch: 204, loss: 34.681097321927275\n",
      "Epoch: 205, loss: 34.681097321927275\n",
      "Epoch: 206, loss: 34.681097321927275\n",
      "Epoch: 207, loss: 34.681097321927275\n",
      "Epoch: 208, loss: 34.681097321927275\n",
      "Epoch: 209, loss: 34.681097321927275\n",
      "Epoch: 210, loss: 34.681097321927275\n",
      "Epoch: 211, loss: 34.681097321927275\n",
      "Epoch: 212, loss: 34.681097321927275\n",
      "Epoch: 213, loss: 34.681097321927275\n",
      "Epoch: 214, loss: 34.681097321927275\n",
      "Epoch: 215, loss: 34.681097321927275\n",
      "Epoch: 216, loss: 34.681097321927275\n",
      "Epoch: 217, loss: 34.681097321927275\n",
      "Epoch: 218, loss: 34.681097321927275\n",
      "Epoch: 219, loss: 34.681097321927275\n",
      "Epoch: 220, loss: 34.681097321927275\n",
      "Epoch: 221, loss: 34.681097321927275\n",
      "Epoch: 222, loss: 34.681097321927275\n",
      "Epoch: 223, loss: 34.681097321927275\n",
      "Epoch: 224, loss: 34.681097321927275\n",
      "Epoch: 225, loss: 34.681097321927275\n",
      "Epoch: 226, loss: 34.681097321927275\n",
      "Epoch: 227, loss: 34.681097321927275\n",
      "Epoch: 228, loss: 34.681097321927275\n",
      "Epoch: 229, loss: 34.681097321927275\n",
      "Epoch: 230, loss: 34.681097321927275\n",
      "Epoch: 231, loss: 34.681097321927275\n",
      "Epoch: 232, loss: 34.681097321927275\n",
      "Epoch: 233, loss: 34.681097321927275\n",
      "Epoch: 234, loss: 34.681097321927275\n",
      "Epoch: 235, loss: 34.681097321927275\n",
      "Epoch: 236, loss: 34.681097321927275\n",
      "Epoch: 237, loss: 34.681097321927275\n",
      "Epoch: 238, loss: 34.681097321927275\n",
      "Epoch: 239, loss: 34.681097321927275\n",
      "Epoch: 240, loss: 34.681097321927275\n",
      "Epoch: 241, loss: 34.681097321927275\n",
      "Epoch: 242, loss: 34.681097321927275\n",
      "Epoch: 243, loss: 34.681097321927275\n",
      "Epoch: 244, loss: 34.681097321927275\n",
      "Epoch: 245, loss: 34.681097321927275\n",
      "Epoch: 246, loss: 34.681097321927275\n",
      "Epoch: 247, loss: 34.681097321927275\n",
      "Epoch: 248, loss: 34.681097321927275\n",
      "Epoch: 249, loss: 34.681097321927275\n",
      "Epoch: 250, loss: 34.681097321927275\n",
      "Epoch: 251, loss: 34.681097321927275\n",
      "Epoch: 252, loss: 34.681097321927275\n",
      "Epoch: 253, loss: 34.681097321927275\n",
      "Epoch: 254, loss: 34.681097321927275\n",
      "Epoch: 255, loss: 34.681097321927275\n",
      "Epoch: 256, loss: 34.681097321927275\n",
      "Epoch: 257, loss: 34.681097321927275\n",
      "Epoch: 258, loss: 34.681097321927275\n",
      "Epoch: 259, loss: 34.681097321927275\n",
      "Epoch: 260, loss: 34.681097321927275\n",
      "Epoch: 261, loss: 34.681097321927275\n",
      "Epoch: 262, loss: 34.681097321927275\n",
      "Epoch: 263, loss: 34.681097321927275\n",
      "Epoch: 264, loss: 34.681097321927275\n",
      "Epoch: 265, loss: 34.681097321927275\n",
      "Epoch: 266, loss: 34.681097321927275\n",
      "Epoch: 267, loss: 34.681097321927275\n",
      "Epoch: 268, loss: 34.681097321927275\n",
      "Epoch: 269, loss: 34.681097321927275\n",
      "Epoch: 270, loss: 34.681097321927275\n",
      "Epoch: 271, loss: 34.681097321927275\n",
      "Epoch: 272, loss: 34.681097321927275\n",
      "Epoch: 273, loss: 34.681097321927275\n",
      "Epoch: 274, loss: 34.681097321927275\n",
      "Epoch: 275, loss: 34.681097321927275\n",
      "Epoch: 276, loss: 34.681097321927275\n",
      "Epoch: 277, loss: 34.681097321927275\n",
      "Epoch: 278, loss: 34.681097321927275\n",
      "Epoch: 279, loss: 34.681097321927275\n",
      "Epoch: 280, loss: 34.681097321927275\n",
      "Epoch: 281, loss: 34.681097321927275\n",
      "Epoch: 282, loss: 34.681097321927275\n",
      "Epoch: 283, loss: 34.681097321927275\n",
      "Epoch: 284, loss: 34.681097321927275\n",
      "Epoch: 285, loss: 34.681097321927275\n",
      "Epoch: 286, loss: 34.681097321927275\n",
      "Epoch: 287, loss: 34.681097321927275\n",
      "Epoch: 288, loss: 34.681097321927275\n",
      "Epoch: 289, loss: 34.681097321927275\n",
      "Epoch: 290, loss: 34.681097321927275\n",
      "Epoch: 291, loss: 34.681097321927275\n",
      "Epoch: 292, loss: 34.681097321927275\n",
      "Epoch: 293, loss: 34.681097321927275\n",
      "Epoch: 294, loss: 34.681097321927275\n",
      "Epoch: 295, loss: 34.681097321927275\n",
      "Epoch: 296, loss: 34.681097321927275\n",
      "Epoch: 297, loss: 34.681097321927275\n",
      "Epoch: 298, loss: 34.681097321927275\n",
      "Epoch: 299, loss: 34.681097321927275\n",
      "Epoch: 300, loss: 34.681097321927275\n",
      "Epoch: 301, loss: 34.681097321927275\n",
      "Epoch: 302, loss: 34.681097321927275\n",
      "Epoch: 303, loss: 34.681097321927275\n",
      "Epoch: 304, loss: 34.681097321927275\n",
      "Epoch: 305, loss: 34.681097321927275\n",
      "Epoch: 306, loss: 34.681097321927275\n",
      "Epoch: 307, loss: 34.681097321927275\n",
      "Epoch: 308, loss: 34.681097321927275\n",
      "Epoch: 309, loss: 34.681097321927275\n",
      "Epoch: 310, loss: 34.681097321927275\n",
      "Epoch: 311, loss: 34.681097321927275\n",
      "Epoch: 312, loss: 34.681097321927275\n",
      "Epoch: 313, loss: 34.681097321927275\n",
      "Epoch: 314, loss: 34.681097321927275\n",
      "Epoch: 315, loss: 34.681097321927275\n",
      "Epoch: 316, loss: 34.681097321927275\n",
      "Epoch: 317, loss: 34.681097321927275\n",
      "Epoch: 318, loss: 34.681097321927275\n",
      "Epoch: 319, loss: 34.681097321927275\n",
      "Epoch: 320, loss: 34.681097321927275\n",
      "Epoch: 321, loss: 34.681097321927275\n",
      "Epoch: 322, loss: 34.681097321927275\n",
      "Epoch: 323, loss: 34.681097321927275\n",
      "Epoch: 324, loss: 34.681097321927275\n",
      "Epoch: 325, loss: 34.681097321927275\n",
      "Epoch: 326, loss: 34.681097321927275\n",
      "Epoch: 327, loss: 34.681097321927275\n",
      "Epoch: 328, loss: 34.681097321927275\n",
      "Epoch: 329, loss: 34.681097321927275\n",
      "Epoch: 330, loss: 34.681097321927275\n",
      "Epoch: 331, loss: 34.681097321927275\n",
      "Epoch: 332, loss: 34.681097321927275\n",
      "Epoch: 333, loss: 34.681097321927275\n",
      "Epoch: 334, loss: 34.681097321927275\n",
      "Epoch: 335, loss: 34.681097321927275\n",
      "Epoch: 336, loss: 34.681097321927275\n",
      "Epoch: 337, loss: 34.681097321927275\n",
      "Epoch: 338, loss: 34.681097321927275\n",
      "Epoch: 339, loss: 34.681097321927275\n",
      "Epoch: 340, loss: 34.681097321927275\n",
      "Epoch: 341, loss: 34.681097321927275\n",
      "Epoch: 342, loss: 34.681097321927275\n",
      "Epoch: 343, loss: 34.681097321927275\n",
      "Epoch: 344, loss: 34.681097321927275\n",
      "Epoch: 345, loss: 34.681097321927275\n",
      "Epoch: 346, loss: 34.681097321927275\n",
      "Epoch: 347, loss: 34.681097321927275\n",
      "Epoch: 348, loss: 34.681097321927275\n",
      "Epoch: 349, loss: 34.681097321927275\n",
      "Epoch: 350, loss: 34.681097321927275\n",
      "Epoch: 351, loss: 34.681097321927275\n",
      "Epoch: 352, loss: 34.681097321927275\n",
      "Epoch: 353, loss: 34.681097321927275\n",
      "Epoch: 354, loss: 34.681097321927275\n",
      "Epoch: 355, loss: 34.681097321927275\n",
      "Epoch: 356, loss: 34.681097321927275\n",
      "Epoch: 357, loss: 34.681097321927275\n",
      "Epoch: 358, loss: 34.681097321927275\n",
      "Epoch: 359, loss: 34.681097321927275\n",
      "Epoch: 360, loss: 34.681097321927275\n",
      "Epoch: 361, loss: 34.681097321927275\n",
      "Epoch: 362, loss: 34.681097321927275\n",
      "Epoch: 363, loss: 34.681097321927275\n",
      "Epoch: 364, loss: 34.681097321927275\n",
      "Epoch: 365, loss: 34.681097321927275\n",
      "Epoch: 366, loss: 34.681097321927275\n",
      "Epoch: 367, loss: 34.681097321927275\n",
      "Epoch: 368, loss: 34.681097321927275\n",
      "Epoch: 369, loss: 34.681097321927275\n",
      "Epoch: 370, loss: 34.681097321927275\n",
      "Epoch: 371, loss: 34.681097321927275\n",
      "Epoch: 372, loss: 34.681097321927275\n",
      "Epoch: 373, loss: 34.681097321927275\n",
      "Epoch: 374, loss: 34.681097321927275\n",
      "Epoch: 375, loss: 34.681097321927275\n",
      "Epoch: 376, loss: 34.681097321927275\n",
      "Epoch: 377, loss: 34.681097321927275\n",
      "Epoch: 378, loss: 34.681097321927275\n",
      "Epoch: 379, loss: 34.681097321927275\n",
      "Epoch: 380, loss: 34.681097321927275\n",
      "Epoch: 381, loss: 34.681097321927275\n",
      "Epoch: 382, loss: 34.681097321927275\n",
      "Epoch: 383, loss: 34.681097321927275\n",
      "Epoch: 384, loss: 34.681097321927275\n",
      "Epoch: 385, loss: 34.681097321927275\n",
      "Epoch: 386, loss: 34.681097321927275\n",
      "Epoch: 387, loss: 34.681097321927275\n",
      "Epoch: 388, loss: 34.681097321927275\n",
      "Epoch: 389, loss: 34.681097321927275\n",
      "Epoch: 390, loss: 34.681097321927275\n",
      "Epoch: 391, loss: 34.681097321927275\n",
      "Epoch: 392, loss: 34.681097321927275\n",
      "Epoch: 393, loss: 34.681097321927275\n",
      "Epoch: 394, loss: 34.681097321927275\n",
      "Epoch: 395, loss: 34.681097321927275\n",
      "Epoch: 396, loss: 34.681097321927275\n",
      "Epoch: 397, loss: 34.681097321927275\n",
      "Epoch: 398, loss: 34.681097321927275\n",
      "Epoch: 399, loss: 34.681097321927275\n",
      "Epoch: 400, loss: 34.681097321927275\n",
      "Epoch: 401, loss: 34.681097321927275\n",
      "Epoch: 402, loss: 34.681097321927275\n",
      "Epoch: 403, loss: 34.681097321927275\n",
      "Epoch: 404, loss: 34.681097321927275\n",
      "Epoch: 405, loss: 34.681097321927275\n",
      "Epoch: 406, loss: 34.681097321927275\n",
      "Epoch: 407, loss: 34.681097321927275\n",
      "Epoch: 408, loss: 34.681097321927275\n",
      "Epoch: 409, loss: 34.681097321927275\n",
      "Epoch: 410, loss: 34.681097321927275\n",
      "Epoch: 411, loss: 34.681097321927275\n",
      "Epoch: 412, loss: 34.681097321927275\n",
      "Epoch: 413, loss: 34.681097321927275\n",
      "Epoch: 414, loss: 34.681097321927275\n",
      "Epoch: 415, loss: 34.681097321927275\n",
      "Epoch: 416, loss: 34.681097321927275\n",
      "Epoch: 417, loss: 34.681097321927275\n",
      "Epoch: 418, loss: 34.681097321927275\n",
      "Epoch: 419, loss: 34.681097321927275\n",
      "Epoch: 420, loss: 34.681097321927275\n",
      "Epoch: 421, loss: 34.681097321927275\n",
      "Epoch: 422, loss: 34.681097321927275\n",
      "Epoch: 423, loss: 34.681097321927275\n",
      "Epoch: 424, loss: 34.681097321927275\n",
      "Epoch: 425, loss: 34.681097321927275\n",
      "Epoch: 426, loss: 34.681097321927275\n",
      "Epoch: 427, loss: 34.681097321927275\n",
      "Epoch: 428, loss: 34.681097321927275\n",
      "Epoch: 429, loss: 34.681097321927275\n",
      "Epoch: 430, loss: 34.681097321927275\n",
      "Epoch: 431, loss: 34.681097321927275\n",
      "Epoch: 432, loss: 34.681097321927275\n",
      "Epoch: 433, loss: 34.681097321927275\n",
      "Epoch: 434, loss: 34.681097321927275\n",
      "Epoch: 435, loss: 34.681097321927275\n",
      "Epoch: 436, loss: 34.681097321927275\n",
      "Epoch: 437, loss: 34.681097321927275\n",
      "Epoch: 438, loss: 34.681097321927275\n",
      "Epoch: 439, loss: 34.681097321927275\n",
      "Epoch: 440, loss: 34.681097321927275\n",
      "Epoch: 441, loss: 34.681097321927275\n",
      "Epoch: 442, loss: 34.681097321927275\n",
      "Epoch: 443, loss: 34.681097321927275\n",
      "Epoch: 444, loss: 34.681097321927275\n",
      "Epoch: 445, loss: 34.681097321927275\n",
      "Epoch: 446, loss: 34.681097321927275\n",
      "Epoch: 447, loss: 34.681097321927275\n",
      "Epoch: 448, loss: 34.681097321927275\n",
      "Epoch: 449, loss: 34.681097321927275\n",
      "Epoch: 450, loss: 34.681097321927275\n",
      "Epoch: 451, loss: 34.681097321927275\n",
      "Epoch: 452, loss: 34.681097321927275\n",
      "Epoch: 453, loss: 34.681097321927275\n",
      "Epoch: 454, loss: 34.681097321927275\n",
      "Epoch: 455, loss: 34.681097321927275\n",
      "Epoch: 456, loss: 34.681097321927275\n",
      "Epoch: 457, loss: 34.681097321927275\n",
      "Epoch: 458, loss: 34.681097321927275\n",
      "Epoch: 459, loss: 34.681097321927275\n",
      "Epoch: 460, loss: 34.681097321927275\n",
      "Epoch: 461, loss: 34.681097321927275\n",
      "Epoch: 462, loss: 34.681097321927275\n",
      "Epoch: 463, loss: 34.681097321927275\n",
      "Epoch: 464, loss: 34.681097321927275\n",
      "Epoch: 465, loss: 34.681097321927275\n",
      "Epoch: 466, loss: 34.681097321927275\n",
      "Epoch: 467, loss: 34.681097321927275\n",
      "Epoch: 468, loss: 34.681097321927275\n",
      "Epoch: 469, loss: 34.681097321927275\n",
      "Epoch: 470, loss: 34.681097321927275\n",
      "Epoch: 471, loss: 34.681097321927275\n",
      "Epoch: 472, loss: 34.681097321927275\n",
      "Epoch: 473, loss: 34.681097321927275\n",
      "Epoch: 474, loss: 34.681097321927275\n",
      "Epoch: 475, loss: 34.681097321927275\n",
      "Epoch: 476, loss: 34.681097321927275\n",
      "Epoch: 477, loss: 34.681097321927275\n",
      "Epoch: 478, loss: 34.681097321927275\n",
      "Epoch: 479, loss: 34.681097321927275\n",
      "Epoch: 480, loss: 34.681097321927275\n",
      "Epoch: 481, loss: 34.681097321927275\n",
      "Epoch: 482, loss: 34.681097321927275\n",
      "Epoch: 483, loss: 34.681097321927275\n",
      "Epoch: 484, loss: 34.681097321927275\n",
      "Epoch: 485, loss: 34.681097321927275\n",
      "Epoch: 486, loss: 34.681097321927275\n",
      "Epoch: 487, loss: 34.681097321927275\n",
      "Epoch: 488, loss: 34.681097321927275\n",
      "Epoch: 489, loss: 34.681097321927275\n",
      "Epoch: 490, loss: 34.681097321927275\n",
      "Epoch: 491, loss: 34.681097321927275\n",
      "Epoch: 492, loss: 34.681097321927275\n",
      "Epoch: 493, loss: 34.681097321927275\n",
      "Epoch: 494, loss: 34.681097321927275\n",
      "Epoch: 495, loss: 34.681097321927275\n",
      "Epoch: 496, loss: 34.681097321927275\n",
      "Epoch: 497, loss: 34.681097321927275\n",
      "Epoch: 498, loss: 34.681097321927275\n",
      "Epoch: 499, loss: 34.681097321927275\n",
      "Epoch: 500, loss: 34.681097321927275\n",
      "Epoch: 501, loss: 34.681097321927275\n",
      "Epoch: 502, loss: 34.681097321927275\n",
      "Epoch: 503, loss: 34.681097321927275\n",
      "Epoch: 504, loss: 34.681097321927275\n",
      "Epoch: 505, loss: 34.681097321927275\n",
      "Epoch: 506, loss: 34.681097321927275\n",
      "Epoch: 507, loss: 34.681097321927275\n",
      "Epoch: 508, loss: 34.681097321927275\n",
      "Epoch: 509, loss: 34.681097321927275\n",
      "Epoch: 510, loss: 34.681097321927275\n",
      "Epoch: 511, loss: 34.681097321927275\n",
      "Epoch: 512, loss: 34.681097321927275\n",
      "Epoch: 513, loss: 34.681097321927275\n",
      "Epoch: 514, loss: 34.681097321927275\n",
      "Epoch: 515, loss: 34.681097321927275\n",
      "Epoch: 516, loss: 34.681097321927275\n",
      "Epoch: 517, loss: 34.681097321927275\n",
      "Epoch: 518, loss: 34.681097321927275\n",
      "Epoch: 519, loss: 34.681097321927275\n",
      "Epoch: 520, loss: 34.681097321927275\n",
      "Epoch: 521, loss: 34.681097321927275\n",
      "Epoch: 522, loss: 34.681097321927275\n",
      "Epoch: 523, loss: 34.681097321927275\n",
      "Epoch: 524, loss: 34.681097321927275\n",
      "Epoch: 525, loss: 34.681097321927275\n",
      "Epoch: 526, loss: 34.681097321927275\n",
      "Epoch: 527, loss: 34.681097321927275\n",
      "Epoch: 528, loss: 34.681097321927275\n",
      "Epoch: 529, loss: 34.681097321927275\n",
      "Epoch: 530, loss: 34.681097321927275\n",
      "Epoch: 531, loss: 34.681097321927275\n",
      "Epoch: 532, loss: 34.681097321927275\n",
      "Epoch: 533, loss: 34.681097321927275\n",
      "Epoch: 534, loss: 34.681097321927275\n",
      "Epoch: 535, loss: 34.681097321927275\n",
      "Epoch: 536, loss: 34.681097321927275\n",
      "Epoch: 537, loss: 34.681097321927275\n",
      "Epoch: 538, loss: 34.681097321927275\n",
      "Epoch: 539, loss: 34.681097321927275\n",
      "Epoch: 540, loss: 34.681097321927275\n",
      "Epoch: 541, loss: 34.681097321927275\n",
      "Epoch: 542, loss: 34.681097321927275\n",
      "Epoch: 543, loss: 34.681097321927275\n",
      "Epoch: 544, loss: 34.681097321927275\n",
      "Epoch: 545, loss: 34.681097321927275\n",
      "Epoch: 546, loss: 34.681097321927275\n",
      "Epoch: 547, loss: 34.681097321927275\n",
      "Epoch: 548, loss: 34.681097321927275\n",
      "Epoch: 549, loss: 34.681097321927275\n",
      "Epoch: 550, loss: 34.681097321927275\n",
      "Epoch: 551, loss: 34.681097321927275\n",
      "Epoch: 552, loss: 34.681097321927275\n",
      "Epoch: 553, loss: 34.681097321927275\n",
      "Epoch: 554, loss: 34.681097321927275\n",
      "Epoch: 555, loss: 34.681097321927275\n",
      "Epoch: 556, loss: 34.681097321927275\n",
      "Epoch: 557, loss: 34.681097321927275\n",
      "Epoch: 558, loss: 34.681097321927275\n",
      "Epoch: 559, loss: 34.681097321927275\n",
      "Epoch: 560, loss: 34.681097321927275\n",
      "Epoch: 561, loss: 34.681097321927275\n",
      "Epoch: 562, loss: 34.681097321927275\n",
      "Epoch: 563, loss: 34.681097321927275\n",
      "Epoch: 564, loss: 34.681097321927275\n",
      "Epoch: 565, loss: 34.681097321927275\n",
      "Epoch: 566, loss: 34.681097321927275\n",
      "Epoch: 567, loss: 34.681097321927275\n",
      "Epoch: 568, loss: 34.681097321927275\n",
      "Epoch: 569, loss: 34.681097321927275\n",
      "Epoch: 570, loss: 34.681097321927275\n",
      "Epoch: 571, loss: 34.681097321927275\n",
      "Epoch: 572, loss: 34.681097321927275\n",
      "Epoch: 573, loss: 34.681097321927275\n",
      "Epoch: 574, loss: 34.681097321927275\n",
      "Epoch: 575, loss: 34.681097321927275\n",
      "Epoch: 576, loss: 34.681097321927275\n",
      "Epoch: 577, loss: 34.681097321927275\n",
      "Epoch: 578, loss: 34.681097321927275\n",
      "Epoch: 579, loss: 34.681097321927275\n",
      "Epoch: 580, loss: 34.681097321927275\n",
      "Epoch: 581, loss: 34.681097321927275\n",
      "Epoch: 582, loss: 34.681097321927275\n",
      "Epoch: 583, loss: 34.681097321927275\n",
      "Epoch: 584, loss: 34.681097321927275\n",
      "Epoch: 585, loss: 34.681097321927275\n",
      "Epoch: 586, loss: 34.681097321927275\n",
      "Epoch: 587, loss: 34.681097321927275\n",
      "Epoch: 588, loss: 34.681097321927275\n",
      "Epoch: 589, loss: 34.681097321927275\n",
      "Epoch: 590, loss: 34.681097321927275\n",
      "Epoch: 591, loss: 34.681097321927275\n",
      "Epoch: 592, loss: 34.681097321927275\n",
      "Epoch: 593, loss: 34.681097321927275\n",
      "Epoch: 594, loss: 34.681097321927275\n",
      "Epoch: 595, loss: 34.681097321927275\n",
      "Epoch: 596, loss: 34.681097321927275\n",
      "Epoch: 597, loss: 34.681097321927275\n",
      "Epoch: 598, loss: 34.681097321927275\n",
      "Epoch: 599, loss: 34.681097321927275\n",
      "Epoch: 600, loss: 34.681097321927275\n",
      "Epoch: 601, loss: 34.681097321927275\n",
      "Epoch: 602, loss: 34.681097321927275\n",
      "Epoch: 603, loss: 34.681097321927275\n",
      "Epoch: 604, loss: 34.681097321927275\n",
      "Epoch: 605, loss: 34.681097321927275\n",
      "Epoch: 606, loss: 34.681097321927275\n",
      "Epoch: 607, loss: 34.681097321927275\n",
      "Epoch: 608, loss: 34.681097321927275\n",
      "Epoch: 609, loss: 34.681097321927275\n",
      "Epoch: 610, loss: 34.681097321927275\n",
      "Epoch: 611, loss: 34.681097321927275\n",
      "Epoch: 612, loss: 34.681097321927275\n",
      "Epoch: 613, loss: 34.681097321927275\n",
      "Epoch: 614, loss: 34.681097321927275\n",
      "Epoch: 615, loss: 34.681097321927275\n",
      "Epoch: 616, loss: 34.681097321927275\n",
      "Epoch: 617, loss: 34.681097321927275\n",
      "Epoch: 618, loss: 34.681097321927275\n",
      "Epoch: 619, loss: 34.681097321927275\n",
      "Epoch: 620, loss: 34.681097321927275\n",
      "Epoch: 621, loss: 34.681097321927275\n",
      "Epoch: 622, loss: 34.681097321927275\n",
      "Epoch: 623, loss: 34.681097321927275\n",
      "Epoch: 624, loss: 34.681097321927275\n",
      "Epoch: 625, loss: 34.681097321927275\n",
      "Epoch: 626, loss: 34.681097321927275\n",
      "Epoch: 627, loss: 34.681097321927275\n",
      "Epoch: 628, loss: 34.681097321927275\n",
      "Epoch: 629, loss: 34.681097321927275\n",
      "Epoch: 630, loss: 34.681097321927275\n",
      "Epoch: 631, loss: 34.681097321927275\n",
      "Epoch: 632, loss: 34.681097321927275\n",
      "Epoch: 633, loss: 34.681097321927275\n",
      "Epoch: 634, loss: 34.681097321927275\n",
      "Epoch: 635, loss: 34.681097321927275\n",
      "Epoch: 636, loss: 34.681097321927275\n",
      "Epoch: 637, loss: 34.681097321927275\n",
      "Epoch: 638, loss: 34.681097321927275\n",
      "Epoch: 639, loss: 34.681097321927275\n",
      "Epoch: 640, loss: 34.681097321927275\n",
      "Epoch: 641, loss: 34.681097321927275\n",
      "Epoch: 642, loss: 34.681097321927275\n",
      "Epoch: 643, loss: 34.681097321927275\n",
      "Epoch: 644, loss: 34.681097321927275\n",
      "Epoch: 645, loss: 34.681097321927275\n",
      "Epoch: 646, loss: 34.681097321927275\n",
      "Epoch: 647, loss: 34.681097321927275\n",
      "Epoch: 648, loss: 34.681097321927275\n",
      "Epoch: 649, loss: 34.681097321927275\n",
      "Epoch: 650, loss: 34.681097321927275\n",
      "Epoch: 651, loss: 34.681097321927275\n",
      "Epoch: 652, loss: 34.681097321927275\n",
      "Epoch: 653, loss: 34.681097321927275\n",
      "Epoch: 654, loss: 34.681097321927275\n",
      "Epoch: 655, loss: 34.681097321927275\n",
      "Epoch: 656, loss: 34.681097321927275\n",
      "Epoch: 657, loss: 34.681097321927275\n",
      "Epoch: 658, loss: 34.681097321927275\n",
      "Epoch: 659, loss: 34.681097321927275\n",
      "Epoch: 660, loss: 34.681097321927275\n",
      "Epoch: 661, loss: 34.681097321927275\n",
      "Epoch: 662, loss: 34.681097321927275\n",
      "Epoch: 663, loss: 34.681097321927275\n",
      "Epoch: 664, loss: 34.681097321927275\n",
      "Epoch: 665, loss: 34.681097321927275\n",
      "Epoch: 666, loss: 34.681097321927275\n",
      "Epoch: 667, loss: 34.681097321927275\n",
      "Epoch: 668, loss: 34.681097321927275\n",
      "Epoch: 669, loss: 34.681097321927275\n",
      "Epoch: 670, loss: 34.681097321927275\n",
      "Epoch: 671, loss: 34.681097321927275\n",
      "Epoch: 672, loss: 34.681097321927275\n",
      "Epoch: 673, loss: 34.681097321927275\n",
      "Epoch: 674, loss: 34.681097321927275\n",
      "Epoch: 675, loss: 34.681097321927275\n",
      "Epoch: 676, loss: 34.681097321927275\n",
      "Epoch: 677, loss: 34.681097321927275\n",
      "Epoch: 678, loss: 34.681097321927275\n",
      "Epoch: 679, loss: 34.681097321927275\n",
      "Epoch: 680, loss: 34.681097321927275\n",
      "Epoch: 681, loss: 34.681097321927275\n",
      "Epoch: 682, loss: 34.681097321927275\n",
      "Epoch: 683, loss: 34.681097321927275\n",
      "Epoch: 684, loss: 34.681097321927275\n",
      "Epoch: 685, loss: 34.681097321927275\n",
      "Epoch: 686, loss: 34.681097321927275\n",
      "Epoch: 687, loss: 34.681097321927275\n",
      "Epoch: 688, loss: 34.681097321927275\n",
      "Epoch: 689, loss: 34.681097321927275\n",
      "Epoch: 690, loss: 34.681097321927275\n",
      "Epoch: 691, loss: 34.681097321927275\n",
      "Epoch: 692, loss: 34.681097321927275\n",
      "Epoch: 693, loss: 34.681097321927275\n",
      "Epoch: 694, loss: 34.681097321927275\n",
      "Epoch: 695, loss: 34.681097321927275\n",
      "Epoch: 696, loss: 34.681097321927275\n",
      "Epoch: 697, loss: 34.681097321927275\n",
      "Epoch: 698, loss: 34.681097321927275\n",
      "Epoch: 699, loss: 34.681097321927275\n",
      "Epoch: 700, loss: 34.681097321927275\n",
      "Epoch: 701, loss: 34.681097321927275\n",
      "Epoch: 702, loss: 34.681097321927275\n",
      "Epoch: 703, loss: 34.681097321927275\n",
      "Epoch: 704, loss: 34.681097321927275\n",
      "Epoch: 705, loss: 34.681097321927275\n",
      "Epoch: 706, loss: 34.681097321927275\n",
      "Epoch: 707, loss: 34.681097321927275\n",
      "Epoch: 708, loss: 34.681097321927275\n",
      "Epoch: 709, loss: 34.681097321927275\n",
      "Epoch: 710, loss: 34.681097321927275\n",
      "Epoch: 711, loss: 34.681097321927275\n",
      "Epoch: 712, loss: 34.681097321927275\n",
      "Epoch: 713, loss: 34.681097321927275\n",
      "Epoch: 714, loss: 34.681097321927275\n",
      "Epoch: 715, loss: 34.681097321927275\n",
      "Epoch: 716, loss: 34.681097321927275\n",
      "Epoch: 717, loss: 34.681097321927275\n",
      "Epoch: 718, loss: 34.681097321927275\n",
      "Epoch: 719, loss: 34.681097321927275\n",
      "Epoch: 720, loss: 34.681097321927275\n",
      "Epoch: 721, loss: 34.681097321927275\n",
      "Epoch: 722, loss: 34.681097321927275\n",
      "Epoch: 723, loss: 34.681097321927275\n",
      "Epoch: 724, loss: 34.681097321927275\n",
      "Epoch: 725, loss: 34.681097321927275\n",
      "Epoch: 726, loss: 34.681097321927275\n",
      "Epoch: 727, loss: 34.681097321927275\n",
      "Epoch: 728, loss: 34.681097321927275\n",
      "Epoch: 729, loss: 34.681097321927275\n",
      "Epoch: 730, loss: 34.681097321927275\n",
      "Epoch: 731, loss: 34.681097321927275\n",
      "Epoch: 732, loss: 34.681097321927275\n",
      "Epoch: 733, loss: 34.681097321927275\n",
      "Epoch: 734, loss: 34.681097321927275\n",
      "Epoch: 735, loss: 34.681097321927275\n",
      "Epoch: 736, loss: 34.681097321927275\n",
      "Epoch: 737, loss: 34.681097321927275\n",
      "Epoch: 738, loss: 34.681097321927275\n",
      "Epoch: 739, loss: 34.681097321927275\n",
      "Epoch: 740, loss: 34.681097321927275\n",
      "Epoch: 741, loss: 34.681097321927275\n",
      "Epoch: 742, loss: 34.681097321927275\n",
      "Epoch: 743, loss: 34.681097321927275\n",
      "Epoch: 744, loss: 34.681097321927275\n",
      "Epoch: 745, loss: 34.681097321927275\n",
      "Epoch: 746, loss: 34.681097321927275\n",
      "Epoch: 747, loss: 34.681097321927275\n",
      "Epoch: 748, loss: 34.681097321927275\n",
      "Epoch: 749, loss: 34.681097321927275\n",
      "Epoch: 750, loss: 34.681097321927275\n",
      "Epoch: 751, loss: 34.681097321927275\n",
      "Epoch: 752, loss: 34.681097321927275\n",
      "Epoch: 753, loss: 34.681097321927275\n",
      "Epoch: 754, loss: 34.681097321927275\n",
      "Epoch: 755, loss: 34.681097321927275\n",
      "Epoch: 756, loss: 34.681097321927275\n",
      "Epoch: 757, loss: 34.681097321927275\n",
      "Epoch: 758, loss: 34.681097321927275\n",
      "Epoch: 759, loss: 34.681097321927275\n",
      "Epoch: 760, loss: 34.681097321927275\n",
      "Epoch: 761, loss: 34.681097321927275\n",
      "Epoch: 762, loss: 34.681097321927275\n",
      "Epoch: 763, loss: 34.681097321927275\n",
      "Epoch: 764, loss: 34.681097321927275\n",
      "Epoch: 765, loss: 34.681097321927275\n",
      "Epoch: 766, loss: 34.681097321927275\n",
      "Epoch: 767, loss: 34.681097321927275\n",
      "Epoch: 768, loss: 34.681097321927275\n",
      "Epoch: 769, loss: 34.681097321927275\n",
      "Epoch: 770, loss: 34.681097321927275\n",
      "Epoch: 771, loss: 34.681097321927275\n",
      "Epoch: 772, loss: 34.681097321927275\n",
      "Epoch: 773, loss: 34.681097321927275\n",
      "Epoch: 774, loss: 34.681097321927275\n",
      "Epoch: 775, loss: 34.681097321927275\n",
      "Epoch: 776, loss: 34.681097321927275\n",
      "Epoch: 777, loss: 34.681097321927275\n",
      "Epoch: 778, loss: 34.681097321927275\n",
      "Epoch: 779, loss: 34.681097321927275\n",
      "Epoch: 780, loss: 34.681097321927275\n",
      "Epoch: 781, loss: 34.681097321927275\n",
      "Epoch: 782, loss: 34.681097321927275\n",
      "Epoch: 783, loss: 34.681097321927275\n",
      "Epoch: 784, loss: 34.681097321927275\n",
      "Epoch: 785, loss: 34.681097321927275\n",
      "Epoch: 786, loss: 34.681097321927275\n",
      "Epoch: 787, loss: 34.681097321927275\n",
      "Epoch: 788, loss: 34.681097321927275\n",
      "Epoch: 789, loss: 34.681097321927275\n",
      "Epoch: 790, loss: 34.681097321927275\n",
      "Epoch: 791, loss: 34.681097321927275\n",
      "Epoch: 792, loss: 34.681097321927275\n",
      "Epoch: 793, loss: 34.681097321927275\n",
      "Epoch: 794, loss: 34.681097321927275\n",
      "Epoch: 795, loss: 34.681097321927275\n",
      "Epoch: 796, loss: 34.681097321927275\n",
      "Epoch: 797, loss: 34.681097321927275\n",
      "Epoch: 798, loss: 34.681097321927275\n",
      "Epoch: 799, loss: 34.681097321927275\n",
      "Epoch: 800, loss: 34.681097321927275\n",
      "Epoch: 801, loss: 34.681097321927275\n",
      "Epoch: 802, loss: 34.681097321927275\n",
      "Epoch: 803, loss: 34.681097321927275\n",
      "Epoch: 804, loss: 34.681097321927275\n",
      "Epoch: 805, loss: 34.681097321927275\n",
      "Epoch: 806, loss: 34.681097321927275\n",
      "Epoch: 807, loss: 34.681097321927275\n",
      "Epoch: 808, loss: 34.681097321927275\n",
      "Epoch: 809, loss: 34.681097321927275\n",
      "Epoch: 810, loss: 34.681097321927275\n",
      "Epoch: 811, loss: 34.681097321927275\n",
      "Epoch: 812, loss: 34.681097321927275\n",
      "Epoch: 813, loss: 34.681097321927275\n",
      "Epoch: 814, loss: 34.681097321927275\n",
      "Epoch: 815, loss: 34.681097321927275\n",
      "Epoch: 816, loss: 34.681097321927275\n",
      "Epoch: 817, loss: 34.681097321927275\n",
      "Epoch: 818, loss: 34.681097321927275\n",
      "Epoch: 819, loss: 34.681097321927275\n",
      "Epoch: 820, loss: 34.681097321927275\n",
      "Epoch: 821, loss: 34.681097321927275\n",
      "Epoch: 822, loss: 34.681097321927275\n",
      "Epoch: 823, loss: 34.681097321927275\n",
      "Epoch: 824, loss: 34.681097321927275\n",
      "Epoch: 825, loss: 34.681097321927275\n",
      "Epoch: 826, loss: 34.681097321927275\n",
      "Epoch: 827, loss: 34.681097321927275\n",
      "Epoch: 828, loss: 34.681097321927275\n",
      "Epoch: 829, loss: 34.681097321927275\n",
      "Epoch: 830, loss: 34.681097321927275\n",
      "Epoch: 831, loss: 34.681097321927275\n",
      "Epoch: 832, loss: 34.681097321927275\n",
      "Epoch: 833, loss: 34.681097321927275\n",
      "Epoch: 834, loss: 34.681097321927275\n",
      "Epoch: 835, loss: 34.681097321927275\n",
      "Epoch: 836, loss: 34.681097321927275\n",
      "Epoch: 837, loss: 34.681097321927275\n",
      "Epoch: 838, loss: 34.681097321927275\n",
      "Epoch: 839, loss: 34.681097321927275\n",
      "Epoch: 840, loss: 34.681097321927275\n",
      "Epoch: 841, loss: 34.681097321927275\n",
      "Epoch: 842, loss: 34.681097321927275\n",
      "Epoch: 843, loss: 34.681097321927275\n",
      "Epoch: 844, loss: 34.681097321927275\n",
      "Epoch: 845, loss: 34.681097321927275\n",
      "Epoch: 846, loss: 34.681097321927275\n",
      "Epoch: 847, loss: 34.681097321927275\n",
      "Epoch: 848, loss: 34.681097321927275\n",
      "Epoch: 849, loss: 34.681097321927275\n",
      "Epoch: 850, loss: 34.681097321927275\n",
      "Epoch: 851, loss: 34.681097321927275\n",
      "Epoch: 852, loss: 34.681097321927275\n",
      "Epoch: 853, loss: 34.681097321927275\n",
      "Epoch: 854, loss: 34.681097321927275\n",
      "Epoch: 855, loss: 34.681097321927275\n",
      "Epoch: 856, loss: 34.681097321927275\n",
      "Epoch: 857, loss: 34.681097321927275\n",
      "Epoch: 858, loss: 34.681097321927275\n",
      "Epoch: 859, loss: 34.681097321927275\n",
      "Epoch: 860, loss: 34.681097321927275\n",
      "Epoch: 861, loss: 34.681097321927275\n",
      "Epoch: 862, loss: 34.681097321927275\n",
      "Epoch: 863, loss: 34.681097321927275\n",
      "Epoch: 864, loss: 34.681097321927275\n",
      "Epoch: 865, loss: 34.681097321927275\n",
      "Epoch: 866, loss: 34.681097321927275\n",
      "Epoch: 867, loss: 34.681097321927275\n",
      "Epoch: 868, loss: 34.681097321927275\n",
      "Epoch: 869, loss: 34.681097321927275\n",
      "Epoch: 870, loss: 34.681097321927275\n",
      "Epoch: 871, loss: 34.681097321927275\n",
      "Epoch: 872, loss: 34.681097321927275\n",
      "Epoch: 873, loss: 34.681097321927275\n",
      "Epoch: 874, loss: 34.681097321927275\n",
      "Epoch: 875, loss: 34.681097321927275\n",
      "Epoch: 876, loss: 34.681097321927275\n",
      "Epoch: 877, loss: 34.681097321927275\n",
      "Epoch: 878, loss: 34.681097321927275\n",
      "Epoch: 879, loss: 34.681097321927275\n",
      "Epoch: 880, loss: 34.681097321927275\n",
      "Epoch: 881, loss: 34.681097321927275\n",
      "Epoch: 882, loss: 34.681097321927275\n",
      "Epoch: 883, loss: 34.681097321927275\n",
      "Epoch: 884, loss: 34.681097321927275\n",
      "Epoch: 885, loss: 34.681097321927275\n",
      "Epoch: 886, loss: 34.681097321927275\n",
      "Epoch: 887, loss: 34.681097321927275\n",
      "Epoch: 888, loss: 34.681097321927275\n",
      "Epoch: 889, loss: 34.681097321927275\n",
      "Epoch: 890, loss: 34.681097321927275\n",
      "Epoch: 891, loss: 34.681097321927275\n",
      "Epoch: 892, loss: 34.681097321927275\n",
      "Epoch: 893, loss: 34.681097321927275\n",
      "Epoch: 894, loss: 34.681097321927275\n",
      "Epoch: 895, loss: 34.681097321927275\n",
      "Epoch: 896, loss: 34.681097321927275\n",
      "Epoch: 897, loss: 34.681097321927275\n",
      "Epoch: 898, loss: 34.681097321927275\n",
      "Epoch: 899, loss: 34.681097321927275\n",
      "Epoch: 900, loss: 34.681097321927275\n",
      "Epoch: 901, loss: 34.681097321927275\n",
      "Epoch: 902, loss: 34.681097321927275\n",
      "Epoch: 903, loss: 34.681097321927275\n",
      "Epoch: 904, loss: 34.681097321927275\n",
      "Epoch: 905, loss: 34.681097321927275\n",
      "Epoch: 906, loss: 34.681097321927275\n",
      "Epoch: 907, loss: 34.681097321927275\n",
      "Epoch: 908, loss: 34.681097321927275\n",
      "Epoch: 909, loss: 34.681097321927275\n",
      "Epoch: 910, loss: 34.681097321927275\n",
      "Epoch: 911, loss: 34.681097321927275\n",
      "Epoch: 912, loss: 34.681097321927275\n",
      "Epoch: 913, loss: 34.681097321927275\n",
      "Epoch: 914, loss: 34.681097321927275\n",
      "Epoch: 915, loss: 34.681097321927275\n",
      "Epoch: 916, loss: 34.681097321927275\n",
      "Epoch: 917, loss: 34.681097321927275\n",
      "Epoch: 918, loss: 34.681097321927275\n",
      "Epoch: 919, loss: 34.681097321927275\n",
      "Epoch: 920, loss: 34.681097321927275\n",
      "Epoch: 921, loss: 34.681097321927275\n",
      "Epoch: 922, loss: 34.681097321927275\n",
      "Epoch: 923, loss: 34.681097321927275\n",
      "Epoch: 924, loss: 34.681097321927275\n",
      "Epoch: 925, loss: 34.681097321927275\n",
      "Epoch: 926, loss: 34.681097321927275\n",
      "Epoch: 927, loss: 34.681097321927275\n",
      "Epoch: 928, loss: 34.681097321927275\n",
      "Epoch: 929, loss: 34.681097321927275\n",
      "Epoch: 930, loss: 34.681097321927275\n",
      "Epoch: 931, loss: 34.681097321927275\n",
      "Epoch: 932, loss: 34.681097321927275\n",
      "Epoch: 933, loss: 34.681097321927275\n",
      "Epoch: 934, loss: 34.681097321927275\n",
      "Epoch: 935, loss: 34.681097321927275\n",
      "Epoch: 936, loss: 34.681097321927275\n",
      "Epoch: 937, loss: 34.681097321927275\n",
      "Epoch: 938, loss: 34.681097321927275\n",
      "Epoch: 939, loss: 34.681097321927275\n",
      "Epoch: 940, loss: 34.681097321927275\n",
      "Epoch: 941, loss: 34.681097321927275\n",
      "Epoch: 942, loss: 34.681097321927275\n",
      "Epoch: 943, loss: 34.681097321927275\n",
      "Epoch: 944, loss: 34.681097321927275\n",
      "Epoch: 945, loss: 34.681097321927275\n",
      "Epoch: 946, loss: 34.681097321927275\n",
      "Epoch: 947, loss: 34.681097321927275\n",
      "Epoch: 948, loss: 34.681097321927275\n",
      "Epoch: 949, loss: 34.681097321927275\n",
      "Epoch: 950, loss: 34.681097321927275\n",
      "Epoch: 951, loss: 34.681097321927275\n",
      "Epoch: 952, loss: 34.681097321927275\n",
      "Epoch: 953, loss: 34.681097321927275\n",
      "Epoch: 954, loss: 34.681097321927275\n",
      "Epoch: 955, loss: 34.681097321927275\n",
      "Epoch: 956, loss: 34.681097321927275\n",
      "Epoch: 957, loss: 34.681097321927275\n",
      "Epoch: 958, loss: 34.681097321927275\n",
      "Epoch: 959, loss: 34.681097321927275\n",
      "Epoch: 960, loss: 34.681097321927275\n",
      "Epoch: 961, loss: 34.681097321927275\n",
      "Epoch: 962, loss: 34.681097321927275\n",
      "Epoch: 963, loss: 34.681097321927275\n",
      "Epoch: 964, loss: 34.681097321927275\n",
      "Epoch: 965, loss: 34.681097321927275\n",
      "Epoch: 966, loss: 34.681097321927275\n",
      "Epoch: 967, loss: 34.681097321927275\n",
      "Epoch: 968, loss: 34.681097321927275\n",
      "Epoch: 969, loss: 34.681097321927275\n",
      "Epoch: 970, loss: 34.681097321927275\n",
      "Epoch: 971, loss: 34.681097321927275\n",
      "Epoch: 972, loss: 34.681097321927275\n",
      "Epoch: 973, loss: 34.681097321927275\n",
      "Epoch: 974, loss: 34.681097321927275\n",
      "Epoch: 975, loss: 34.681097321927275\n",
      "Epoch: 976, loss: 34.681097321927275\n",
      "Epoch: 977, loss: 34.681097321927275\n",
      "Epoch: 978, loss: 34.681097321927275\n",
      "Epoch: 979, loss: 34.681097321927275\n",
      "Epoch: 980, loss: 34.681097321927275\n",
      "Epoch: 981, loss: 34.681097321927275\n",
      "Epoch: 982, loss: 34.681097321927275\n",
      "Epoch: 983, loss: 34.681097321927275\n",
      "Epoch: 984, loss: 34.681097321927275\n",
      "Epoch: 985, loss: 34.681097321927275\n",
      "Epoch: 986, loss: 34.681097321927275\n",
      "Epoch: 987, loss: 34.681097321927275\n",
      "Epoch: 988, loss: 34.681097321927275\n",
      "Epoch: 989, loss: 34.681097321927275\n",
      "Epoch: 990, loss: 34.681097321927275\n",
      "Epoch: 991, loss: 34.681097321927275\n",
      "Epoch: 992, loss: 34.681097321927275\n",
      "Epoch: 993, loss: 34.681097321927275\n",
      "Epoch: 994, loss: 34.681097321927275\n",
      "Epoch: 995, loss: 34.681097321927275\n",
      "Epoch: 996, loss: 34.681097321927275\n",
      "Epoch: 997, loss: 34.681097321927275\n",
      "Epoch: 998, loss: 34.681097321927275\n",
      "Epoch: 999, loss: 34.681097321927275\n",
      "Epoch: 1000, loss: 34.681097321927275\n"
     ]
    }
   ],
   "source": [
    "# initialize weights and biases\n",
    "# in Keras/TensorFlow/PyTorch etc. these are usually randomized in the beginning\n",
    "w1 = 1.5\n",
    "w2 = 0.5\n",
    "w3 = -2\n",
    "w4 = -0.5\n",
    "w5 = 1.5\n",
    "w6 = 1.2\n",
    "bias1 = 0.5\n",
    "bias2 = -0.35\n",
    "bias3 = 0.5\n",
    "\n",
    "# just for comparison after the training\n",
    "original_w1 = w1\n",
    "original_w2 = w2\n",
    "original_w3 = w3\n",
    "original_w4 = w4\n",
    "original_w5 = w5\n",
    "original_w6 = w6\n",
    "original_b1 = bias1\n",
    "original_b2 = bias2\n",
    "original_b3 = bias3\n",
    "\n",
    "# use generated training data from our helper function\n",
    "data = generate_train_data()\n",
    "\n",
    "# learning rate\n",
    "LR = 0.025\n",
    "epochs = 1000\n",
    "\n",
    "# let's initalize a list for loss visualizations\n",
    "loss_points = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # the previous version only measured the loss value\n",
    "    # of the last calculation done in the code (node 3)\n",
    "    # it's probably better to measure the average loss for each epoch\n",
    "    epoch_losses = []\n",
    "\n",
    "    for row in data:\n",
    "        # this is where we do Forward pass + backpropagation\n",
    "        input1 = row[0]\n",
    "        input2 = row[1]\n",
    "        true_value = row[2]\n",
    "\n",
    "        # NODE 1 OUTPUT\n",
    "        node_1_output = input1 * w1 + input2 * w3 + bias1\n",
    "        node_1_output = activation_ReLu(node_1_output)\n",
    "        node_1_output\n",
    "\n",
    "        # NODE 2 OUTPUT\n",
    "        node_2_output = input1 * w2 + input2 * w4 + bias2\n",
    "        node_2_output = activation_ReLu(node_2_output)\n",
    "        node_2_output\n",
    "\n",
    "        # NODE 3 OUTPUT\n",
    "        # we can just use Node 1 and 2 outputs, since they\n",
    "        # already contain the previous weights in their result\n",
    "        node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "        node_3_output = activation_ReLu(node_3_output)\n",
    "        node_3_output\n",
    "\n",
    "        # LOSS FUNCTION - we are going to use MSE -> mean squared error\n",
    "        # MSE formula for LOSS => (predicted_value - true_value) ^ 2\n",
    "        predicted_value = node_3_output\n",
    "        loss = (predicted_value - true_value) ** 2\n",
    "\n",
    "        # add current loss into epoch losses -list\n",
    "        epoch_losses.append(loss)\n",
    "        \n",
    "        # BACKPROPAGATION - LAST LAYER FIRST\n",
    "        # solving the partial derivative of the loss function with respect to w5\n",
    "        deriv_L_w5 = 2 * node_1_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w5 = w5 - LR * deriv_L_w5\n",
    "\n",
    "        deriv_L_w6 = 2 * node_2_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w6 = w6 - LR * deriv_L_w6\n",
    "\n",
    "        deriv_L_b3 = 2 * 1 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_b3 = bias3 - LR * deriv_L_b3\n",
    "\n",
    "        # BACKPROPAGATION - THE FIRST LAYER\n",
    "        # FROM THIS POINT ONWARD WE HAVE TO USE THE MORE COMPLEX VERSION\n",
    "        # OF UPDATING THE VALUES => CHAIN RULE\n",
    "\n",
    "        # weight 1\n",
    "        deriv_L_w1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input1\n",
    "        deriv_L_w1 = deriv_L_w1_left * deriv_L_w1_right\n",
    "        new_w1 = w1 - LR * deriv_L_w1\n",
    "\n",
    "        # weight 2\n",
    "        deriv_L_w2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w2_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w4 + bias2) * input1\n",
    "        deriv_L_w2 = deriv_L_w2_left * deriv_L_w2_right\n",
    "        new_w2 = w2 - LR * deriv_L_w2\n",
    "\n",
    "        # weight 3\n",
    "        deriv_L_w3_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w3_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input2\n",
    "        deriv_L_w3 = deriv_L_w3_left * deriv_L_w3_right\n",
    "        new_w3 = w3 - LR * deriv_L_w3\n",
    "\n",
    "        # weight 4\n",
    "        deriv_L_w4_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w4_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w4 + bias2) * input2\n",
    "        deriv_L_w4 = deriv_L_w4_left * deriv_L_w4_right\n",
    "        new_w4 = w4 - LR * deriv_L_w4\n",
    "\n",
    "        # bias 1\n",
    "        deriv_L_b1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * 1\n",
    "        deriv_L_b1 = deriv_L_b1_left * deriv_L_b1_right\n",
    "        new_b1 = bias1 - LR * deriv_L_b1\n",
    "\n",
    "        # bias 2\n",
    "        deriv_L_b2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b2_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w4 + bias2) * 1\n",
    "        deriv_L_b2 = deriv_L_b2_left * deriv_L_b2_right\n",
    "        new_b2 = bias2 - LR * deriv_L_b2\n",
    "\n",
    "        # ALL DONE! FINALLY UPDATE THE EXISTING WEIGHTS\n",
    "        w1 = new_w1\n",
    "        w2 = new_w2\n",
    "        w3 = new_w3\n",
    "        w4 = new_w4\n",
    "        w5 = new_w5\n",
    "        w6 = new_w6\n",
    "        bias1 = new_b1\n",
    "        bias2 = new_b2\n",
    "        bias3 = new_b3\n",
    "\n",
    "    # calculate average epoch-wise loss and add it to loss points\n",
    "    average_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "\n",
    "    # place the overall epoch loss into the loss_points list\n",
    "    loss_points.append(average_loss)\n",
    "    print(f\"Epoch: {epoch + 1}, loss: {average_loss}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_points)\n",
    "# plt.ylim(-1, 5)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL WEIGHTS AND BIASES\n",
      "w1: 1.5\n",
      "w2: 0.5\n",
      "w3: -2\n",
      "w4: -0.5\n",
      "w5: 1.5\n",
      "w6: 1.2\n",
      "b1: 0.5\n",
      "b2: -0.35\n",
      "b3: 0.5\n",
      "\n",
      "\n",
      "#################################\n",
      "\n",
      "\n",
      "NEW WEIGHTS AND BIASES\n",
      "w1: -4.572127779972796\n",
      "w2: -3.3341779233861617\n",
      "w3: -19.622444971941107\n",
      "w4: -12.550907376368736\n",
      "w5: -50.00537224725678\n",
      "w6: -46.492627142827395\n",
      "b1: -3.631701772385501\n",
      "b2: -3.1435992676123634\n",
      "b3: 12.282967287497813\n"
     ]
    }
   ],
   "source": [
    "print(\"ORIGINAL WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {original_w1}\")\n",
    "print(f\"w2: {original_w2}\")\n",
    "print(f\"w3: {original_w3}\")\n",
    "print(f\"w4: {original_w4}\")\n",
    "print(f\"w5: {original_w5}\")\n",
    "print(f\"w6: {original_w6}\")\n",
    "print(f\"b1: {original_b1}\")\n",
    "print(f\"b2: {original_b2}\")\n",
    "print(f\"b3: {original_b3}\")\n",
    "\n",
    "print(\"\\n\\n#################################\\n\\n\")\n",
    "\n",
    "print(\"NEW WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {new_w1}\")\n",
    "print(f\"w2: {new_w2}\")\n",
    "print(f\"w3: {new_w3}\")\n",
    "print(f\"w4: {new_w4}\")\n",
    "print(f\"w5: {new_w5}\")\n",
    "print(f\"w6: {new_w6}\")\n",
    "print(f\"b1: {new_b1}\")\n",
    "print(f\"b2: {new_b2}\")\n",
    "print(f\"b3: {new_b3}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction function, just doing the forward pass\n",
    "# again (but only that)\n",
    "def predict(x1, x2):\n",
    "    input1 = x1\n",
    "    input2 = x2\n",
    "\n",
    "    # NODE 1 OUTPUT\n",
    "    node_1_output = input1 * w1 + input2 * w3 + bias1\n",
    "    node_1_output = activation_ReLu(node_1_output)\n",
    "\n",
    "    # NODE 2 OUTPUT\n",
    "    node_2_output = input1 * w2 + input2 * w4 + bias2\n",
    "    node_2_output = activation_ReLu(node_2_output)\n",
    "\n",
    "    # NODE 3 OUTPUT\n",
    "    # we can just use Node 1 and 2 outputs, since they\n",
    "    # already contain the previous weights in their result\n",
    "    node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "    node_3_output = activation_ReLu(node_3_output)\n",
    "    return node_3_output\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 9]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.282967287497813"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try using the model with our prediction function\n",
    "# the value tends to be same as final bias 3\n",
    "# so if node1 and node2 outputs are small => more or less bias3\n",
    "result = predict(1, 5)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
