{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network, experimentation tool, version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# activation functions\n",
    "# ReLu is very simple, it filters out all negative values\n",
    "# this is a powerful activation function in reality\n",
    "def activation_ReLu(number):\n",
    "    if number > 0:\n",
    "        return number\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# we also need a derivated version of ReLu\n",
    "# otherwise same as original, but instead of original value, return 1 instead\n",
    "def activation_ReLu_partial_derivative(number):\n",
    "    if number > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lock down the randomness in order to get same results everytime\n",
    "# you can change or disable this if you want\n",
    "np.random.seed(123)\n",
    "\n",
    "def generate_train_data():\n",
    "    result = []\n",
    "\n",
    "    # create 100 numbers\n",
    "    for x in range(100):\n",
    "        n1 = np.random.randint(0, 5)\n",
    "        n2 = np.random.randint(3, 7)\n",
    "\n",
    "        # formula for the target variable: x1 ^^ 2 + x2 + (random integer between 0-5)\n",
    "        # the only point of this is to have some kind of logic in the data\n",
    "        n3 = n1 ** 2 + n2 + np.random.randint(0, 5)\n",
    "        n3 = int(n3)\n",
    "\n",
    "        result.append([n1, n2, n3])\n",
    "\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 0.04988701892795735\n",
      "Epoch: 2, loss: 0.04030225752119891\n",
      "Epoch: 3, loss: 0.03906453256695342\n",
      "Epoch: 4, loss: 0.03782243173726641\n",
      "Epoch: 5, loss: 0.036105293652331664\n",
      "Epoch: 6, loss: 0.034836486403193895\n",
      "Epoch: 7, loss: 0.03353787153624354\n",
      "Epoch: 8, loss: 0.03235671844226724\n",
      "Epoch: 9, loss: 0.031226439827038887\n",
      "Epoch: 10, loss: 0.03010396500350466\n",
      "Epoch: 11, loss: 0.029065803756561884\n",
      "Epoch: 12, loss: 0.028087226091310348\n",
      "Epoch: 13, loss: 0.027155852559874915\n",
      "Epoch: 14, loss: 0.026319004516195545\n",
      "Epoch: 15, loss: 0.02547791727528541\n",
      "Epoch: 16, loss: 0.02467603103044667\n",
      "Epoch: 17, loss: 0.02388754231717982\n",
      "Epoch: 18, loss: 0.023158400892710235\n",
      "Epoch: 19, loss: 0.02243912087680966\n",
      "Epoch: 20, loss: 0.02174534715895894\n",
      "Epoch: 21, loss: 0.02107541439777708\n",
      "Epoch: 22, loss: 0.02042845846183896\n",
      "Epoch: 23, loss: 0.019925660259005516\n",
      "Epoch: 24, loss: 0.019358385601562243\n",
      "Epoch: 25, loss: 0.018814436616906502\n",
      "Epoch: 26, loss: 0.018288463215133166\n",
      "Epoch: 27, loss: 0.01777906582615464\n",
      "Epoch: 28, loss: 0.017303026816849892\n",
      "Epoch: 29, loss: 0.016824673365062593\n",
      "Epoch: 30, loss: 0.016370074245422573\n",
      "Epoch: 31, loss: 0.015948567049108044\n",
      "Epoch: 32, loss: 0.015577930884512927\n",
      "Epoch: 33, loss: 0.015208326981879993\n",
      "Epoch: 34, loss: 0.014866501818099384\n",
      "Epoch: 35, loss: 0.014597882935231789\n",
      "Epoch: 36, loss: 0.014307456521183055\n",
      "Epoch: 37, loss: 0.01403128518231386\n",
      "Epoch: 38, loss: 0.01375786236894768\n",
      "Epoch: 39, loss: 0.013559418402312362\n",
      "Epoch: 40, loss: 0.013310441996825845\n",
      "Epoch: 41, loss: 0.013066268028770884\n",
      "Epoch: 42, loss: 0.012827559737962416\n",
      "Epoch: 43, loss: 0.012594572332215295\n",
      "Epoch: 44, loss: 0.01236735940868437\n",
      "Epoch: 45, loss: 0.012156245359790374\n",
      "Epoch: 46, loss: 0.011943516528628278\n",
      "Epoch: 47, loss: 0.011736323595835438\n",
      "Epoch: 48, loss: 0.011531573328036311\n",
      "Epoch: 49, loss: 0.011335301897054458\n",
      "Epoch: 50, loss: 0.011130825803743733\n",
      "Epoch: 51, loss: 0.010931486200979198\n",
      "Epoch: 52, loss: 0.010737193176940593\n",
      "Epoch: 53, loss: 0.010547912422213977\n",
      "Epoch: 54, loss: 0.010379870275448287\n",
      "Epoch: 55, loss: 0.01019576631142427\n",
      "Epoch: 56, loss: 0.010017664040484209\n",
      "Epoch: 57, loss: 0.009844563973917024\n",
      "Epoch: 58, loss: 0.009676228415972647\n",
      "Epoch: 59, loss: 0.00951258167993816\n",
      "Epoch: 60, loss: 0.00935358498705644\n",
      "Epoch: 61, loss: 0.009199205054917281\n",
      "Epoch: 62, loss: 0.00904940595175862\n",
      "Epoch: 63, loss: 0.008904146985186741\n",
      "Epoch: 64, loss: 0.008743768043880322\n",
      "Epoch: 65, loss: 0.008615802121945241\n",
      "Epoch: 66, loss: 0.008490894170120932\n",
      "Epoch: 67, loss: 0.008369495015227048\n",
      "Epoch: 68, loss: 0.008252864800624898\n",
      "Epoch: 69, loss: 0.008139793936536499\n",
      "Epoch: 70, loss: 0.008034251394191462\n",
      "Epoch: 71, loss: 0.007921615398110048\n",
      "Epoch: 72, loss: 0.007812789151645972\n",
      "Epoch: 73, loss: 0.0077070571548346276\n",
      "Epoch: 74, loss: 0.0076082685675128246\n",
      "Epoch: 75, loss: 0.00749961746583136\n",
      "Epoch: 76, loss: 0.007397393552226707\n",
      "Epoch: 77, loss: 0.007297935653821124\n",
      "Epoch: 78, loss: 0.007200996828255955\n",
      "Epoch: 79, loss: 0.007106514095368732\n",
      "Epoch: 80, loss: 0.007014460876645629\n",
      "Epoch: 81, loss: 0.006924816800095225\n",
      "Epoch: 82, loss: 0.006837561333207978\n",
      "Epoch: 83, loss: 0.006752672450714333\n",
      "Epoch: 84, loss: 0.006670126380260376\n",
      "Epoch: 85, loss: 0.006589897585213869\n",
      "Epoch: 86, loss: 0.006511958805365381\n",
      "Epoch: 87, loss: 0.006436281116782738\n",
      "Epoch: 88, loss: 0.006362834001863027\n",
      "Epoch: 89, loss: 0.006291585426936664\n",
      "Epoch: 90, loss: 0.006222501926124934\n",
      "Epoch: 91, loss: 0.0061555486904550675\n",
      "Epoch: 92, loss: 0.006090689661322984\n",
      "Epoch: 93, loss: 0.006027887627437573\n",
      "Epoch: 94, loss: 0.005967104324417638\n",
      "Epoch: 95, loss: 0.005908300536251799\n",
      "Epoch: 96, loss: 0.005851436197872468\n",
      "Epoch: 97, loss: 0.0057964704981391816\n",
      "Epoch: 98, loss: 0.005743361982571642\n",
      "Epoch: 99, loss: 0.005692068655220876\n",
      "Epoch: 100, loss: 0.005642548079114791\n",
      "Epoch: 101, loss: 0.0055947574747639926\n",
      "Epoch: 102, loss: 0.005548653816263147\n",
      "Epoch: 103, loss: 0.00550419392457256\n",
      "Epoch: 104, loss: 0.00546133455761352\n",
      "Epoch: 105, loss: 0.005420032496859009\n",
      "Epoch: 106, loss: 0.005380244630148043\n",
      "Epoch: 107, loss: 0.005341928030497331\n",
      "Epoch: 108, loss: 0.0053050400307271765\n",
      "Epoch: 109, loss: 0.005269538293760173\n",
      "Epoch: 110, loss: 0.005235380878490325\n",
      "Epoch: 111, loss: 0.005202526301157061\n",
      "Epoch: 112, loss: 0.00517093359219301\n",
      "Epoch: 113, loss: 0.005140562348545998\n",
      "Epoch: 114, loss: 0.00511137278150495\n",
      "Epoch: 115, loss: 0.005083325760085657\n",
      "Epoch: 116, loss: 0.005056382850056266\n",
      "Epoch: 117, loss: 0.005030506348703283\n",
      "Epoch: 118, loss: 0.0050056593154576\n",
      "Epoch: 119, loss: 0.004981805598515956\n",
      "Epoch: 120, loss: 0.0049589098576069204\n",
      "Epoch: 121, loss: 0.004936937583061783\n",
      "Epoch: 122, loss: 0.0049158551113599675\n",
      "Epoch: 123, loss: 0.0048956296373254835\n",
      "Epoch: 124, loss: 0.004876229223156324\n",
      "Epoch: 125, loss: 0.004857622804471963\n",
      "Epoch: 126, loss: 0.004839780193565976\n",
      "Epoch: 127, loss: 0.004822672080051019\n",
      "Epoch: 128, loss: 0.00480627002908251\n",
      "Epoch: 129, loss: 0.004790546477344892\n",
      "Epoch: 130, loss: 0.004775474726981332\n",
      "Epoch: 131, loss: 0.004761028937643341\n",
      "Epoch: 132, loss: 0.004747184116831795\n",
      "Epoch: 133, loss: 0.004733916108695318\n",
      "Epoch: 134, loss: 0.0047212015814455475\n",
      "Epoch: 135, loss: 0.004709018013542399\n",
      "Epoch: 136, loss: 0.004697343678795108\n",
      "Epoch: 137, loss: 0.004686157630517886\n",
      "Epoch: 138, loss: 0.004675439684871292\n",
      "Epoch: 139, loss: 0.004665170403513061\n",
      "Epoch: 140, loss: 0.004655331075674455\n",
      "Epoch: 141, loss: 0.004645903699770687\n",
      "Epoch: 142, loss: 0.004636870964646476\n",
      "Epoch: 143, loss: 0.004628216230550513\n",
      "Epoch: 144, loss: 0.004619923509925353\n",
      "Epoch: 145, loss: 0.004611977448092387\n",
      "Epoch: 146, loss: 0.004604363303904714\n",
      "Epoch: 147, loss: 0.004597066930434332\n",
      "Epoch: 148, loss: 0.004590074755753819\n",
      "Epoch: 149, loss: 0.004583373763866747\n",
      "Epoch: 150, loss: 0.004576951475835458\n",
      "Epoch: 151, loss: 0.004570795931149476\n",
      "Epoch: 152, loss: 0.004564895669372845\n",
      "Epoch: 153, loss: 0.004559239712103921\n",
      "Epoch: 154, loss: 0.004553817545276775\n",
      "Epoch: 155, loss: 0.004548619101829122\n",
      "Epoch: 156, loss: 0.004543634744758067\n",
      "Epoch: 157, loss: 0.00453885525058119\n",
      "Epoch: 158, loss: 0.004534271793217351\n",
      "Epoch: 159, loss: 0.004529875928298557\n",
      "Epoch: 160, loss: 0.004525659577921447\n",
      "Epoch: 161, loss: 0.004521615015844475\n",
      "Epoch: 162, loss: 0.004517734853134534\n",
      "Epoch: 163, loss: 0.004514012024264791\n",
      "Epoch: 164, loss: 0.004510439773663511\n",
      "Epoch: 165, loss: 0.004507011642712085\n",
      "Epoch: 166, loss: 0.004503721457188981\n",
      "Epoch: 167, loss: 0.004500563315154958\n",
      "Epoch: 168, loss: 0.004497531575273784\n",
      "Epoch: 169, loss: 0.00449462084556169\n",
      "Epoch: 170, loss: 0.004491825972557901\n",
      "Epoch: 171, loss: 0.004489142030907789\n",
      "Epoch: 172, loss: 0.004486564313349729\n",
      "Epoch: 173, loss: 0.004484088321095946\n",
      "Epoch: 174, loss: 0.004481709754597512\n",
      "Epoch: 175, loss: 0.004479424504683056\n",
      "Epoch: 176, loss: 0.004477228644060661\n",
      "Epoch: 177, loss: 0.004475118419172129\n",
      "Epoch: 178, loss: 0.004473090242388775\n",
      "Epoch: 179, loss: 0.004471140684537676\n",
      "Epoch: 180, loss: 0.004469266467747566\n",
      "Epoch: 181, loss: 0.004467464458603233\n",
      "Epoch: 182, loss: 0.004465731661597764\n",
      "Epoch: 183, loss: 0.004464065212871683\n",
      "Epoch: 184, loss: 0.0044624623742285175\n",
      "Epoch: 185, loss: 0.004460920527416273\n",
      "Epoch: 186, loss: 0.004459437168664594\n",
      "Epoch: 187, loss: 0.004458009903467564\n",
      "Epoch: 188, loss: 0.004456636441602299\n",
      "Epoch: 189, loss: 0.004455314592373846\n",
      "Epoch: 190, loss: 0.004454042260076996\n",
      "Epoch: 191, loss: 0.0044528174396659785\n",
      "Epoch: 192, loss: 0.004451638212623275\n",
      "Epoch: 193, loss: 0.004450502743018988\n",
      "Epoch: 194, loss: 0.004449409273752588\n",
      "Epoch: 195, loss: 0.004448356122968993\n",
      "Epoch: 196, loss: 0.004447341680641395\n",
      "Epoch: 197, loss: 0.004446364405313287\n",
      "Epoch: 198, loss: 0.00444542282099268\n",
      "Epoch: 199, loss: 0.0044445155141915535\n",
      "Epoch: 200, loss: 0.004443641131103928\n",
      "Epoch: 201, loss: 0.0044427983749162495\n",
      "Epoch: 202, loss: 0.004441986003243986\n",
      "Epoch: 203, loss: 0.004441202825688504\n",
      "Epoch: 204, loss: 0.004440447701508769\n",
      "Epoch: 205, loss: 0.004439719537402342\n",
      "Epoch: 206, loss: 0.00443901728539063\n",
      "Epoch: 207, loss: 0.0044383399408034555\n",
      "Epoch: 208, loss: 0.004437686540358195\n",
      "Epoch: 209, loss: 0.004437056160329043\n",
      "Epoch: 210, loss: 0.004436447914802095\n",
      "Epoch: 211, loss: 0.004435860954012098\n",
      "Epoch: 212, loss: 0.004435294462757054\n",
      "Epoch: 213, loss: 0.004434747658886829\n",
      "Epoch: 214, loss: 0.004434219791862277\n",
      "Epoch: 215, loss: 0.004433710141381487\n",
      "Epoch: 216, loss: 0.0044332180160698495\n",
      "Epoch: 217, loss: 0.004432742752230963\n",
      "Epoch: 218, loss: 0.004432283712655333\n",
      "Epoch: 219, loss: 0.004431840285484148\n",
      "Epoch: 220, loss: 0.004431411883125425\n",
      "Epoch: 221, loss: 0.004430997941220022\n",
      "Epoch: 222, loss: 0.004430597917655102\n",
      "Epoch: 223, loss: 0.004430211291622699\n",
      "Epoch: 224, loss: 0.00442983756272129\n",
      "Epoch: 225, loss: 0.004429476250098268\n",
      "Epoch: 226, loss: 0.004429126891631272\n",
      "Epoch: 227, loss: 0.004428789043146622\n",
      "Epoch: 228, loss: 0.004428462277672974\n",
      "Epoch: 229, loss: 0.004428146184728541\n",
      "Epoch: 230, loss: 0.004427840369640264\n",
      "Epoch: 231, loss: 0.004427544452893438\n",
      "Epoch: 232, loss: 0.004427258069510202\n",
      "Epoch: 233, loss: 0.0044269808684557314\n",
      "Epoch: 234, loss: 0.004426712512070582\n",
      "Epoch: 235, loss: 0.004426452675528141\n",
      "Epoch: 236, loss: 0.004426201046315814\n",
      "Epoch: 237, loss: 0.0044259573237390056\n",
      "Epoch: 238, loss: 0.004425721218446628\n",
      "Epoch: 239, loss: 0.0044254924519773016\n",
      "Epoch: 240, loss: 0.0044252707563251425\n",
      "Epoch: 241, loss: 0.0044250558735242865\n",
      "Epoch: 242, loss: 0.004424847555251264\n",
      "Epoch: 243, loss: 0.004424645562444369\n",
      "Epoch: 244, loss: 0.004424449664939282\n",
      "Epoch: 245, loss: 0.004424259641120136\n",
      "Epoch: 246, loss: 0.004424075277585352\n",
      "Epoch: 247, loss: 0.004423896368827585\n",
      "Epoch: 248, loss: 0.004423722716927038\n",
      "Epoch: 249, loss: 0.004423554131257687\n",
      "Epoch: 250, loss: 0.004423390428205667\n",
      "Epoch: 251, loss: 0.004423231430899402\n",
      "Epoch: 252, loss: 0.004423076968950838\n",
      "Epoch: 253, loss: 0.004422926878207378\n",
      "Epoch: 254, loss: 0.004422781000513984\n",
      "Epoch: 255, loss: 0.00442263918348498\n",
      "Epoch: 256, loss: 0.004422501280285158\n",
      "Epoch: 257, loss: 0.0044223671494197925\n",
      "Epoch: 258, loss: 0.004422236654533091\n",
      "Epoch: 259, loss: 0.0044221096642148\n",
      "Epoch: 260, loss: 0.004421986051814558\n",
      "Epoch: 261, loss: 0.004421865695263653\n",
      "Epoch: 262, loss: 0.004421748476903902\n",
      "Epoch: 263, loss: 0.00442163428332333\n",
      "Epoch: 264, loss: 0.00442152300519831\n",
      "Epoch: 265, loss: 0.00442141453714196\n",
      "Epoch: 266, loss: 0.00442130877755847\n",
      "Epoch: 267, loss: 0.004421205628503153\n",
      "Epoch: 268, loss: 0.004421104995547959\n",
      "Epoch: 269, loss: 0.0044210067876521535\n",
      "Epoch: 270, loss: 0.004420910917038127\n",
      "Epoch: 271, loss: 0.004420817299071873\n",
      "Epoch: 272, loss: 0.004420725852148174\n",
      "Epoch: 273, loss: 0.004420636497580108\n",
      "Epoch: 274, loss: 0.004420549159492864\n",
      "Epoch: 275, loss: 0.004420463764721544\n",
      "Epoch: 276, loss: 0.004420380242712882\n",
      "Epoch: 277, loss: 0.004420298525430692\n",
      "Epoch: 278, loss: 0.004420218547264881\n",
      "Epoch: 279, loss: 0.004420140244943896\n",
      "Epoch: 280, loss: 0.004420063557450468\n",
      "Epoch: 281, loss: 0.004419988425940504\n",
      "Epoch: 282, loss: 0.004419914793665034\n",
      "Epoch: 283, loss: 0.004419842605895044\n",
      "Epoch: 284, loss: 0.00441977180984913\n",
      "Epoch: 285, loss: 0.0044197023546237975\n",
      "Epoch: 286, loss: 0.004419634191126408\n",
      "Epoch: 287, loss: 0.004419567272010528\n",
      "Epoch: 288, loss: 0.004419501551613714\n",
      "Epoch: 289, loss: 0.004419436985897552\n",
      "Epoch: 290, loss: 0.004419373532389917\n",
      "Epoch: 291, loss: 0.0044193111501293295\n",
      "Epoch: 292, loss: 0.004419249799611358\n",
      "Epoch: 293, loss: 0.004419189442736959\n",
      "Epoch: 294, loss: 0.0044191300427627095\n",
      "Epoch: 295, loss: 0.004419071564252844\n",
      "Epoch: 296, loss: 0.0044190139730330095\n",
      "Epoch: 297, loss: 0.004418957236145713\n",
      "Epoch: 298, loss: 0.004418901321807383\n",
      "Epoch: 299, loss: 0.0044188461993669445\n",
      "Epoch: 300, loss: 0.004418791839265923\n",
      "Epoch: 301, loss: 0.004418738212999946\n",
      "Epoch: 302, loss: 0.004418685293081639\n",
      "Epoch: 303, loss: 0.004418633053004859\n",
      "Epoch: 304, loss: 0.004418581467210186\n",
      "Epoch: 305, loss: 0.004418530511051664\n",
      "Epoch: 306, loss: 0.004418480160764698\n",
      "Epoch: 307, loss: 0.004418430393435132\n",
      "Epoch: 308, loss: 0.0044183811869693895\n",
      "Epoch: 309, loss: 0.0044183325200656855\n",
      "Epoch: 310, loss: 0.004418284372186274\n",
      "Epoch: 311, loss: 0.0044182367235306485\n",
      "Epoch: 312, loss: 0.004418189555009706\n",
      "Epoch: 313, loss: 0.004418142848220824\n",
      "Epoch: 314, loss: 0.0044180965854238065\n",
      "Epoch: 315, loss: 0.004418050749517678\n",
      "Epoch: 316, loss: 0.004418005324018312\n",
      "Epoch: 317, loss: 0.004417960293036807\n",
      "Epoch: 318, loss: 0.004417915641258668\n",
      "Epoch: 319, loss: 0.004417871353923682\n",
      "Epoch: 320, loss: 0.004417827416806503\n",
      "Epoch: 321, loss: 0.00441778381619795\n",
      "Epoch: 322, loss: 0.004417740538886903\n",
      "Epoch: 323, loss: 0.004417697572142885\n",
      "Epoch: 324, loss: 0.004417654903699217\n",
      "Epoch: 325, loss: 0.004417612521736767\n",
      "Epoch: 326, loss: 0.004417570414868293\n",
      "Epoch: 327, loss: 0.004417528572123283\n",
      "Epoch: 328, loss: 0.0044174869829333675\n",
      "Epoch: 329, loss: 0.004417445637118213\n",
      "Epoch: 330, loss: 0.004417404524871908\n",
      "Epoch: 331, loss: 0.00441736363674985\n",
      "Epoch: 332, loss: 0.004417322963656038\n",
      "Epoch: 333, loss: 0.004417282496830855\n",
      "Epoch: 334, loss: 0.004417242227839239\n",
      "Epoch: 335, loss: 0.0044172021485593006\n",
      "Epoch: 336, loss: 0.004417162251171286\n",
      "Epoch: 337, loss: 0.004417122528146966\n",
      "Epoch: 338, loss: 0.004417082972239383\n",
      "Epoch: 339, loss: 0.004417043576472926\n",
      "Epoch: 340, loss: 0.004417004334133788\n",
      "Epoch: 341, loss: 0.004416965238760732\n",
      "Epoch: 342, loss: 0.004416926284136175\n",
      "Epoch: 343, loss: 0.004416887464277594\n",
      "Epoch: 344, loss: 0.004416848773429219\n",
      "Epoch: 345, loss: 0.004416810206054001\n",
      "Epoch: 346, loss: 0.004416771756825899\n",
      "Epoch: 347, loss: 0.004416733420622388\n",
      "Epoch: 348, loss: 0.004416695192517249\n",
      "Epoch: 349, loss: 0.004416657067773609\n",
      "Epoch: 350, loss: 0.004416619041837209\n",
      "Epoch: 351, loss: 0.004416581110329925\n",
      "Epoch: 352, loss: 0.00441654326904349\n",
      "Epoch: 353, loss: 0.0044165055139334455\n",
      "Epoch: 354, loss: 0.00441646784111331\n",
      "Epoch: 355, loss: 0.004416430246848932\n",
      "Epoch: 356, loss: 0.004416392727553034\n",
      "Epoch: 357, loss: 0.004416355279779995\n",
      "Epoch: 358, loss: 0.004416317900220727\n",
      "Epoch: 359, loss: 0.004416280585697816\n",
      "Epoch: 360, loss: 0.004416243333160784\n",
      "Epoch: 361, loss: 0.004416206139681503\n",
      "Epoch: 362, loss: 0.004416169002449821\n",
      "Epoch: 363, loss: 0.004416131918769287\n",
      "Epoch: 364, loss: 0.004416094886053043\n",
      "Epoch: 365, loss: 0.00441605790181987\n",
      "Epoch: 366, loss: 0.004416020963690359\n",
      "Epoch: 367, loss: 0.0044159840693832105\n",
      "Epoch: 368, loss: 0.004415947216711679\n",
      "Epoch: 369, loss: 0.004415910403580117\n",
      "Epoch: 370, loss: 0.004415873627980666\n",
      "Epoch: 371, loss: 0.0044158368879900385\n",
      "Epoch: 372, loss: 0.004415800181766435\n",
      "Epoch: 373, loss: 0.004415763507546538\n",
      "Epoch: 374, loss: 0.004415726863642652\n",
      "Epoch: 375, loss: 0.004415690248439888\n",
      "Epoch: 376, loss: 0.004415653660393516\n",
      "Epoch: 377, loss: 0.0044156170980263355\n",
      "Epoch: 378, loss: 0.0044155805599262\n",
      "Epoch: 379, loss: 0.004415544044743587\n",
      "Epoch: 380, loss: 0.004415507551189271\n",
      "Epoch: 381, loss: 0.00441547107803208\n",
      "Epoch: 382, loss: 0.004415434624096715\n",
      "Epoch: 383, loss: 0.004415398188261662\n",
      "Epoch: 384, loss: 0.004415361769457175\n",
      "Epoch: 385, loss: 0.004415325366663309\n",
      "Epoch: 386, loss: 0.004415288978908073\n",
      "Epoch: 387, loss: 0.004415252605265572\n",
      "Epoch: 388, loss: 0.0044152162448542745\n",
      "Epoch: 389, loss: 0.004415179896835336\n",
      "Epoch: 390, loss: 0.004415143560410952\n",
      "Epoch: 391, loss: 0.004415107234822779\n",
      "Epoch: 392, loss: 0.004415070919350447\n",
      "Epoch: 393, loss: 0.004415034613310051\n",
      "Epoch: 394, loss: 0.004414998316052781\n",
      "Epoch: 395, loss: 0.004414962026963546\n",
      "Epoch: 396, loss: 0.004414925745459645\n",
      "Epoch: 397, loss: 0.004414889470989519\n",
      "Epoch: 398, loss: 0.004414853203031527\n",
      "Epoch: 399, loss: 0.0044148169410927526\n",
      "Epoch: 400, loss: 0.004414780684707886\n",
      "Epoch: 401, loss: 0.004414744433438126\n",
      "Epoch: 402, loss: 0.004414708186870108\n",
      "Epoch: 403, loss: 0.0044146719446149065\n",
      "Epoch: 404, loss: 0.004414635706307021\n",
      "Epoch: 405, loss: 0.004414599471603475\n",
      "Epoch: 406, loss: 0.004414563240182865\n",
      "Epoch: 407, loss: 0.004414527011744491\n",
      "Epoch: 408, loss: 0.004414490786007515\n",
      "Epoch: 409, loss: 0.004414454562710146\n",
      "Epoch: 410, loss: 0.004414418341608829\n",
      "Epoch: 411, loss: 0.004414382122477516\n",
      "Epoch: 412, loss: 0.004414345905106904\n",
      "Epoch: 413, loss: 0.004414309689303767\n",
      "Epoch: 414, loss: 0.004414273474890239\n",
      "Epoch: 415, loss: 0.004414237261703169\n",
      "Epoch: 416, loss: 0.0044142010495935155\n",
      "Epoch: 417, loss: 0.004414164838425705\n",
      "Epoch: 418, loss: 0.0044141286280770685\n",
      "Epoch: 419, loss: 0.00441409241843727\n",
      "Epoch: 420, loss: 0.004414056209407758\n",
      "Epoch: 421, loss: 0.004414020000901252\n",
      "Epoch: 422, loss: 0.004413983792841247\n",
      "Epoch: 423, loss: 0.004413947585161496\n",
      "Epoch: 424, loss: 0.004413911377805588\n",
      "Epoch: 425, loss: 0.004413875170726453\n",
      "Epoch: 426, loss: 0.0044138389638859726\n",
      "Epoch: 427, loss: 0.0044138027572545304\n",
      "Epoch: 428, loss: 0.004413766550810634\n",
      "Epoch: 429, loss: 0.004413730344540511\n",
      "Epoch: 430, loss: 0.004413694138437754\n",
      "Epoch: 431, loss: 0.004413657932502952\n",
      "Epoch: 432, loss: 0.004413621726743361\n",
      "Epoch: 433, loss: 0.004413585521172558\n",
      "Epoch: 434, loss: 0.0044135493158101256\n",
      "Epoch: 435, loss: 0.004413513110681372\n",
      "Epoch: 436, loss: 0.004413476905816996\n",
      "Epoch: 437, loss: 0.004413440701252842\n",
      "Epoch: 438, loss: 0.004413404497029605\n",
      "Epoch: 439, loss: 0.004413368293192579\n",
      "Epoch: 440, loss: 0.004413332089791414\n",
      "Epoch: 441, loss: 0.004413295886879863\n",
      "Epoch: 442, loss: 0.004413259684515545\n",
      "Epoch: 443, loss: 0.004413223482759752\n",
      "Epoch: 444, loss: 0.004413187281677211\n",
      "Epoch: 445, loss: 0.004413151081335882\n",
      "Epoch: 446, loss: 0.0044131148818067675\n",
      "Epoch: 447, loss: 0.004413078683163718\n",
      "Epoch: 448, loss: 0.004413042485483267\n",
      "Epoch: 449, loss: 0.004413006288844437\n",
      "Epoch: 450, loss: 0.004412970093328583\n",
      "Epoch: 451, loss: 0.004412933899019227\n",
      "Epoch: 452, loss: 0.004412897706001925\n",
      "Epoch: 453, loss: 0.004412861514364084\n",
      "Epoch: 454, loss: 0.00441282532419486\n",
      "Epoch: 455, loss: 0.004412789135584996\n",
      "Epoch: 456, loss: 0.004412752948626698\n",
      "Epoch: 457, loss: 0.004412716763413542\n",
      "Epoch: 458, loss: 0.004412680580040287\n",
      "Epoch: 459, loss: 0.004412644398602844\n",
      "Epoch: 460, loss: 0.004412608219198115\n",
      "Epoch: 461, loss: 0.004412572041923892\n",
      "Epoch: 462, loss: 0.004412535866878782\n",
      "Epoch: 463, loss: 0.004412499694162094\n",
      "Epoch: 464, loss: 0.004412463523873759\n",
      "Epoch: 465, loss: 0.004412427356114227\n",
      "Epoch: 466, loss: 0.004412391190984404\n",
      "Epoch: 467, loss: 0.004412355028585567\n",
      "Epoch: 468, loss: 0.004412318869019283\n",
      "Epoch: 469, loss: 0.00441228271238734\n",
      "Epoch: 470, loss: 0.0044122465587916795\n",
      "Epoch: 471, loss: 0.0044122104083343374\n",
      "Epoch: 472, loss: 0.004412174261117376\n",
      "Epoch: 473, loss: 0.004412138117242817\n",
      "Epoch: 474, loss: 0.004412101976812606\n",
      "Epoch: 475, loss: 0.004412065839928536\n",
      "Epoch: 476, loss: 0.004412029706692219\n",
      "Epoch: 477, loss: 0.004411993577205018\n",
      "Epoch: 478, loss: 0.0044119574515680115\n",
      "Epoch: 479, loss: 0.004411921329881956\n",
      "Epoch: 480, loss: 0.004411885212247228\n",
      "Epoch: 481, loss: 0.004411849098763802\n",
      "Epoch: 482, loss: 0.004411812989531208\n",
      "Epoch: 483, loss: 0.004411776884648489\n",
      "Epoch: 484, loss: 0.0044117407842141744\n",
      "Epoch: 485, loss: 0.004411704688326261\n",
      "Epoch: 486, loss: 0.0044116685970821586\n",
      "Epoch: 487, loss: 0.004411632510578685\n",
      "Epoch: 488, loss: 0.004411596428912028\n",
      "Epoch: 489, loss: 0.0044115603521777255\n",
      "Epoch: 490, loss: 0.004411524280470645\n",
      "Epoch: 491, loss: 0.004411488213884955\n",
      "Epoch: 492, loss: 0.004411452152514116\n",
      "Epoch: 493, loss: 0.0044114160964508434\n",
      "Epoch: 494, loss: 0.004411380045787118\n",
      "Epoch: 495, loss: 0.004411344000614142\n",
      "Epoch: 496, loss: 0.004411307961022347\n",
      "Epoch: 497, loss: 0.0044112719271013675\n",
      "Epoch: 498, loss: 0.004411235898940025\n",
      "Epoch: 499, loss: 0.004411199876626324\n",
      "Epoch: 500, loss: 0.004411163860247449\n",
      "Epoch: 501, loss: 0.004411127849889739\n",
      "Epoch: 502, loss: 0.0044110918456386865\n",
      "Epoch: 503, loss: 0.004411055847578924\n",
      "Epoch: 504, loss: 0.004411019855794235\n",
      "Epoch: 505, loss: 0.004410983870367526\n",
      "Epoch: 506, loss: 0.004410947891380826\n",
      "Epoch: 507, loss: 0.004410911918915297\n",
      "Epoch: 508, loss: 0.004410875953051208\n",
      "Epoch: 509, loss: 0.004410839993867957\n",
      "Epoch: 510, loss: 0.004410804041444029\n",
      "Epoch: 511, loss: 0.004410768095857049\n",
      "Epoch: 512, loss: 0.0044107321571837214\n",
      "Epoch: 513, loss: 0.00441069622549988\n",
      "Epoch: 514, loss: 0.004410660300880454\n",
      "Epoch: 515, loss: 0.004410624383399483\n",
      "Epoch: 516, loss: 0.004410588473130112\n",
      "Epoch: 517, loss: 0.004410552570144602\n",
      "Epoch: 518, loss: 0.004410516674514311\n",
      "Epoch: 519, loss: 0.004410480786309717\n",
      "Epoch: 520, loss: 0.0044104449056004075\n",
      "Epoch: 521, loss: 0.004410409032455094\n",
      "Epoch: 522, loss: 0.0044103731669415955\n",
      "Epoch: 523, loss: 0.004410337309126863\n",
      "Epoch: 524, loss: 0.0044103014590769745\n",
      "Epoch: 525, loss: 0.00441026561685713\n",
      "Epoch: 526, loss: 0.0044102297825316635\n",
      "Epoch: 527, loss: 0.004410193956164057\n",
      "Epoch: 528, loss: 0.004410158137816931\n",
      "Epoch: 529, loss: 0.004410122327552049\n",
      "Epoch: 530, loss: 0.004410086525430334\n",
      "Epoch: 531, loss: 0.004410050731511861\n",
      "Epoch: 532, loss: 0.004410014945855875\n",
      "Epoch: 533, loss: 0.00440997916852079\n",
      "Epoch: 534, loss: 0.004409943399564188\n",
      "Epoch: 535, loss: 0.004409907639042839\n",
      "Epoch: 536, loss: 0.0044098718870127\n",
      "Epoch: 537, loss: 0.00440983614352892\n",
      "Epoch: 538, loss: 0.004409800408645846\n",
      "Epoch: 539, loss: 0.004409764682417039\n",
      "Epoch: 540, loss: 0.004409728964895266\n",
      "Epoch: 541, loss: 0.004409693256132509\n",
      "Epoch: 542, loss: 0.0044096575561799935\n",
      "Epoch: 543, loss: 0.004409621865088168\n",
      "Epoch: 544, loss: 0.004409586182906716\n",
      "Epoch: 545, loss: 0.0044095505096845775\n",
      "Epoch: 546, loss: 0.004409514845469951\n",
      "Epoch: 547, loss: 0.0044094791903102806\n",
      "Epoch: 548, loss: 0.0044094435442522985\n",
      "Epoch: 549, loss: 0.004409407907341998\n",
      "Epoch: 550, loss: 0.004409372279624667\n",
      "Epoch: 551, loss: 0.004409336661144877\n",
      "Epoch: 552, loss: 0.004409301051946502\n",
      "Epoch: 553, loss: 0.0044092654520727186\n",
      "Epoch: 554, loss: 0.004409229861566015\n",
      "Epoch: 555, loss: 0.004409194280468207\n",
      "Epoch: 556, loss: 0.0044091587088204345\n",
      "Epoch: 557, loss: 0.004409123146663169\n",
      "Epoch: 558, loss: 0.0044090875940362255\n",
      "Epoch: 559, loss: 0.004409052050978772\n",
      "Epoch: 560, loss: 0.004409016517529333\n",
      "Epoch: 561, loss: 0.004408980993725796\n",
      "Epoch: 562, loss: 0.0044089454796054275\n",
      "Epoch: 563, loss: 0.004408909975204855\n",
      "Epoch: 564, loss: 0.0044088744805601135\n",
      "Epoch: 565, loss: 0.004408838995706622\n",
      "Epoch: 566, loss: 0.004408803520679197\n",
      "Epoch: 567, loss: 0.004408768055512071\n",
      "Epoch: 568, loss: 0.004408732600238889\n",
      "Epoch: 569, loss: 0.004408697154892715\n",
      "Epoch: 570, loss: 0.00440866171950605\n",
      "Epoch: 571, loss: 0.00440862629411083\n",
      "Epoch: 572, loss: 0.004408590878738421\n",
      "Epoch: 573, loss: 0.004408555473419662\n",
      "Epoch: 574, loss: 0.004408520078184836\n",
      "Epoch: 575, loss: 0.004408484693063693\n",
      "Epoch: 576, loss: 0.00440844931808545\n",
      "Epoch: 577, loss: 0.004408413953278814\n",
      "Epoch: 578, loss: 0.004408378598671972\n",
      "Epoch: 579, loss: 0.004408343254292594\n",
      "Epoch: 580, loss: 0.004408307920167862\n",
      "Epoch: 581, loss: 0.004408272596324448\n",
      "Epoch: 582, loss: 0.00440823728278855\n",
      "Epoch: 583, loss: 0.004408201979585878\n",
      "Epoch: 584, loss: 0.004408166686741654\n",
      "Epoch: 585, loss: 0.004408131404280662\n",
      "Epoch: 586, loss: 0.004408096132227185\n",
      "Epoch: 587, loss: 0.004408060870605081\n",
      "Epoch: 588, loss: 0.004408025619437741\n",
      "Epoch: 589, loss: 0.004407990378748108\n",
      "Epoch: 590, loss: 0.004407955148558703\n",
      "Epoch: 591, loss: 0.004407919928891604\n",
      "Epoch: 592, loss: 0.004407884719768457\n",
      "Epoch: 593, loss: 0.004407849521210504\n",
      "Epoch: 594, loss: 0.004407814333238554\n",
      "Epoch: 595, loss: 0.0044077791558730225\n",
      "Epoch: 596, loss: 0.004407743989133916\n",
      "Epoch: 597, loss: 0.004407708833040838\n",
      "Epoch: 598, loss: 0.004407673687613009\n",
      "Epoch: 599, loss: 0.0044076385528692644\n",
      "Epoch: 600, loss: 0.004407603428828044\n",
      "Epoch: 601, loss: 0.004407568315507429\n",
      "Epoch: 602, loss: 0.004407533212925127\n",
      "Epoch: 603, loss: 0.004407498121098466\n",
      "Epoch: 604, loss: 0.004407463040044439\n",
      "Epoch: 605, loss: 0.004407427969779661\n",
      "Epoch: 606, loss: 0.004407392910320416\n",
      "Epoch: 607, loss: 0.004407357861682632\n",
      "Epoch: 608, loss: 0.004407322823881905\n",
      "Epoch: 609, loss: 0.004407287796933486\n",
      "Epoch: 610, loss: 0.004407252780852319\n",
      "Epoch: 611, loss: 0.004407217775652997\n",
      "Epoch: 612, loss: 0.0044071827813498094\n",
      "Epoch: 613, loss: 0.004407147797956723\n",
      "Epoch: 614, loss: 0.004407112825487403\n",
      "Epoch: 615, loss: 0.004407077863955199\n",
      "Epoch: 616, loss: 0.004407042913373159\n",
      "Epoch: 617, loss: 0.004407007973754046\n",
      "Epoch: 618, loss: 0.004406973045110316\n",
      "Epoch: 619, loss: 0.004406938127454148\n",
      "Epoch: 620, loss: 0.004406903220797431\n",
      "Epoch: 621, loss: 0.004406868325151772\n",
      "Epoch: 622, loss: 0.004406833440528516\n",
      "Epoch: 623, loss: 0.004406798566938715\n",
      "Epoch: 624, loss: 0.004406763704393175\n",
      "Epoch: 625, loss: 0.004406728852902429\n",
      "Epoch: 626, loss: 0.004406694012476748\n",
      "Epoch: 627, loss: 0.004406659183126155\n",
      "Epoch: 628, loss: 0.004406624364860409\n",
      "Epoch: 629, loss: 0.004406589557689042\n",
      "Epoch: 630, loss: 0.004406554761621323\n",
      "Epoch: 631, loss: 0.004406519976666291\n",
      "Epoch: 632, loss: 0.004406485202832749\n",
      "Epoch: 633, loss: 0.0044064504401292535\n",
      "Epoch: 634, loss: 0.0044064156885641505\n",
      "Epoch: 635, loss: 0.0044063809481455505\n",
      "Epoch: 636, loss: 0.004406346218881339\n",
      "Epoch: 637, loss: 0.004406311500779191\n",
      "Epoch: 638, loss: 0.00440627679384656\n",
      "Epoch: 639, loss: 0.00440624209809069\n",
      "Epoch: 640, loss: 0.004406207413518608\n",
      "Epoch: 641, loss: 0.004406172740137154\n",
      "Epoch: 642, loss: 0.004406138077952943\n",
      "Epoch: 643, loss: 0.004406103426972407\n",
      "Epoch: 644, loss: 0.0044060687872017725\n",
      "Epoch: 645, loss: 0.0044060341586470795\n",
      "Epoch: 646, loss: 0.004405999541314173\n",
      "Epoch: 647, loss: 0.004405964935208717\n",
      "Epoch: 648, loss: 0.004405930340336177\n",
      "Epoch: 649, loss: 0.004405895756701852\n",
      "Epoch: 650, loss: 0.004405861184310859\n",
      "Epoch: 651, loss: 0.004405826623168137\n",
      "Epoch: 652, loss: 0.004405792073278447\n",
      "Epoch: 653, loss: 0.004405757534646388\n",
      "Epoch: 654, loss: 0.004405723007276389\n",
      "Epoch: 655, loss: 0.00440568849117272\n",
      "Epoch: 656, loss: 0.0044056539863394665\n",
      "Epoch: 657, loss: 0.004405619492780581\n",
      "Epoch: 658, loss: 0.004405585010499846\n",
      "Epoch: 659, loss: 0.00440555053950089\n",
      "Epoch: 660, loss: 0.004405516079787188\n",
      "Epoch: 661, loss: 0.0044054816313620696\n",
      "Epoch: 662, loss: 0.00440544719422871\n",
      "Epoch: 663, loss: 0.004405412768390149\n",
      "Epoch: 664, loss: 0.004405378353849278\n",
      "Epoch: 665, loss: 0.0044053439506088424\n",
      "Epoch: 666, loss: 0.004405309558671458\n",
      "Epoch: 667, loss: 0.0044052751780396015\n",
      "Epoch: 668, loss: 0.004405240808715616\n",
      "Epoch: 669, loss: 0.0044052064507017095\n",
      "Epoch: 670, loss: 0.004405172103999966\n",
      "Epoch: 671, loss: 0.004405137768612336\n",
      "Epoch: 672, loss: 0.00440510344454065\n",
      "Epoch: 673, loss: 0.00440506913178661\n",
      "Epoch: 674, loss: 0.0044050348303518025\n",
      "Epoch: 675, loss: 0.004405000540237686\n",
      "Epoch: 676, loss: 0.004404966261445612\n",
      "Epoch: 677, loss: 0.004404931993976804\n",
      "Epoch: 678, loss: 0.004404897737832383\n",
      "Epoch: 679, loss: 0.00440486349301335\n",
      "Epoch: 680, loss: 0.004404829259520601\n",
      "Epoch: 681, loss: 0.004404795037354921\n",
      "Epoch: 682, loss: 0.004404760826516992\n",
      "Epoch: 683, loss: 0.004404726627007381\n",
      "Epoch: 684, loss: 0.004404692438826561\n",
      "Epoch: 685, loss: 0.00440465826197491\n",
      "Epoch: 686, loss: 0.004404624096452682\n",
      "Epoch: 687, loss: 0.00440458994226006\n",
      "Epoch: 688, loss: 0.0044045557993971154\n",
      "Epoch: 689, loss: 0.004404521667863827\n",
      "Epoch: 690, loss: 0.00440448754766008\n",
      "Epoch: 691, loss: 0.00440445343878567\n",
      "Epoch: 692, loss: 0.004404419341240298\n",
      "Epoch: 693, loss: 0.004404385255023587\n",
      "Epoch: 694, loss: 0.004404351180135055\n",
      "Epoch: 695, loss: 0.0044043171165741475\n",
      "Epoch: 696, loss: 0.004404283064340219\n",
      "Epoch: 697, loss: 0.004404249023432549\n",
      "Epoch: 698, loss: 0.004404214993850321\n",
      "Epoch: 699, loss: 0.004404180975592655\n",
      "Epoch: 700, loss: 0.00440414696865857\n",
      "Epoch: 701, loss: 0.0044041129730470335\n",
      "Epoch: 702, loss: 0.004404078988756916\n",
      "Epoch: 703, loss: 0.004404045015787025\n",
      "Epoch: 704, loss: 0.00440401105413608\n",
      "Epoch: 705, loss: 0.004403977103802746\n",
      "Epoch: 706, loss: 0.004403943164785602\n",
      "Epoch: 707, loss: 0.004403909237083165\n",
      "Epoch: 708, loss: 0.004403875320693877\n",
      "Epoch: 709, loss: 0.004403841415616112\n",
      "Epoch: 710, loss: 0.004403807521848183\n",
      "Epoch: 711, loss: 0.004403773639388337\n",
      "Epoch: 712, loss: 0.004403739768234748\n",
      "Epoch: 713, loss: 0.0044037059083855325\n",
      "Epoch: 714, loss: 0.004403672059838747\n",
      "Epoch: 715, loss: 0.004403638222592373\n",
      "Epoch: 716, loss: 0.004403604396644353\n",
      "Epoch: 717, loss: 0.004403570581992554\n",
      "Epoch: 718, loss: 0.0044035367786347925\n",
      "Epoch: 719, loss: 0.0044035029865688215\n",
      "Epoch: 720, loss: 0.004403469205792337\n",
      "Epoch: 721, loss: 0.004403435436302994\n",
      "Epoch: 722, loss: 0.004403401678098373\n",
      "Epoch: 723, loss: 0.00440336793117601\n",
      "Epoch: 724, loss: 0.004403334195533394\n",
      "Epoch: 725, loss: 0.004403300471167953\n",
      "Epoch: 726, loss: 0.004403266758077068\n",
      "Epoch: 727, loss: 0.004403233056258073\n",
      "Epoch: 728, loss: 0.0044031993657082415\n",
      "Epoch: 729, loss: 0.004403165686424813\n",
      "Epoch: 730, loss: 0.004403132018404973\n",
      "Epoch: 731, loss: 0.004403098361645861\n",
      "Epoch: 732, loss: 0.0044030647161445715\n",
      "Epoch: 733, loss: 0.0044030310818981445\n",
      "Epoch: 734, loss: 0.004402997458903593\n",
      "Epoch: 735, loss: 0.004402963847157874\n",
      "Epoch: 736, loss: 0.004402930246657905\n",
      "Epoch: 737, loss: 0.004402896657400559\n",
      "Epoch: 738, loss: 0.004402863079382675\n",
      "Epoch: 739, loss: 0.004402829512601041\n",
      "Epoch: 740, loss: 0.004402795957052415\n",
      "Epoch: 741, loss: 0.004402762412733511\n",
      "Epoch: 742, loss: 0.004402728879641005\n",
      "Epoch: 743, loss: 0.0044026953577715406\n",
      "Epoch: 744, loss: 0.004402661847121706\n",
      "Epoch: 745, loss: 0.004402628347688078\n",
      "Epoch: 746, loss: 0.004402594859467182\n",
      "Epoch: 747, loss: 0.004402561382455508\n",
      "Epoch: 748, loss: 0.004402527916649518\n",
      "Epoch: 749, loss: 0.0044024944620456394\n",
      "Epoch: 750, loss: 0.004402461018640261\n",
      "Epoch: 751, loss: 0.004402427586429741\n",
      "Epoch: 752, loss: 0.0044023941654104115\n",
      "Epoch: 753, loss: 0.004402360755578556\n",
      "Epoch: 754, loss: 0.004402327356930452\n",
      "Epoch: 755, loss: 0.004402293969462328\n",
      "Epoch: 756, loss: 0.004402260593170383\n",
      "Epoch: 757, loss: 0.004402227228050801\n",
      "Epoch: 758, loss: 0.00440219387409972\n",
      "Epoch: 759, loss: 0.00440216053131326\n",
      "Epoch: 760, loss: 0.004402127199687506\n",
      "Epoch: 761, loss: 0.004402093879218527\n",
      "Epoch: 762, loss: 0.004402060569902352\n",
      "Epoch: 763, loss: 0.004402027271734988\n",
      "Epoch: 764, loss: 0.004401993984712424\n",
      "Epoch: 765, loss: 0.004401960708830618\n",
      "Epoch: 766, loss: 0.004401927444085495\n",
      "Epoch: 767, loss: 0.004401894190472971\n",
      "Epoch: 768, loss: 0.004401860947988925\n",
      "Epoch: 769, loss: 0.004401827716629217\n",
      "Epoch: 770, loss: 0.004401794496389687\n",
      "Epoch: 771, loss: 0.004401761287266149\n",
      "Epoch: 772, loss: 0.004401728089254398\n",
      "Epoch: 773, loss: 0.004401694902350195\n",
      "Epoch: 774, loss: 0.004401661726549299\n",
      "Epoch: 775, loss: 0.004401628561847436\n",
      "Epoch: 776, loss: 0.004401595408240307\n",
      "Epoch: 777, loss: 0.004401562265723607\n",
      "Epoch: 778, loss: 0.004401529134292997\n",
      "Epoch: 779, loss: 0.004401496013944132\n",
      "Epoch: 780, loss: 0.004401462904672632\n",
      "Epoch: 781, loss: 0.004401429806474113\n",
      "Epoch: 782, loss: 0.004401396719344164\n",
      "Epoch: 783, loss: 0.004401363643278356\n",
      "Epoch: 784, loss: 0.004401330578272246\n",
      "Epoch: 785, loss: 0.004401297524321374\n",
      "Epoch: 786, loss: 0.004401264481421265\n",
      "Epoch: 787, loss: 0.00440123144956741\n",
      "Epoch: 788, loss: 0.004401198428755308\n",
      "Epoch: 789, loss: 0.004401165418980432\n",
      "Epoch: 790, loss: 0.004401132420238233\n",
      "Epoch: 791, loss: 0.004401099432524157\n",
      "Epoch: 792, loss: 0.0044010664558336255\n",
      "Epoch: 793, loss: 0.004401033490162052\n",
      "Epoch: 794, loss: 0.004401000535504828\n",
      "Epoch: 795, loss: 0.004400967591857341\n",
      "Epoch: 796, loss: 0.004400934659214961\n",
      "Epoch: 797, loss: 0.004400901737573038\n",
      "Epoch: 798, loss: 0.004400868826926913\n",
      "Epoch: 799, loss: 0.00440083592727192\n",
      "Epoch: 800, loss: 0.004400803038603368\n",
      "Epoch: 801, loss: 0.004400770160916565\n",
      "Epoch: 802, loss: 0.004400737294206796\n",
      "Epoch: 803, loss: 0.004400704438469344\n",
      "Epoch: 804, loss: 0.004400671593699473\n",
      "Epoch: 805, loss: 0.004400638759892442\n",
      "Epoch: 806, loss: 0.0044006059370434916\n",
      "Epoch: 807, loss: 0.004400573125147854\n",
      "Epoch: 808, loss: 0.004400540324200748\n",
      "Epoch: 809, loss: 0.004400507534197395\n",
      "Epoch: 810, loss: 0.004400474755132985\n",
      "Epoch: 811, loss: 0.004400441987002714\n",
      "Epoch: 812, loss: 0.004400409229801768\n",
      "Epoch: 813, loss: 0.004400376483525306\n",
      "Epoch: 814, loss: 0.004400343748168497\n",
      "Epoch: 815, loss: 0.004400311023726495\n",
      "Epoch: 816, loss: 0.004400278310194437\n",
      "Epoch: 817, loss: 0.004400245607567462\n",
      "Epoch: 818, loss: 0.0044002129158406955\n",
      "Epoch: 819, loss: 0.004400180235009252\n",
      "Epoch: 820, loss: 0.004400147565068242\n",
      "Epoch: 821, loss: 0.004400114906012771\n",
      "Epoch: 822, loss: 0.004400082257837921\n",
      "Epoch: 823, loss: 0.0044000496205387875\n",
      "Epoch: 824, loss: 0.00440001699411044\n",
      "Epoch: 825, loss: 0.004399984378547955\n",
      "Epoch: 826, loss: 0.004399951773846395\n",
      "Epoch: 827, loss: 0.004399919180000812\n",
      "Epoch: 828, loss: 0.0043998865970062626\n",
      "Epoch: 829, loss: 0.004399854024857776\n",
      "Epoch: 830, loss: 0.004399821463550404\n",
      "Epoch: 831, loss: 0.004399788913079166\n",
      "Epoch: 832, loss: 0.004399756373439087\n",
      "Epoch: 833, loss: 0.004399723844625187\n",
      "Epoch: 834, loss: 0.004399691326632474\n",
      "Epoch: 835, loss: 0.004399658819455959\n",
      "Epoch: 836, loss: 0.004399626323090638\n",
      "Epoch: 837, loss: 0.004399593837531507\n",
      "Epoch: 838, loss: 0.004399561362773553\n",
      "Epoch: 839, loss: 0.004399528898811769\n",
      "Epoch: 840, loss: 0.004399496445641121\n",
      "Epoch: 841, loss: 0.004399464003256599\n",
      "Epoch: 842, loss: 0.004399431571653157\n",
      "Epoch: 843, loss: 0.004399399150825772\n",
      "Epoch: 844, loss: 0.0043993667407693955\n",
      "Epoch: 845, loss: 0.0043993343414789905\n",
      "Epoch: 846, loss: 0.004399301952949506\n",
      "Epoch: 847, loss: 0.0043992695751758955\n",
      "Epoch: 848, loss: 0.004399237208153095\n",
      "Epoch: 849, loss: 0.004399204851876049\n",
      "Epoch: 850, loss: 0.004399172506339692\n",
      "Epoch: 851, loss: 0.004399140171538957\n",
      "Epoch: 852, loss: 0.004399107847468778\n",
      "Epoch: 853, loss: 0.004399075534124075\n",
      "Epoch: 854, loss: 0.00439904323149977\n",
      "Epoch: 855, loss: 0.004399010939590786\n",
      "Epoch: 856, loss: 0.004398978658392042\n",
      "Epoch: 857, loss: 0.00439894638789844\n",
      "Epoch: 858, loss: 0.004398914128104905\n",
      "Epoch: 859, loss: 0.004398881879006337\n",
      "Epoch: 860, loss: 0.00439884964059764\n",
      "Epoch: 861, loss: 0.00439881741287372\n",
      "Epoch: 862, loss: 0.004398785195829478\n",
      "Epoch: 863, loss: 0.004398752989459809\n",
      "Epoch: 864, loss: 0.0043987207937596084\n",
      "Epoch: 865, loss: 0.004398688608723771\n",
      "Epoch: 866, loss: 0.004398656434347187\n",
      "Epoch: 867, loss: 0.004398624270624745\n",
      "Epoch: 868, loss: 0.004398592117551336\n",
      "Epoch: 869, loss: 0.004398559975121842\n",
      "Epoch: 870, loss: 0.0043985278433311455\n",
      "Epoch: 871, loss: 0.0043984957221741335\n",
      "Epoch: 872, loss: 0.0043984636116456855\n",
      "Epoch: 873, loss: 0.004398431511740673\n",
      "Epoch: 874, loss: 0.0043983994224539875\n",
      "Epoch: 875, loss: 0.004398367343780491\n",
      "Epoch: 876, loss: 0.00439833527571507\n",
      "Epoch: 877, loss: 0.004398303218252591\n",
      "Epoch: 878, loss: 0.004398271171387928\n",
      "Epoch: 879, loss: 0.004398239135115955\n",
      "Epoch: 880, loss: 0.00439820710943154\n",
      "Epoch: 881, loss: 0.004398175094329555\n",
      "Epoch: 882, loss: 0.004398143089804869\n",
      "Epoch: 883, loss: 0.004398111095852352\n",
      "Epoch: 884, loss: 0.004398079112466866\n",
      "Epoch: 885, loss: 0.004398047139643284\n",
      "Epoch: 886, loss: 0.00439801517737647\n",
      "Epoch: 887, loss: 0.004397983225661287\n",
      "Epoch: 888, loss: 0.004397951284492607\n",
      "Epoch: 889, loss: 0.004397919353865293\n",
      "Epoch: 890, loss: 0.004397887433774203\n",
      "Epoch: 891, loss: 0.004397855524214209\n",
      "Epoch: 892, loss: 0.004397823625180173\n",
      "Epoch: 893, loss: 0.004397791736666961\n",
      "Epoch: 894, loss: 0.004397759858669437\n",
      "Epoch: 895, loss: 0.004397727991182465\n",
      "Epoch: 896, loss: 0.004397696134200903\n",
      "Epoch: 897, loss: 0.004397664287719622\n",
      "Epoch: 898, loss: 0.004397632451733488\n",
      "Epoch: 899, loss: 0.004397600626237363\n",
      "Epoch: 900, loss: 0.00439756881122611\n",
      "Epoch: 901, loss: 0.004397537006694595\n",
      "Epoch: 902, loss: 0.0043975052126376864\n",
      "Epoch: 903, loss: 0.004397473429050251\n",
      "Epoch: 904, loss: 0.004397441655927147\n",
      "Epoch: 905, loss: 0.004397409893263252\n",
      "Epoch: 906, loss: 0.004397378141053427\n",
      "Epoch: 907, loss: 0.004397346399292539\n",
      "Epoch: 908, loss: 0.004397314667975461\n",
      "Epoch: 909, loss: 0.00439728294709706\n",
      "Epoch: 910, loss: 0.004397251236652206\n",
      "Epoch: 911, loss: 0.004397219536635769\n",
      "Epoch: 912, loss: 0.004397187847042623\n",
      "Epoch: 913, loss: 0.004397156167867638\n",
      "Epoch: 914, loss: 0.0043971244991056905\n",
      "Epoch: 915, loss: 0.004397092840751648\n",
      "Epoch: 916, loss: 0.0043970611928003936\n",
      "Epoch: 917, loss: 0.004397029555246799\n",
      "Epoch: 918, loss: 0.004396997928085748\n",
      "Epoch: 919, loss: 0.004396966311312106\n",
      "Epoch: 920, loss: 0.004396934704920769\n",
      "Epoch: 921, loss: 0.004396903108906605\n",
      "Epoch: 922, loss: 0.004396871523264499\n",
      "Epoch: 923, loss: 0.004396839947989329\n",
      "Epoch: 924, loss: 0.004396808383075993\n",
      "Epoch: 925, loss: 0.004396776828519363\n",
      "Epoch: 926, loss: 0.004396745284314332\n",
      "Epoch: 927, loss: 0.0043967137504557895\n",
      "Epoch: 928, loss: 0.004396682226938622\n",
      "Epoch: 929, loss: 0.004396650713757724\n",
      "Epoch: 930, loss: 0.004396619210907979\n",
      "Epoch: 931, loss: 0.004396587718384291\n",
      "Epoch: 932, loss: 0.004396556236181551\n",
      "Epoch: 933, loss: 0.004396524764294653\n",
      "Epoch: 934, loss: 0.004396493302718503\n",
      "Epoch: 935, loss: 0.0043964618514479945\n",
      "Epoch: 936, loss: 0.0043964304104780316\n",
      "Epoch: 937, loss: 0.004396398979803519\n",
      "Epoch: 938, loss: 0.004396367559419359\n",
      "Epoch: 939, loss: 0.004396336149320462\n",
      "Epoch: 940, loss: 0.004396304749501728\n",
      "Epoch: 941, loss: 0.004396273359958078\n",
      "Epoch: 942, loss: 0.004396241980684416\n",
      "Epoch: 943, loss: 0.004396210611675661\n",
      "Epoch: 944, loss: 0.004396179252926727\n",
      "Epoch: 945, loss: 0.004396147904432529\n",
      "Epoch: 946, loss: 0.004396116566187991\n",
      "Epoch: 947, loss: 0.004396085238188028\n",
      "Epoch: 948, loss: 0.004396053920427566\n",
      "Epoch: 949, loss: 0.004396022612901531\n",
      "Epoch: 950, loss: 0.004395991315604849\n",
      "Epoch: 951, loss: 0.00439596002853245\n",
      "Epoch: 952, loss: 0.00439592875167926\n",
      "Epoch: 953, loss: 0.00439589748504022\n",
      "Epoch: 954, loss: 0.004395866228610261\n",
      "Epoch: 955, loss: 0.004395834982384314\n",
      "Epoch: 956, loss: 0.00439580374635733\n",
      "Epoch: 957, loss: 0.004395772520524242\n",
      "Epoch: 958, loss: 0.004395741304879994\n",
      "Epoch: 959, loss: 0.004395710099419535\n",
      "Epoch: 960, loss: 0.00439567890413781\n",
      "Epoch: 961, loss: 0.004395647719029766\n",
      "Epoch: 962, loss: 0.0043956165440903585\n",
      "Epoch: 963, loss: 0.004395585379314544\n",
      "Epoch: 964, loss: 0.004395554224697277\n",
      "Epoch: 965, loss: 0.004395523080233512\n",
      "Epoch: 966, loss: 0.004395491945918215\n",
      "Epoch: 967, loss: 0.004395460821746345\n",
      "Epoch: 968, loss: 0.0043954297077128685\n",
      "Epoch: 969, loss: 0.004395398603812757\n",
      "Epoch: 970, loss: 0.0043953675100409755\n",
      "Epoch: 971, loss: 0.004395336426392497\n",
      "Epoch: 972, loss: 0.0043953053528623\n",
      "Epoch: 973, loss: 0.004395274289445362\n",
      "Epoch: 974, loss: 0.0043952432361366554\n",
      "Epoch: 975, loss: 0.0043952121929311675\n",
      "Epoch: 976, loss: 0.004395181159823878\n",
      "Epoch: 977, loss: 0.004395150136809779\n",
      "Epoch: 978, loss: 0.004395119123883855\n",
      "Epoch: 979, loss: 0.004395088121041099\n",
      "Epoch: 980, loss: 0.0043950571282765066\n",
      "Epoch: 981, loss: 0.0043950261455850724\n",
      "Epoch: 982, loss: 0.004394995172961794\n",
      "Epoch: 983, loss: 0.004394964210401677\n",
      "Epoch: 984, loss: 0.00439493325789972\n",
      "Epoch: 985, loss: 0.004394902315450933\n",
      "Epoch: 986, loss: 0.004394871383050321\n",
      "Epoch: 987, loss: 0.004394840460692901\n",
      "Epoch: 988, loss: 0.004394809548373678\n",
      "Epoch: 989, loss: 0.004394778646087683\n",
      "Epoch: 990, loss: 0.004394747753829918\n",
      "Epoch: 991, loss: 0.004394716871595413\n",
      "Epoch: 992, loss: 0.0043946859993791955\n",
      "Epoch: 993, loss: 0.004394655137176282\n",
      "Epoch: 994, loss: 0.004394624284981712\n",
      "Epoch: 995, loss: 0.004394593442790512\n",
      "Epoch: 996, loss: 0.004394562610597714\n",
      "Epoch: 997, loss: 0.004394531788398363\n",
      "Epoch: 998, loss: 0.004394500976187495\n",
      "Epoch: 999, loss: 0.00439447017396015\n",
      "Epoch: 1000, loss: 0.004394439381711376\n"
     ]
    }
   ],
   "source": [
    "# initialize weights and biases\n",
    "# in Keras/TensorFlow/PyTorch etc. these are usually randomized in the beginning\n",
    "w1 = 1.5\n",
    "w2 = 0.5\n",
    "w3 = -2\n",
    "w4 = -0.5\n",
    "w5 = 1.5\n",
    "w6 = 1.2\n",
    "bias1 = 0.5\n",
    "bias2 = -0.35\n",
    "bias3 = 0.5\n",
    "\n",
    "# just for comparison after the training\n",
    "original_w1 = w1\n",
    "original_w2 = w2\n",
    "original_w3 = w3\n",
    "original_w4 = w4\n",
    "original_w5 = w5\n",
    "original_w6 = w6\n",
    "original_b1 = bias1\n",
    "original_b2 = bias2\n",
    "original_b3 = bias3\n",
    "\n",
    "# use generated training data from our helper function\n",
    "data = generate_train_data()\n",
    "df = pd.DataFrame(data, columns=[\"x1\", \"x2\", \"y\"])\n",
    "\n",
    "# DataFrame values as a list\n",
    "data = list(df.values)\n",
    "\n",
    "# use min/max -scaling to make values in the range 0-1\n",
    "data = (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "# learning rate\n",
    "LR = 0.025\n",
    "epochs = 1000\n",
    "\n",
    "# let's initalize a list for loss visualizations\n",
    "loss_points = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # the previous version only measured the loss value\n",
    "    # of the last calculation done in the code (node 3)\n",
    "    # it's probably better to measure the average loss for each epoch\n",
    "    epoch_losses = []\n",
    "\n",
    "    for row in data:\n",
    "        # this is where we do Forward pass + backpropagation\n",
    "        input1 = row[0]\n",
    "        input2 = row[1]\n",
    "        true_value = row[2]\n",
    "\n",
    "        # NODE 1 OUTPUT\n",
    "        node_1_output = input1 * w1 + input2 * w3 + bias1\n",
    "        node_1_output = activation_ReLu(node_1_output)\n",
    "        node_1_output\n",
    "\n",
    "        # NODE 2 OUTPUT\n",
    "        node_2_output = input1 * w2 + input2 * w4 + bias2\n",
    "        node_2_output = activation_ReLu(node_2_output)\n",
    "        node_2_output\n",
    "\n",
    "        # NODE 3 OUTPUT\n",
    "        # we can just use Node 1 and 2 outputs, since they\n",
    "        # already contain the previous weights in their result\n",
    "        node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "        node_3_output = activation_ReLu(node_3_output)\n",
    "        node_3_output\n",
    "\n",
    "        # LOSS FUNCTION - we are going to use MSE -> mean squared error\n",
    "        # MSE formula for LOSS => (predicted_value - true_value) ^ 2\n",
    "        predicted_value = node_3_output\n",
    "        loss = (predicted_value - true_value) ** 2\n",
    "\n",
    "        # add current loss into epoch losses -list\n",
    "        epoch_losses.append(loss)\n",
    "        \n",
    "        # BACKPROPAGATION - LAST LAYER FIRST\n",
    "        # solving the partial derivative of the loss function with respect to w5\n",
    "        deriv_L_w5 = 2 * node_1_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w5 = w5 - LR * deriv_L_w5\n",
    "\n",
    "        deriv_L_w6 = 2 * node_2_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w6 = w6 - LR * deriv_L_w6\n",
    "\n",
    "        deriv_L_b3 = 2 * 1 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_b3 = bias3 - LR * deriv_L_b3\n",
    "\n",
    "        # BACKPROPAGATION - THE FIRST LAYER\n",
    "        # FROM THIS POINT ONWARD WE HAVE TO USE THE MORE COMPLEX VERSION\n",
    "        # OF UPDATING THE VALUES => CHAIN RULE\n",
    "\n",
    "        # weight 1\n",
    "        deriv_L_w1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input1\n",
    "        deriv_L_w1 = deriv_L_w1_left * deriv_L_w1_right\n",
    "        new_w1 = w1 - LR * deriv_L_w1\n",
    "\n",
    "        deriv_L_w2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w2_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input1\n",
    "        deriv_L_w2 = deriv_L_w2_left * deriv_L_w2_right\n",
    "        new_w2 = w2 - LR * deriv_L_w2\n",
    "\n",
    "        deriv_L_w3_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w3_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input2\n",
    "        deriv_L_w3 = deriv_L_w3_left * deriv_L_w3_right\n",
    "        new_w3 = w3 - LR * deriv_L_w3\n",
    "\n",
    "        deriv_L_w4_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w4_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input2\n",
    "        deriv_L_w4 = deriv_L_w4_left * deriv_L_w4_right\n",
    "        new_w4 = w4 - LR * deriv_L_w4\n",
    "\n",
    "        deriv_L_b1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * 1\n",
    "        deriv_L_b1 = deriv_L_b1_left * deriv_L_b1_right\n",
    "        new_b1 = bias1 - LR * deriv_L_b1\n",
    "\n",
    "        deriv_L_b2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b2_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * 1\n",
    "        deriv_L_b2 = deriv_L_b2_left * deriv_L_b2_right\n",
    "        new_b2 = bias2 - LR * deriv_L_b2\n",
    "\n",
    "        # ALL DONE! FINALLY UPDATE THE EXISTING WEIGHTS\n",
    "        w1 = new_w1\n",
    "        w2 = new_w2\n",
    "        w3 = new_w3\n",
    "        w4 = new_w4\n",
    "        w5 = new_w5\n",
    "        w6 = new_w6\n",
    "        bias1 = new_b1\n",
    "        bias2 = new_b2\n",
    "        bias3 = new_b3\n",
    "\n",
    "    # calculate average epoch-wise loss and add it to loss points\n",
    "    average_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "\n",
    "    # place the overall epoch loss into the loss_points list\n",
    "    loss_points.append(average_loss)\n",
    "    print(f\"Epoch: {epoch + 1}, loss: {average_loss}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_points)\n",
    "# plt.ylim(-1, 5)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL WEIGHTS AND BIASES\n",
      "w1: 1.5\n",
      "w2: 0.5\n",
      "w3: -2\n",
      "w4: -0.5\n",
      "w5: 1.5\n",
      "w6: 1.2\n",
      "b1: 0.5\n",
      "b2: -0.35\n",
      "b3: 0.5\n",
      "\n",
      "\n",
      "#################################\n",
      "\n",
      "\n",
      "NEW WEIGHTS AND BIASES\n",
      "w1: 3.1585035189006563\n",
      "w2: 1.7704530008879307\n",
      "w3: 0.4005156324024293\n",
      "w4: 1.2636637793038958\n",
      "w5: 1.8784463175988217\n",
      "w6: 1.2\n",
      "b1: -0.2534089771501538\n",
      "b2: -0.8721611139065493\n",
      "b3: 0.2737953732582682\n"
     ]
    }
   ],
   "source": [
    "print(\"ORIGINAL WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {original_w1}\")\n",
    "print(f\"w2: {original_w2}\")\n",
    "print(f\"w3: {original_w3}\")\n",
    "print(f\"w4: {original_w4}\")\n",
    "print(f\"w5: {original_w5}\")\n",
    "print(f\"w6: {original_w6}\")\n",
    "print(f\"b1: {original_b1}\")\n",
    "print(f\"b2: {original_b2}\")\n",
    "print(f\"b3: {original_b3}\")\n",
    "\n",
    "print(\"\\n\\n#################################\\n\\n\")\n",
    "\n",
    "print(\"NEW WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {new_w1}\")\n",
    "print(f\"w2: {new_w2}\")\n",
    "print(f\"w3: {new_w3}\")\n",
    "print(f\"w4: {new_w4}\")\n",
    "print(f\"w5: {new_w5}\")\n",
    "print(f\"w6: {new_w6}\")\n",
    "print(f\"b1: {new_b1}\")\n",
    "print(f\"b2: {new_b2}\")\n",
    "print(f\"b3: {new_b3}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction function, just doing the forward pass\n",
    "# again (but only that)\n",
    "def predict(x1, x2):\n",
    "    input1 = x1\n",
    "    input2 = x2\n",
    "\n",
    "    # NODE 1 OUTPUT\n",
    "    node_1_output = input1 * w1 + input2 * w3 + bias1\n",
    "    node_1_output = activation_ReLu(node_1_output)\n",
    "\n",
    "    # NODE 2 OUTPUT\n",
    "    node_2_output = input1 * w2 + input2 * w4 + bias2\n",
    "    node_2_output = activation_ReLu(node_2_output)\n",
    "\n",
    "    # NODE 3 OUTPUT\n",
    "    # we can just use Node 1 and 2 outputs, since they\n",
    "    # already contain the previous weights in their result\n",
    "    node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "    node_3_output = activation_ReLu(node_3_output)\n",
    "    return node_3_output\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11538462, 0.19230769, 0.65384615])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(16.9999999)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['y'].max() * 0.65384615"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.6270484499598392)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try using the model with our prediction function\n",
    "# the value tends to be same as final bias 3\n",
    "# so if node1 and node2 outputs are small => more or less bias3\n",
    "result = predict(0.11538462, 0.19230769)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(16.30325969895582)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['y'].max() * result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
