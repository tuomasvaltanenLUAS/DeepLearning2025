{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A little bit more complex transformer + training code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code originally from:\n",
    "https://medium.com/data-science/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\tuomas.valtanen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\tuomas.valtanen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\tuomas.valtanen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\tuomas.valtanen\\AppData\\Local\\Temp\\ipykernel_3216\\2730290085.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\torch\\__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\torch\\functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "c:\\Users\\tuomas.valtanen\\DL2025lecturenotes2\\DeepLearning2025\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "# pip install torch==2.2.1\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 8.679659843444824\n",
      "Epoch: 2, Loss: 8.5496826171875\n",
      "Epoch: 3, Loss: 8.47811508178711\n",
      "Epoch: 4, Loss: 8.425033569335938\n",
      "Epoch: 5, Loss: 8.36827278137207\n",
      "Epoch: 6, Loss: 8.299505233764648\n",
      "Epoch: 7, Loss: 8.222620964050293\n",
      "Epoch: 8, Loss: 8.133275985717773\n",
      "Epoch: 9, Loss: 8.050451278686523\n",
      "Epoch: 10, Loss: 7.971932411193848\n",
      "Epoch: 11, Loss: 7.8940887451171875\n",
      "Epoch: 12, Loss: 7.8137736320495605\n",
      "Epoch: 13, Loss: 7.725255012512207\n",
      "Epoch: 14, Loss: 7.6384124755859375\n",
      "Epoch: 15, Loss: 7.558730602264404\n",
      "Epoch: 16, Loss: 7.472562313079834\n",
      "Epoch: 17, Loss: 7.387964248657227\n",
      "Epoch: 18, Loss: 7.307355880737305\n",
      "Epoch: 19, Loss: 7.227423191070557\n",
      "Epoch: 20, Loss: 7.1403303146362305\n",
      "Epoch: 21, Loss: 7.057950019836426\n",
      "Epoch: 22, Loss: 6.996657848358154\n",
      "Epoch: 23, Loss: 6.926945209503174\n",
      "Epoch: 24, Loss: 6.834604740142822\n",
      "Epoch: 25, Loss: 6.757898807525635\n",
      "Epoch: 26, Loss: 6.686751365661621\n",
      "Epoch: 27, Loss: 6.609136581420898\n",
      "Epoch: 28, Loss: 6.54117488861084\n",
      "Epoch: 29, Loss: 6.467886447906494\n",
      "Epoch: 30, Loss: 6.392455577850342\n",
      "Epoch: 31, Loss: 6.331907749176025\n",
      "Epoch: 32, Loss: 6.261394023895264\n",
      "Epoch: 33, Loss: 6.1861395835876465\n",
      "Epoch: 34, Loss: 6.119948863983154\n",
      "Epoch: 35, Loss: 6.0502824783325195\n",
      "Epoch: 36, Loss: 5.992386817932129\n",
      "Epoch: 37, Loss: 5.9189772605896\n",
      "Epoch: 38, Loss: 5.859646320343018\n",
      "Epoch: 39, Loss: 5.794297218322754\n",
      "Epoch: 40, Loss: 5.735778331756592\n",
      "Epoch: 41, Loss: 5.677304267883301\n",
      "Epoch: 42, Loss: 5.61850118637085\n",
      "Epoch: 43, Loss: 5.548036575317383\n",
      "Epoch: 44, Loss: 5.496297359466553\n",
      "Epoch: 45, Loss: 5.431546211242676\n",
      "Epoch: 46, Loss: 5.369130611419678\n",
      "Epoch: 47, Loss: 5.311954975128174\n",
      "Epoch: 48, Loss: 5.252424716949463\n",
      "Epoch: 49, Loss: 5.198147773742676\n",
      "Epoch: 50, Loss: 5.137816905975342\n",
      "Epoch: 51, Loss: 5.080683708190918\n",
      "Epoch: 52, Loss: 5.032107353210449\n",
      "Epoch: 53, Loss: 4.97014045715332\n",
      "Epoch: 54, Loss: 4.919454097747803\n",
      "Epoch: 55, Loss: 4.8610992431640625\n",
      "Epoch: 56, Loss: 4.808884620666504\n",
      "Epoch: 57, Loss: 4.752028465270996\n",
      "Epoch: 58, Loss: 4.697004318237305\n",
      "Epoch: 59, Loss: 4.647921562194824\n",
      "Epoch: 60, Loss: 4.599471092224121\n",
      "Epoch: 61, Loss: 4.545231342315674\n",
      "Epoch: 62, Loss: 4.491630554199219\n",
      "Epoch: 63, Loss: 4.435328006744385\n",
      "Epoch: 64, Loss: 4.393916130065918\n",
      "Epoch: 65, Loss: 4.34473991394043\n",
      "Epoch: 66, Loss: 4.28926944732666\n",
      "Epoch: 67, Loss: 4.241560459136963\n",
      "Epoch: 68, Loss: 4.1861042976379395\n",
      "Epoch: 69, Loss: 4.137300968170166\n",
      "Epoch: 70, Loss: 4.086689472198486\n",
      "Epoch: 71, Loss: 4.036684989929199\n",
      "Epoch: 72, Loss: 3.9927587509155273\n",
      "Epoch: 73, Loss: 3.9420323371887207\n",
      "Epoch: 74, Loss: 3.89182186126709\n",
      "Epoch: 75, Loss: 3.8482916355133057\n",
      "Epoch: 76, Loss: 3.797083616256714\n",
      "Epoch: 77, Loss: 3.755228042602539\n",
      "Epoch: 78, Loss: 3.7051830291748047\n",
      "Epoch: 79, Loss: 3.6574411392211914\n",
      "Epoch: 80, Loss: 3.6181700229644775\n",
      "Epoch: 81, Loss: 3.5685880184173584\n",
      "Epoch: 82, Loss: 3.51894211769104\n",
      "Epoch: 83, Loss: 3.468994379043579\n",
      "Epoch: 84, Loss: 3.4340758323669434\n",
      "Epoch: 85, Loss: 3.3908004760742188\n",
      "Epoch: 86, Loss: 3.337818145751953\n",
      "Epoch: 87, Loss: 3.2919936180114746\n",
      "Epoch: 88, Loss: 3.2460594177246094\n",
      "Epoch: 89, Loss: 3.20424222946167\n",
      "Epoch: 90, Loss: 3.1578946113586426\n",
      "Epoch: 91, Loss: 3.119880437850952\n",
      "Epoch: 92, Loss: 3.0670173168182373\n",
      "Epoch: 93, Loss: 3.0248959064483643\n",
      "Epoch: 94, Loss: 2.9776673316955566\n",
      "Epoch: 95, Loss: 2.9350838661193848\n",
      "Epoch: 96, Loss: 2.896873950958252\n",
      "Epoch: 97, Loss: 2.8603806495666504\n",
      "Epoch: 98, Loss: 2.8116915225982666\n",
      "Epoch: 99, Loss: 2.7711119651794434\n",
      "Epoch: 100, Loss: 2.726931571960449\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src_data, tgt_data[:, :-1])\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4262,  915, 2443,  317, 4699,  645, 1060, 2994, 4422, 3371, 2985, 4745,\n",
       "        2993, 3212,  609, 3047,  804, 4671, 2878, 3142, 2801, 2354, 1117, 3190,\n",
       "          11, 4526,  619, 3197, 3310, 2675, 4781, 4949, 4671, 2125, 1294, 1013,\n",
       "         343, 4675, 4189, 1877, 2941,  732, 1995, 2067, 3983, 4052, 1973, 2045,\n",
       "        4968, 1668, 2733,  966, 4649, 2732, 1369, 3650, 3520, 2597, 3524, 1754,\n",
       "         364, 3621, 2672, 2548, 3055,  504, 2907, 2513, 4243, 3222, 4169, 2911,\n",
       "        4459, 1523, 4257, 4927, 2065,   87,  452, 1227, 1952, 2072,  565, 2799,\n",
       "        3747, 4224, 2140, 3709, 1558, 2931, 3727, 3043, 1905, 3622, 3203,  643,\n",
       "          22, 2177, 3943])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try inference with the model and compare what the signal was supposed to be\n",
    "transformer.eval()\n",
    "output = transformer(src_data[:1,:], tgt_data[:1, :-1])\n",
    "output.view(-1,tgt_vocab_size).argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3444, 4262,  915,  ...,   22, 2177, 3943],\n",
       "        [3358, 3681,  998,  ..., 3444, 4449,   74],\n",
       "        [3477, 3135, 4904,  ...,  707, 2644, 2084],\n",
       "        ...,\n",
       "        [4750,  837,  840,  ..., 3721, 1117, 1879],\n",
       "        [4042,  926, 4557,  ..., 3964, 1881, 1179],\n",
       "        [ 433, 4751, 3527,  ..., 1834,  333,  985]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target data, compare to above\n",
    "tgt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
