{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network, experimentation tool, version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# activation functions\n",
    "# ReLu is very simple, it filters out all negative values\n",
    "# this is a powerful activation function in reality\n",
    "def activation_ReLu(number):\n",
    "    if number > 0:\n",
    "        return number\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# we also need a derivated version of ReLu\n",
    "# otherwise same as original, but instead of original value, return 1 instead\n",
    "def activation_ReLu_partial_derivative(number):\n",
    "    if number > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lock down the randomness in order to get same results everytime\n",
    "# you can change or disable this if you want\n",
    "np.random.seed(123)\n",
    "\n",
    "def generate_train_data():\n",
    "    result = []\n",
    "\n",
    "    # create 100 numbers\n",
    "    for x in range(100):\n",
    "        n1 = np.random.randint(0, 5)\n",
    "        n2 = np.random.randint(3, 7)\n",
    "        n3 = n1 ** 2 + n2 + np.random.randint(0, 5)\n",
    "        n3 = int(n3)\n",
    "\n",
    "        result.append([n1, n2, n3])\n",
    "\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 6.016411571791334\n",
      "Epoch: 2, loss: 4.758289558557988\n",
      "Epoch: 3, loss: 4.575386340181411\n",
      "Epoch: 4, loss: 4.727909730016251\n",
      "Epoch: 5, loss: 4.499508535144083\n",
      "Epoch: 6, loss: 4.302577900619439\n",
      "Epoch: 7, loss: 4.100827835458947\n",
      "Epoch: 8, loss: 3.8971622629154234\n",
      "Epoch: 9, loss: 3.6936674015208233\n",
      "Epoch: 10, loss: 3.489421895319458\n",
      "Epoch: 11, loss: 3.284161110795736\n",
      "Epoch: 12, loss: 3.0782125242894\n",
      "Epoch: 13, loss: 2.875852128778724\n",
      "Epoch: 14, loss: 2.677437806867879\n",
      "Epoch: 15, loss: 2.5772452674603326\n",
      "Epoch: 16, loss: 2.4763479374882933\n",
      "Epoch: 17, loss: 2.3761231923916553\n",
      "Epoch: 18, loss: 2.277376561659176\n",
      "Epoch: 19, loss: 2.1802044126348745\n",
      "Epoch: 20, loss: 2.1073869127677303\n",
      "Epoch: 21, loss: 2.044394652187758\n",
      "Epoch: 22, loss: 2.058284456770161\n",
      "Epoch: 23, loss: 2.0871752897900544\n",
      "Epoch: 24, loss: 2.1074660946431156\n",
      "Epoch: 25, loss: 2.1232277202053993\n",
      "Epoch: 26, loss: 2.1362673041692424\n",
      "Epoch: 27, loss: 2.148231309809732\n",
      "Epoch: 28, loss: 2.1592964457713864\n",
      "Epoch: 29, loss: 2.1687682861307938\n",
      "Epoch: 30, loss: 2.1747424198227936\n",
      "Epoch: 31, loss: 2.1781948827109856\n",
      "Epoch: 32, loss: 2.177869085622272\n",
      "Epoch: 33, loss: 2.1725092866918376\n",
      "Epoch: 34, loss: 2.1644019611739505\n",
      "Epoch: 35, loss: 2.155687455443743\n",
      "Epoch: 36, loss: 2.1438131261595244\n",
      "Epoch: 37, loss: 2.1307701738022042\n",
      "Epoch: 38, loss: 2.1239896713038737\n",
      "Epoch: 39, loss: 2.1177969386611126\n",
      "Epoch: 40, loss: 2.111576019234798\n",
      "Epoch: 41, loss: 2.1054129725679798\n",
      "Epoch: 42, loss: 2.0992930259939993\n",
      "Epoch: 43, loss: 2.0933734873397385\n",
      "Epoch: 44, loss: 2.0877105536686855\n",
      "Epoch: 45, loss: 2.0823091737874555\n",
      "Epoch: 46, loss: 2.0771639839736356\n",
      "Epoch: 47, loss: 2.0722668729086515\n",
      "Epoch: 48, loss: 2.067608603186983\n",
      "Epoch: 49, loss: 2.063179326208626\n",
      "Epoch: 50, loss: 2.058968855835681\n",
      "Epoch: 51, loss: 2.0549668623240613\n",
      "Epoch: 52, loss: 2.0511630216181036\n",
      "Epoch: 53, loss: 2.0475471315005978\n",
      "Epoch: 54, loss: 2.0441092010301\n",
      "Epoch: 55, loss: 2.040839518093496\n",
      "Epoch: 56, loss: 2.037728699024602\n",
      "Epoch: 57, loss: 2.034767723585381\n",
      "Epoch: 58, loss: 2.0319479580660453\n",
      "Epoch: 59, loss: 2.0292611688070297\n",
      "Epoch: 60, loss: 2.02669952806218\n",
      "Epoch: 61, loss: 2.0242556137986623\n",
      "Epoch: 62, loss: 2.0219224047564333\n",
      "Epoch: 63, loss: 2.0196932718598277\n",
      "Epoch: 64, loss: 2.0175619668802227\n",
      "Epoch: 65, loss: 2.0155226090870744\n",
      "Epoch: 66, loss: 2.013569670487522\n",
      "Epoch: 67, loss: 2.0116979601418503\n",
      "Epoch: 68, loss: 2.0099026079464966\n",
      "Epoch: 69, loss: 2.0081790481974306\n",
      "Epoch: 70, loss: 2.0065230031809853\n",
      "Epoch: 71, loss: 2.0049304669847636\n",
      "Epoch: 72, loss: 2.0033976896766434\n",
      "Epoch: 73, loss: 2.0019211619625352\n",
      "Epoch: 74, loss: 2.0004976004038273\n",
      "Epoch: 75, loss: 1.9991239332506168\n",
      "Epoch: 76, loss: 1.9977972869269176\n",
      "Epoch: 77, loss: 1.9965149731881946\n",
      "Epoch: 78, loss: 1.9952744769587591\n",
      "Epoch: 79, loss: 1.9940734448465658\n",
      "Epoch: 80, loss: 1.992909674325016\n",
      "Epoch: 81, loss: 1.9917811035655855\n",
      "Epoch: 82, loss: 1.9906858019006621\n",
      "Epoch: 83, loss: 1.9896219608924137\n",
      "Epoch: 84, loss: 1.9885878859818589\n",
      "Epoch: 85, loss: 1.9875819886904083\n",
      "Epoch: 86, loss: 1.986602779345519\n",
      "Epoch: 87, loss: 1.985648860302288\n",
      "Epoch: 88, loss: 1.9847189196321426\n",
      "Epoch: 89, loss: 1.9838117252512117\n",
      "Epoch: 90, loss: 1.9829261194609975\n",
      "Epoch: 91, loss: 1.9820610138751178\n",
      "Epoch: 92, loss: 1.9812153847071066\n",
      "Epoch: 93, loss: 1.9803882683949738\n",
      "Epoch: 94, loss: 1.9795787575397823\n",
      "Epoch: 95, loss: 1.9787859971365338\n",
      "Epoch: 96, loss: 1.9780091810767253\n",
      "Epoch: 97, loss: 1.977247548903547\n",
      "Epoch: 98, loss: 1.9765003828013308\n",
      "Epoch: 99, loss: 1.9757670048024187\n",
      "Epoch: 100, loss: 1.9750467741956024\n",
      "Epoch: 101, loss: 1.974339085121262\n",
      "Epoch: 102, loss: 1.9736433643392792\n",
      "Epoch: 103, loss: 1.9729590691571524\n",
      "Epoch: 104, loss: 1.9722856855061082\n",
      "Epoch: 105, loss: 1.9716227261541919\n",
      "Epoch: 106, loss: 1.9709697290462047\n",
      "Epoch: 107, loss: 1.9703262557605583\n",
      "Epoch: 108, loss: 1.969691890074877\n",
      "Epoch: 109, loss: 1.9690662366314053\n",
      "Epoch: 110, loss: 1.9684489196952883\n",
      "Epoch: 111, loss: 1.9678395819983894\n",
      "Epoch: 112, loss: 1.9672378836624609\n",
      "Epoch: 113, loss: 1.9666435011954262\n",
      "Epoch: 114, loss: 1.9660561265555376\n",
      "Epoch: 115, loss: 1.9654754662781684\n",
      "Epoch: 116, loss: 1.9649012406607138\n",
      "Epoch: 117, loss: 1.9643331830011306\n",
      "Epoch: 118, loss: 1.9637710388862581\n",
      "Epoch: 119, loss: 1.963214565526194\n",
      "Epoch: 120, loss: 1.9626635311314022\n",
      "Epoch: 121, loss: 1.9621177143293138\n",
      "Epoch: 122, loss: 1.9615769036177606\n",
      "Epoch: 123, loss: 1.9610408968523165\n",
      "Epoch: 124, loss: 1.96050950076539\n",
      "Epoch: 125, loss: 1.9599825305145633\n",
      "Epoch: 126, loss: 1.9594598092583142\n",
      "Epoch: 127, loss: 1.9589411677569883\n",
      "Epoch: 128, loss: 1.9584264439975358\n",
      "Epoch: 129, loss: 1.957915482839976\n",
      "Epoch: 130, loss: 1.957408135684576\n",
      "Epoch: 131, loss: 1.9569042601578956\n",
      "Epoch: 132, loss: 1.9564037198167108\n",
      "Epoch: 133, loss: 1.9559063838685296\n",
      "Epoch: 134, loss: 1.9554121269075726\n",
      "Epoch: 135, loss: 1.9549208286651982\n",
      "Epoch: 136, loss: 1.9544323737739075\n",
      "Epoch: 137, loss: 1.953946651544049\n",
      "Epoch: 138, loss: 1.9534635557523317\n",
      "Epoch: 139, loss: 1.9529829844413866\n",
      "Epoch: 140, loss: 1.9525048397299438\n",
      "Epoch: 141, loss: 1.9520290276325407\n",
      "Epoch: 142, loss: 1.9515554578885999\n",
      "Epoch: 143, loss: 1.9510840437999952\n",
      "Epoch: 144, loss: 1.9506147020768967\n",
      "Epoch: 145, loss: 1.950147352691015\n",
      "Epoch: 146, loss: 1.949681918736243\n",
      "Epoch: 147, loss: 1.9492183262959446\n",
      "Epoch: 148, loss: 1.9487565043166093\n",
      "Epoch: 149, loss: 1.948296384487714\n",
      "Epoch: 150, loss: 1.94783790112695\n",
      "Epoch: 151, loss: 1.947380991071151\n",
      "Epoch: 152, loss: 1.946925593572025\n",
      "Epoch: 153, loss: 1.9464716501969357\n",
      "Epoch: 154, loss: 1.946019104733961\n",
      "Epoch: 155, loss: 1.945567903101483\n",
      "Epoch: 156, loss: 1.9451179932617235\n",
      "Epoch: 157, loss: 1.9446693251382234\n",
      "Epoch: 158, loss: 1.9442218505369002\n",
      "Epoch: 159, loss: 1.9437755230706872\n",
      "Epoch: 160, loss: 1.9433302980873646\n",
      "Epoch: 161, loss: 1.9428861326006852\n",
      "Epoch: 162, loss: 1.9424429852242868\n",
      "Epoch: 163, loss: 1.9420008161086098\n",
      "Epoch: 164, loss: 1.9415595868804723\n",
      "Epoch: 165, loss: 1.9411192605851446\n",
      "Epoch: 166, loss: 1.9406798016309896\n",
      "Epoch: 167, loss: 1.9402411757363063\n",
      "Epoch: 168, loss: 1.939803349878507\n",
      "Epoch: 169, loss: 1.9393662922454087\n",
      "Epoch: 170, loss: 1.9389299721884066\n",
      "Epoch: 171, loss: 1.9384943601777833\n",
      "Epoch: 172, loss: 1.9380594277596805\n",
      "Epoch: 173, loss: 1.9376251475149764\n",
      "Epoch: 174, loss: 1.937191493019705\n",
      "Epoch: 175, loss: 1.9367584388072154\n",
      "Epoch: 176, loss: 1.936325960331755\n",
      "Epoch: 177, loss: 1.9358940339335573\n",
      "Epoch: 178, loss: 1.9354626368053838\n",
      "Epoch: 179, loss: 1.935031746960309\n",
      "Epoch: 180, loss: 1.934601343200863\n",
      "Epoch: 181, loss: 1.9341714050894192\n",
      "Epoch: 182, loss: 1.9337419129196507\n",
      "Epoch: 183, loss: 1.93331284768924\n",
      "Epoch: 184, loss: 1.9328841910735677\n",
      "Epoch: 185, loss: 1.9324559254004965\n",
      "Epoch: 186, loss: 1.9320280336261417\n",
      "Epoch: 187, loss: 1.9316004993115476\n",
      "Epoch: 188, loss: 1.9311733066002803\n",
      "Epoch: 189, loss: 1.9307464401969625\n",
      "Epoch: 190, loss: 1.9303198853465364\n",
      "Epoch: 191, loss: 1.9298936278144188\n",
      "Epoch: 192, loss: 1.9294676538673812\n",
      "Epoch: 193, loss: 1.9290419502551437\n",
      "Epoch: 194, loss: 1.9286165041927168\n",
      "Epoch: 195, loss: 1.928191303343467\n",
      "Epoch: 196, loss: 1.9277663358026769\n",
      "Epoch: 197, loss: 1.9273415900819149\n",
      "Epoch: 198, loss: 1.9269170550938464\n",
      "Epoch: 199, loss: 1.9264927201377495\n",
      "Epoch: 200, loss: 1.9260685748855153\n",
      "Epoch: 201, loss: 1.9256446093681538\n",
      "Epoch: 202, loss: 1.9252208139629075\n",
      "Epoch: 203, loss: 1.9247971793807226\n",
      "Epoch: 204, loss: 1.9243736966543008\n",
      "Epoch: 205, loss: 1.9239503571265588\n",
      "Epoch: 206, loss: 1.9235271524395425\n",
      "Epoch: 207, loss: 1.9231040745236612\n",
      "Epoch: 208, loss: 1.9226811155875085\n",
      "Epoch: 209, loss: 1.9222582681078835\n",
      "Epoch: 210, loss: 1.921835524820291\n",
      "Epoch: 211, loss: 1.9214128787097378\n",
      "Epoch: 212, loss: 1.9209903230019265\n",
      "Epoch: 213, loss: 1.9205678511547573\n",
      "Epoch: 214, loss: 1.9201454568500846\n",
      "Epoch: 215, loss: 1.9197231339858511\n",
      "Epoch: 216, loss: 1.9193008766685282\n",
      "Epoch: 217, loss: 1.9188786792057437\n",
      "Epoch: 218, loss: 1.9184565360992636\n",
      "Epoch: 219, loss: 1.9180344420382092\n",
      "Epoch: 220, loss: 1.9176123918925014\n",
      "Epoch: 221, loss: 1.9171903807065684\n",
      "Epoch: 222, loss: 1.916768403693261\n",
      "Epoch: 223, loss: 1.9163464562280603\n",
      "Epoch: 224, loss: 1.915924533843374\n",
      "Epoch: 225, loss: 1.9155026322231508\n",
      "Epoch: 226, loss: 1.9150807471976563\n",
      "Epoch: 227, loss: 1.9146588747383884\n",
      "Epoch: 228, loss: 1.9142370109532723\n",
      "Epoch: 229, loss: 1.913815152081948\n",
      "Epoch: 230, loss: 1.913393294491256\n",
      "Epoch: 231, loss: 1.9129714346708828\n",
      "Epoch: 232, loss: 1.9125495692292334\n",
      "Epoch: 233, loss: 1.9121276948892627\n",
      "Epoch: 234, loss: 1.9117058084847118\n",
      "Epoch: 235, loss: 1.9112839069562604\n",
      "Epoch: 236, loss: 1.9108619873479422\n",
      "Epoch: 237, loss: 1.9104400468036438\n",
      "Epoch: 238, loss: 1.91001808256372\n",
      "Epoch: 239, loss: 1.9095960919617718\n",
      "Epoch: 240, loss: 1.9091740724215216\n",
      "Epoch: 241, loss: 1.9087520214537228\n",
      "Epoch: 242, loss: 1.9083299366533437\n",
      "Epoch: 243, loss: 1.9079078156966571\n",
      "Epoch: 244, loss: 1.907485656338605\n",
      "Epoch: 245, loss: 1.9070634564101416\n",
      "Epoch: 246, loss: 1.90664121381572\n",
      "Epoch: 247, loss: 1.906218926530849\n",
      "Epoch: 248, loss: 1.9057965925997482\n",
      "Epoch: 249, loss: 1.9053742101330775\n",
      "Epoch: 250, loss: 1.904951777305756\n",
      "Epoch: 251, loss: 1.9045292923548205\n",
      "Epoch: 252, loss: 1.90410675357743\n",
      "Epoch: 253, loss: 1.9036841593288587\n",
      "Epoch: 254, loss: 1.9032615080206214\n",
      "Epoch: 255, loss: 1.9028387981186152\n",
      "Epoch: 256, loss: 1.9024160281413698\n",
      "Epoch: 257, loss: 1.901993196658339\n",
      "Epoch: 258, loss: 1.9015703022882322\n",
      "Epoch: 259, loss: 1.9011473436974147\n",
      "Epoch: 260, loss: 1.9007243195983883\n",
      "Epoch: 261, loss: 1.9003012287483316\n",
      "Epoch: 262, loss: 1.8998780699475628\n",
      "Epoch: 263, loss: 1.8994548420382968\n",
      "Epoch: 264, loss: 1.8990315439031586\n",
      "Epoch: 265, loss: 1.898608174464017\n",
      "Epoch: 266, loss: 1.8981847326806354\n",
      "Epoch: 267, loss: 1.8977612175495167\n",
      "Epoch: 268, loss: 1.8973376281027383\n",
      "Epoch: 269, loss: 1.8969139634067782\n",
      "Epoch: 270, loss: 1.8964902225615121\n",
      "Epoch: 271, loss: 1.8960664046990272\n",
      "Epoch: 272, loss: 1.8956425089827529\n",
      "Epoch: 273, loss: 1.8952185346063584\n",
      "Epoch: 274, loss: 1.8947944807928854\n",
      "Epoch: 275, loss: 1.8943703467937536\n",
      "Epoch: 276, loss: 1.8939461318879494\n",
      "Epoch: 277, loss: 1.8935218353811054\n",
      "Epoch: 278, loss: 1.8930974566046948\n",
      "Epoch: 279, loss: 1.8926729949152206\n",
      "Epoch: 280, loss: 1.892248449693478\n",
      "Epoch: 281, loss: 1.8918238203437119\n",
      "Epoch: 282, loss: 1.891399106292977\n",
      "Epoch: 283, loss: 1.8909743069904328\n",
      "Epoch: 284, loss: 1.890549421906599\n",
      "Epoch: 285, loss: 1.8901244505327224\n",
      "Epoch: 286, loss: 1.8896993923801957\n",
      "Epoch: 287, loss: 1.8892742469798807\n",
      "Epoch: 288, loss: 1.8888490138815424\n",
      "Epoch: 289, loss: 1.888423692653243\n",
      "Epoch: 290, loss: 1.8879982828808033\n",
      "Epoch: 291, loss: 1.8875727841672874\n",
      "Epoch: 292, loss: 1.887147196132445\n",
      "Epoch: 293, loss: 1.8867215184122348\n",
      "Epoch: 294, loss: 1.8862957506582685\n",
      "Epoch: 295, loss: 1.8858698925374462\n",
      "Epoch: 296, loss: 1.885443943731412\n",
      "Epoch: 297, loss: 1.8850179039361354\n",
      "Epoch: 298, loss: 1.8845917728614845\n",
      "Epoch: 299, loss: 1.8841655502308328\n",
      "Epoch: 300, loss: 1.8837392357805935\n",
      "Epoch: 301, loss: 1.883312829259896\n",
      "Epoch: 302, loss: 1.8828863304301682\n",
      "Epoch: 303, loss: 1.8824597390648041\n",
      "Epoch: 304, loss: 1.8820330549487627\n",
      "Epoch: 305, loss: 1.8816062778782536\n",
      "Epoch: 306, loss: 1.8811794076604191\n",
      "Epoch: 307, loss: 1.880752444112964\n",
      "Epoch: 308, loss: 1.8803253870639236\n",
      "Epoch: 309, loss: 1.8798982363512557\n",
      "Epoch: 310, loss: 1.8794709918226196\n",
      "Epoch: 311, loss: 1.8790436533351131\n",
      "Epoch: 312, loss: 1.878616220754887\n",
      "Epoch: 313, loss: 1.878188693957024\n",
      "Epoch: 314, loss: 1.8777610728251484\n",
      "Epoch: 315, loss: 1.8773333572512896\n",
      "Epoch: 316, loss: 1.8769055471355338\n",
      "Epoch: 317, loss: 1.876477642385866\n",
      "Epoch: 318, loss: 1.8760496429178952\n",
      "Epoch: 319, loss: 1.875621548654684\n",
      "Epoch: 320, loss: 1.8751933595264563\n",
      "Epoch: 321, loss: 1.8747650754704785\n",
      "Epoch: 322, loss: 1.874336696430768\n",
      "Epoch: 323, loss: 1.8739082223580001\n",
      "Epoch: 324, loss: 1.873479653209216\n",
      "Epoch: 325, loss: 1.8730509889476885\n",
      "Epoch: 326, loss: 1.8726222295427672\n",
      "Epoch: 327, loss: 1.8721933749696533\n",
      "Epoch: 328, loss: 1.8717644252092418\n",
      "Epoch: 329, loss: 1.871335380248012\n",
      "Epoch: 330, loss: 1.8709062400778043\n",
      "Epoch: 331, loss: 1.8704770046956773\n",
      "Epoch: 332, loss: 1.8700476741038226\n",
      "Epoch: 333, loss: 1.869618248309308\n",
      "Epoch: 334, loss: 1.869188727324051\n",
      "Epoch: 335, loss: 1.8687591111646034\n",
      "Epoch: 336, loss: 1.8683293998520691\n",
      "Epoch: 337, loss: 1.8678995934119234\n",
      "Epoch: 338, loss: 1.867469691873966\n",
      "Epoch: 339, loss: 1.8670396952721051\n",
      "Epoch: 340, loss: 1.8666096036443314\n",
      "Epoch: 341, loss: 1.8661794170325257\n",
      "Epoch: 342, loss: 1.8657491354824125\n",
      "Epoch: 343, loss: 1.8653187590434144\n",
      "Epoch: 344, loss: 1.8648882877685555\n",
      "Epoch: 345, loss: 1.864457721714368\n",
      "Epoch: 346, loss: 1.8640270609407936\n",
      "Epoch: 347, loss: 1.8635963055110794\n",
      "Epoch: 348, loss: 1.8631654554916786\n",
      "Epoch: 349, loss: 1.862734510952205\n",
      "Epoch: 350, loss: 1.862303471965268\n",
      "Epoch: 351, loss: 1.8618723386064775\n",
      "Epoch: 352, loss: 1.8614411109542657\n",
      "Epoch: 353, loss: 1.861009789089918\n",
      "Epoch: 354, loss: 1.860578373097387\n",
      "Epoch: 355, loss: 1.860146863063282\n",
      "Epoch: 356, loss: 1.8597152590767752\n",
      "Epoch: 357, loss: 1.8592835612295366\n",
      "Epoch: 358, loss: 1.8588517696156706\n",
      "Epoch: 359, loss: 1.858419884331626\n",
      "Epoch: 360, loss: 1.857987905476153\n",
      "Epoch: 361, loss: 1.8575558331502127\n",
      "Epoch: 362, loss: 1.8571236674569718\n",
      "Epoch: 363, loss: 1.8566914085016566\n",
      "Epoch: 364, loss: 1.8562590563915895\n",
      "Epoch: 365, loss: 1.8558266112360433\n",
      "Epoch: 366, loss: 1.8553940731462784\n",
      "Epoch: 367, loss: 1.854961442235372\n",
      "Epoch: 368, loss: 1.8545287186183048\n",
      "Epoch: 369, loss: 1.854095902411771\n",
      "Epoch: 370, loss: 1.8536629937342586\n",
      "Epoch: 371, loss: 1.8532299927059095\n",
      "Epoch: 372, loss: 1.8527968994484882\n",
      "Epoch: 373, loss: 1.8523637140853773\n",
      "Epoch: 374, loss: 1.851930436741497\n",
      "Epoch: 375, loss: 1.851497067543288\n",
      "Epoch: 376, loss: 1.8510636066186301\n",
      "Epoch: 377, loss: 1.8506300540968432\n",
      "Epoch: 378, loss: 1.8501964101086423\n",
      "Epoch: 379, loss: 1.8497626747860596\n",
      "Epoch: 380, loss: 1.849328848262448\n",
      "Epoch: 381, loss: 1.8488949306724614\n",
      "Epoch: 382, loss: 1.8484609221519333\n",
      "Epoch: 383, loss: 1.848026822837982\n",
      "Epoch: 384, loss: 1.8475926328688161\n",
      "Epoch: 385, loss: 1.8471583523838269\n",
      "Epoch: 386, loss: 1.8467239815235093\n",
      "Epoch: 387, loss: 1.846289520429413\n",
      "Epoch: 388, loss: 1.8458549692441713\n",
      "Epoch: 389, loss: 1.8454203281113888\n",
      "Epoch: 390, loss: 1.8449855971757134\n",
      "Epoch: 391, loss: 1.8445507765826963\n",
      "Epoch: 392, loss: 1.8441158664788866\n",
      "Epoch: 393, loss: 1.8436808670116727\n",
      "Epoch: 394, loss: 1.8432457783293916\n",
      "Epoch: 395, loss: 1.8428106005812093\n",
      "Epoch: 396, loss: 1.8423753339171232\n",
      "Epoch: 397, loss: 1.8419399784879673\n",
      "Epoch: 398, loss: 1.8415045344453354\n",
      "Epoch: 399, loss: 1.841069001941617\n",
      "Epoch: 400, loss: 1.8406333811299265\n",
      "Epoch: 401, loss: 1.8401976721641307\n",
      "Epoch: 402, loss: 1.839761875198771\n",
      "Epoch: 403, loss: 1.8393259903890877\n",
      "Epoch: 404, loss: 1.8388900178910095\n",
      "Epoch: 405, loss: 1.838453957861076\n",
      "Epoch: 406, loss: 1.8380178104564706\n",
      "Epoch: 407, loss: 1.8375815758350211\n",
      "Epoch: 408, loss: 1.8371452541550972\n",
      "Epoch: 409, loss: 1.8367088455756777\n",
      "Epoch: 410, loss: 1.8362723502563165\n",
      "Epoch: 411, loss: 1.8358357683570798\n",
      "Epoch: 412, loss: 1.8353991000385745\n",
      "Epoch: 413, loss: 1.8349623454619282\n",
      "Epoch: 414, loss: 1.8345255047887985\n",
      "Epoch: 415, loss: 1.8340885781812724\n",
      "Epoch: 416, loss: 1.8336515658019756\n",
      "Epoch: 417, loss: 1.8332144678139144\n",
      "Epoch: 418, loss: 1.8327772843806183\n",
      "Epoch: 419, loss: 1.8323400156660061\n",
      "Epoch: 420, loss: 1.831902661834429\n",
      "Epoch: 421, loss: 1.83146522305065\n",
      "Epoch: 422, loss: 1.831027699479835\n",
      "Epoch: 423, loss: 1.8305900912875035\n",
      "Epoch: 424, loss: 1.830152398639597\n",
      "Epoch: 425, loss: 1.8297146217023994\n",
      "Epoch: 426, loss: 1.8292767606425446\n",
      "Epoch: 427, loss: 1.8288388156269901\n",
      "Epoch: 428, loss: 1.8284007868230838\n",
      "Epoch: 429, loss: 1.8279626743984358\n",
      "Epoch: 430, loss: 1.8275244785209983\n",
      "Epoch: 431, loss: 1.8270861993590222\n",
      "Epoch: 432, loss: 1.826647837081077\n",
      "Epoch: 433, loss: 1.8262093918559728\n",
      "Epoch: 434, loss: 1.8257708638528227\n",
      "Epoch: 435, loss: 1.8253322532410334\n",
      "Epoch: 436, loss: 1.8248935601902376\n",
      "Epoch: 437, loss: 1.8244547848703268\n",
      "Epoch: 438, loss: 1.8240159274514371\n",
      "Epoch: 439, loss: 1.8235769881039732\n",
      "Epoch: 440, loss: 1.8231379669985408\n",
      "Epoch: 441, loss: 1.8226988643059605\n",
      "Epoch: 442, loss: 1.8222596801973068\n",
      "Epoch: 443, loss: 1.8218204148438297\n",
      "Epoch: 444, loss: 1.8213810684170186\n",
      "Epoch: 445, loss: 1.8209416410885193\n",
      "Epoch: 446, loss: 1.8205021330302016\n",
      "Epoch: 447, loss: 1.8200625444141012\n",
      "Epoch: 448, loss: 1.8196228754124388\n",
      "Epoch: 449, loss: 1.8191831261976108\n",
      "Epoch: 450, loss: 1.8187432969421975\n",
      "Epoch: 451, loss: 1.8183033878188972\n",
      "Epoch: 452, loss: 1.817863399000607\n",
      "Epoch: 453, loss: 1.8174233306603553\n",
      "Epoch: 454, loss: 1.8169831829713399\n",
      "Epoch: 455, loss: 1.8165429561068662\n",
      "Epoch: 456, loss: 1.8161026502404038\n",
      "Epoch: 457, loss: 1.8156622655455439\n",
      "Epoch: 458, loss: 1.8152218021960178\n",
      "Epoch: 459, loss: 1.8147812603656583\n",
      "Epoch: 460, loss: 1.8143406402284388\n",
      "Epoch: 461, loss: 1.8138999419584192\n",
      "Epoch: 462, loss: 1.8134591657298091\n",
      "Epoch: 463, loss: 1.8130183117169196\n",
      "Epoch: 464, loss: 1.812577380094114\n",
      "Epoch: 465, loss: 1.812136371035915\n",
      "Epoch: 466, loss: 1.811695284716927\n",
      "Epoch: 467, loss: 1.811254121311816\n",
      "Epoch: 468, loss: 1.8108128809953732\n",
      "Epoch: 469, loss: 1.8103715639424707\n",
      "Epoch: 470, loss: 1.8099301703280242\n",
      "Epoch: 471, loss: 1.8094887003270728\n",
      "Epoch: 472, loss: 1.8090471541147222\n",
      "Epoch: 473, loss: 1.808605531866131\n",
      "Epoch: 474, loss: 1.8081638337565569\n",
      "Epoch: 475, loss: 1.8077220599612716\n",
      "Epoch: 476, loss: 1.8072802106556796\n",
      "Epoch: 477, loss: 1.806838286015204\n",
      "Epoch: 478, loss: 1.806396286215324\n",
      "Epoch: 479, loss: 1.805954211431599\n",
      "Epoch: 480, loss: 1.8055120618396252\n",
      "Epoch: 481, loss: 1.8050698376150411\n",
      "Epoch: 482, loss: 1.8046275389335646\n",
      "Epoch: 483, loss: 1.8041851659709265\n",
      "Epoch: 484, loss: 1.8037427189029132\n",
      "Epoch: 485, loss: 1.8033001979053573\n",
      "Epoch: 486, loss: 1.8028576031541135\n",
      "Epoch: 487, loss: 1.8024149348251015\n",
      "Epoch: 488, loss: 1.801972193094258\n",
      "Epoch: 489, loss: 1.8015293781375656\n",
      "Epoch: 490, loss: 1.8010864901310046\n",
      "Epoch: 491, loss: 1.8006435292506247\n",
      "Epoch: 492, loss: 1.8002004956724682\n",
      "Epoch: 493, loss: 1.7997573895726613\n",
      "Epoch: 494, loss: 1.7993142111272513\n",
      "Epoch: 495, loss: 1.7988709605124165\n",
      "Epoch: 496, loss: 1.7984276379042892\n",
      "Epoch: 497, loss: 1.7979842434790374\n",
      "Epoch: 498, loss: 1.797540777412836\n",
      "Epoch: 499, loss: 1.7970972398819185\n",
      "Epoch: 500, loss: 1.7966536310624683\n",
      "Epoch: 501, loss: 1.7962099511307221\n",
      "Epoch: 502, loss: 1.7957662002629335\n",
      "Epoch: 503, loss: 1.7953223786353234\n",
      "Epoch: 504, loss: 1.794878486424148\n",
      "Epoch: 505, loss: 1.7944345238056882\n",
      "Epoch: 506, loss: 1.7939904909562079\n",
      "Epoch: 507, loss: 1.7935463880519666\n",
      "Epoch: 508, loss: 1.7931022152692495\n",
      "Epoch: 509, loss: 1.7926579727843086\n",
      "Epoch: 510, loss: 1.7922136607734502\n",
      "Epoch: 511, loss: 1.7917692794129285\n",
      "Epoch: 512, loss: 1.7913248288790224\n",
      "Epoch: 513, loss: 1.7908803093480166\n",
      "Epoch: 514, loss: 1.7904357209961583\n",
      "Epoch: 515, loss: 1.7899910639997045\n",
      "Epoch: 516, loss: 1.7895463385349368\n",
      "Epoch: 517, loss: 1.78910154477808\n",
      "Epoch: 518, loss: 1.7886566829053787\n",
      "Epoch: 519, loss: 1.7882117530930772\n",
      "Epoch: 520, loss: 1.7877667555173682\n",
      "Epoch: 521, loss: 1.7873216903544917\n",
      "Epoch: 522, loss: 1.7868765577806358\n",
      "Epoch: 523, loss: 1.7864313579719744\n",
      "Epoch: 524, loss: 1.7859860911046954\n",
      "Epoch: 525, loss: 1.7855407573549391\n",
      "Epoch: 526, loss: 1.785095356898874\n",
      "Epoch: 527, loss: 1.784649889912607\n",
      "Epoch: 528, loss: 1.7842043565722634\n",
      "Epoch: 529, loss: 1.7837587570539204\n",
      "Epoch: 530, loss: 1.7833130915336692\n",
      "Epoch: 531, loss: 1.7828673601875578\n",
      "Epoch: 532, loss: 1.7824215631916192\n",
      "Epoch: 533, loss: 1.7819757007218764\n",
      "Epoch: 534, loss: 1.7815297729543371\n",
      "Epoch: 535, loss: 1.7810837800649564\n",
      "Epoch: 536, loss: 1.7806377222297023\n",
      "Epoch: 537, loss: 1.7801915996245046\n",
      "Epoch: 538, loss: 1.7797454124252488\n",
      "Epoch: 539, loss: 1.7792991608078335\n",
      "Epoch: 540, loss: 1.7788528449481285\n",
      "Epoch: 541, loss: 1.778406465021968\n",
      "Epoch: 542, loss: 1.7779600212051339\n",
      "Epoch: 543, loss: 1.7775135136734348\n",
      "Epoch: 544, loss: 1.7770669426026253\n",
      "Epoch: 545, loss: 1.7766203081684264\n",
      "Epoch: 546, loss: 1.776173610546528\n",
      "Epoch: 547, loss: 1.7757268499126286\n",
      "Epoch: 548, loss: 1.7752800264423636\n",
      "Epoch: 549, loss: 1.7748331403113424\n",
      "Epoch: 550, loss: 1.7743861916951589\n",
      "Epoch: 551, loss: 1.773939180769367\n",
      "Epoch: 552, loss: 1.7734921077095047\n",
      "Epoch: 553, loss: 1.773044972691051\n",
      "Epoch: 554, loss: 1.772597775889474\n",
      "Epoch: 555, loss: 1.772150517480229\n",
      "Epoch: 556, loss: 1.771703197638695\n",
      "Epoch: 557, loss: 1.771255816540243\n",
      "Epoch: 558, loss: 1.7708083743602185\n",
      "Epoch: 559, loss: 1.7703608712739265\n",
      "Epoch: 560, loss: 1.7699133074566369\n",
      "Epoch: 561, loss: 1.769465683083579\n",
      "Epoch: 562, loss: 1.769017998329961\n",
      "Epoch: 563, loss: 1.7685702533709458\n",
      "Epoch: 564, loss: 1.7681224483816744\n",
      "Epoch: 565, loss: 1.7676745835372434\n",
      "Epoch: 566, loss: 1.7672266590127175\n",
      "Epoch: 567, loss: 1.7667786749831258\n",
      "Epoch: 568, loss: 1.7663306316234475\n",
      "Epoch: 569, loss: 1.765882529108654\n",
      "Epoch: 570, loss: 1.7654343676136615\n",
      "Epoch: 571, loss: 1.7649861473133504\n",
      "Epoch: 572, loss: 1.76453786838256\n",
      "Epoch: 573, loss: 1.7640895309961082\n",
      "Epoch: 574, loss: 1.7636411353287471\n",
      "Epoch: 575, loss: 1.7631926815552361\n",
      "Epoch: 576, loss: 1.7627441698502364\n",
      "Epoch: 577, loss: 1.7622956003884291\n",
      "Epoch: 578, loss: 1.7618469733444218\n",
      "Epoch: 579, loss: 1.7613982888927942\n",
      "Epoch: 580, loss: 1.7609495472080805\n",
      "Epoch: 581, loss: 1.7605007484647828\n",
      "Epoch: 582, loss: 1.7600518928373667\n",
      "Epoch: 583, loss: 1.7596029805002371\n",
      "Epoch: 584, loss: 1.7591540116277815\n",
      "Epoch: 585, loss: 1.7587049863943498\n",
      "Epoch: 586, loss: 1.7582559049742228\n",
      "Epoch: 587, loss: 1.7578067675416764\n",
      "Epoch: 588, loss: 1.757357574270913\n",
      "Epoch: 589, loss: 1.7569083253361204\n",
      "Epoch: 590, loss: 1.7564590209114217\n",
      "Epoch: 591, loss: 1.7560096611709117\n",
      "Epoch: 592, loss: 1.7555602462886533\n",
      "Epoch: 593, loss: 1.755110776438648\n",
      "Epoch: 594, loss: 1.7546612517948794\n",
      "Epoch: 595, loss: 1.7542116725312273\n",
      "Epoch: 596, loss: 1.7537620388216286\n",
      "Epoch: 597, loss: 1.7533123508398976\n",
      "Epoch: 598, loss: 1.752862608759821\n",
      "Epoch: 599, loss: 1.7524128127551941\n",
      "Epoch: 600, loss: 1.7519629629996818\n",
      "Epoch: 601, loss: 1.7515130596669715\n",
      "Epoch: 602, loss: 1.751063102930699\n",
      "Epoch: 603, loss: 1.750613092964429\n",
      "Epoch: 604, loss: 1.7501630299417081\n",
      "Epoch: 605, loss: 1.7497129140360255\n",
      "Epoch: 606, loss: 1.749262745420828\n",
      "Epoch: 607, loss: 1.7488125242695336\n",
      "Epoch: 608, loss: 1.748362250755485\n",
      "Epoch: 609, loss: 1.7479119250520057\n",
      "Epoch: 610, loss: 1.7474615473323662\n",
      "Epoch: 611, loss: 1.7470111177698089\n",
      "Epoch: 612, loss: 1.746560636537491\n",
      "Epoch: 613, loss: 1.7461101038085642\n",
      "Epoch: 614, loss: 1.7456595197561182\n",
      "Epoch: 615, loss: 1.7452088845531948\n",
      "Epoch: 616, loss: 1.7447581983728073\n",
      "Epoch: 617, loss: 1.7443074613878926\n",
      "Epoch: 618, loss: 1.7438566737713725\n",
      "Epoch: 619, loss: 1.7434058356960882\n",
      "Epoch: 620, loss: 1.7429549473348938\n",
      "Epoch: 621, loss: 1.7425040088605435\n",
      "Epoch: 622, loss: 1.7420530204457392\n",
      "Epoch: 623, loss: 1.7416019822631996\n",
      "Epoch: 624, loss: 1.7411508944855258\n",
      "Epoch: 625, loss: 1.7406997572853213\n",
      "Epoch: 626, loss: 1.7402485708351185\n",
      "Epoch: 627, loss: 1.7397973353074059\n",
      "Epoch: 628, loss: 1.7393460508746468\n",
      "Epoch: 629, loss: 1.7388947177092002\n",
      "Epoch: 630, loss: 1.738443335983442\n",
      "Epoch: 631, loss: 1.7379919058696904\n",
      "Epoch: 632, loss: 1.7375404275401822\n",
      "Epoch: 633, loss: 1.7370889011671147\n",
      "Epoch: 634, loss: 1.7366373269226694\n",
      "Epoch: 635, loss: 1.7361857049789557\n",
      "Epoch: 636, loss: 1.7357340355080297\n",
      "Epoch: 637, loss: 1.7352823186819124\n",
      "Epoch: 638, loss: 1.734830554672572\n",
      "Epoch: 639, loss: 1.7343787436519358\n",
      "Epoch: 640, loss: 1.7339268857918508\n",
      "Epoch: 641, loss: 1.7334749812641845\n",
      "Epoch: 642, loss: 1.7330230302406897\n",
      "Epoch: 643, loss: 1.7325710328930752\n",
      "Epoch: 644, loss: 1.732118989393052\n",
      "Epoch: 645, loss: 1.731666899912221\n",
      "Epoch: 646, loss: 1.7312147646221807\n",
      "Epoch: 647, loss: 1.7307625836944616\n",
      "Epoch: 648, loss: 1.73031035730054\n",
      "Epoch: 649, loss: 1.7298580856118388\n",
      "Epoch: 650, loss: 1.7294057687997684\n",
      "Epoch: 651, loss: 1.7289534070356474\n",
      "Epoch: 652, loss: 1.7285010004907548\n",
      "Epoch: 653, loss: 1.728048549336343\n",
      "Epoch: 654, loss: 1.7275960537435922\n",
      "Epoch: 655, loss: 1.727143513883618\n",
      "Epoch: 656, loss: 1.7266909299275333\n",
      "Epoch: 657, loss: 1.7262383020463594\n",
      "Epoch: 658, loss: 1.7257856304110915\n",
      "Epoch: 659, loss: 1.7253329151926602\n",
      "Epoch: 660, loss: 1.7248801565619514\n",
      "Epoch: 661, loss: 1.7244273546897966\n",
      "Epoch: 662, loss: 1.7239745097469905\n",
      "Epoch: 663, loss: 1.7235216219042604\n",
      "Epoch: 664, loss: 1.7230686913322872\n",
      "Epoch: 665, loss: 1.7226157182017205\n",
      "Epoch: 666, loss: 1.7221627026831279\n",
      "Epoch: 667, loss: 1.7217096449470397\n",
      "Epoch: 668, loss: 1.7212565451639554\n",
      "Epoch: 669, loss: 1.720803403504268\n",
      "Epoch: 670, loss: 1.7203502201383951\n",
      "Epoch: 671, loss: 1.7198969952366336\n",
      "Epoch: 672, loss: 1.7194437289692908\n",
      "Epoch: 673, loss: 1.7189904215065628\n",
      "Epoch: 674, loss: 1.7185370730186327\n",
      "Epoch: 675, loss: 1.7180836836756193\n",
      "Epoch: 676, loss: 1.7176302536476087\n",
      "Epoch: 677, loss: 1.7171767831046048\n",
      "Epoch: 678, loss: 1.71672327221656\n",
      "Epoch: 679, loss: 1.7162697211534184\n",
      "Epoch: 680, loss: 1.715816130085017\n",
      "Epoch: 681, loss: 1.715362499181189\n",
      "Epoch: 682, loss: 1.7149088286116705\n",
      "Epoch: 683, loss: 1.7144551185461832\n",
      "Epoch: 684, loss: 1.71400136915437\n",
      "Epoch: 685, loss: 1.713547580605847\n",
      "Epoch: 686, loss: 1.7130937530701411\n",
      "Epoch: 687, loss: 1.712639886716766\n",
      "Epoch: 688, loss: 1.7121859817151512\n",
      "Epoch: 689, loss: 1.711732038234713\n",
      "Epoch: 690, loss: 1.7112780564447598\n",
      "Epoch: 691, loss: 1.7108240365145915\n",
      "Epoch: 692, loss: 1.7103699786134374\n",
      "Epoch: 693, loss: 1.7099158829104761\n",
      "Epoch: 694, loss: 1.7094617495748348\n",
      "Epoch: 695, loss: 1.7090075787755747\n",
      "Epoch: 696, loss: 1.7085533706817437\n",
      "Epoch: 697, loss: 1.7080991254622817\n",
      "Epoch: 698, loss: 1.7076448432860913\n",
      "Epoch: 699, loss: 1.7071905243220702\n",
      "Epoch: 700, loss: 1.706736168738985\n",
      "Epoch: 701, loss: 1.7062817767056164\n",
      "Epoch: 702, loss: 1.7058273483906323\n",
      "Epoch: 703, loss: 1.7053728839627054\n",
      "Epoch: 704, loss: 1.7049183835904094\n",
      "Epoch: 705, loss: 1.704463847442285\n",
      "Epoch: 706, loss: 1.7040092756868124\n",
      "Epoch: 707, loss: 1.7035546684924006\n",
      "Epoch: 708, loss: 1.7031000260274576\n",
      "Epoch: 709, loss: 1.7026453484602753\n",
      "Epoch: 710, loss: 1.70219063595913\n",
      "Epoch: 711, loss: 1.7017358886922274\n",
      "Epoch: 712, loss: 1.70128110682774\n",
      "Epoch: 713, loss: 1.7008262905337324\n",
      "Epoch: 714, loss: 1.7003714399782632\n",
      "Epoch: 715, loss: 1.699916555329339\n",
      "Epoch: 716, loss: 1.6994616367548676\n",
      "Epoch: 717, loss: 1.6990066844227696\n",
      "Epoch: 718, loss: 1.6985516985008342\n",
      "Epoch: 719, loss: 1.6980966791568448\n",
      "Epoch: 720, loss: 1.6976416265585135\n",
      "Epoch: 721, loss: 1.697186540873519\n",
      "Epoch: 722, loss: 1.6967314222694267\n",
      "Epoch: 723, loss: 1.6962762709138148\n",
      "Epoch: 724, loss: 1.6958210869741714\n",
      "Epoch: 725, loss: 1.695365870617923\n",
      "Epoch: 726, loss: 1.6949106220124757\n",
      "Epoch: 727, loss: 1.6944553413251275\n",
      "Epoch: 728, loss: 1.6940000287231605\n",
      "Epoch: 729, loss: 1.693544684373795\n",
      "Epoch: 730, loss: 1.69308930844417\n",
      "Epoch: 731, loss: 1.6926339011014098\n",
      "Epoch: 732, loss: 1.6921784625125478\n",
      "Epoch: 733, loss: 1.691722992844565\n",
      "Epoch: 734, loss: 1.691267492264412\n",
      "Epoch: 735, loss: 1.6908119609389591\n",
      "Epoch: 736, loss: 1.6903563990350146\n",
      "Epoch: 737, loss: 1.689900806719351\n",
      "Epoch: 738, loss: 1.6894451841586933\n",
      "Epoch: 739, loss: 1.6889895315196393\n",
      "Epoch: 740, loss: 1.6885338489688297\n",
      "Epoch: 741, loss: 1.6880781366727842\n",
      "Epoch: 742, loss: 1.6876223947979905\n",
      "Epoch: 743, loss: 1.687166623510852\n",
      "Epoch: 744, loss: 1.6867108229777505\n",
      "Epoch: 745, loss: 1.6862549933649729\n",
      "Epoch: 746, loss: 1.6857991348387893\n",
      "Epoch: 747, loss: 1.6853432475653798\n",
      "Epoch: 748, loss: 1.684887331710889\n",
      "Epoch: 749, loss: 1.6844313874413808\n",
      "Epoch: 750, loss: 1.6839754149228925\n",
      "Epoch: 751, loss: 1.683519414321362\n",
      "Epoch: 752, loss: 1.6830633858027013\n",
      "Epoch: 753, loss: 1.682607329532782\n",
      "Epoch: 754, loss: 1.6821512456773764\n",
      "Epoch: 755, loss: 1.6816951344021887\n",
      "Epoch: 756, loss: 1.6812389958729246\n",
      "Epoch: 757, loss: 1.6807828302551857\n",
      "Epoch: 758, loss: 1.6803266377145238\n",
      "Epoch: 759, loss: 1.6798704184164277\n",
      "Epoch: 760, loss: 1.6794141725263687\n",
      "Epoch: 761, loss: 1.678957900209691\n",
      "Epoch: 762, loss: 1.678501601631735\n",
      "Epoch: 763, loss: 1.6780452769577592\n",
      "Epoch: 764, loss: 1.6775889263529546\n",
      "Epoch: 765, loss: 1.6771325499824852\n",
      "Epoch: 766, loss: 1.6766761480114243\n",
      "Epoch: 767, loss: 1.6762197206047995\n",
      "Epoch: 768, loss: 1.6757632679275758\n",
      "Epoch: 769, loss: 1.6753067901446816\n",
      "Epoch: 770, loss: 1.6748502874209543\n",
      "Epoch: 771, loss: 1.6743937599211771\n",
      "Epoch: 772, loss: 1.6739372078100736\n",
      "Epoch: 773, loss: 1.6734806312523414\n",
      "Epoch: 774, loss: 1.6730240304125625\n",
      "Epoch: 775, loss: 1.672567405455307\n",
      "Epoch: 776, loss: 1.6721107565450668\n",
      "Epoch: 777, loss: 1.6716540838462608\n",
      "Epoch: 778, loss: 1.6711973875232717\n",
      "Epoch: 779, loss: 1.6707406677404135\n",
      "Epoch: 780, loss: 1.6702839246619274\n",
      "Epoch: 781, loss: 1.669827158452018\n",
      "Epoch: 782, loss: 1.6693703692748028\n",
      "Epoch: 783, loss: 1.6689135572943588\n",
      "Epoch: 784, loss: 1.6684567226746978\n",
      "Epoch: 785, loss: 1.667999865579769\n",
      "Epoch: 786, loss: 1.6675429861734883\n",
      "Epoch: 787, loss: 1.6670860846196298\n",
      "Epoch: 788, loss: 1.6666291610819954\n",
      "Epoch: 789, loss: 1.6661722157242953\n",
      "Epoch: 790, loss: 1.6657152487101656\n",
      "Epoch: 791, loss: 1.6652582602032016\n",
      "Epoch: 792, loss: 1.6648012503669247\n",
      "Epoch: 793, loss: 1.664344219364783\n",
      "Epoch: 794, loss: 1.6638871673602016\n",
      "Epoch: 795, loss: 1.6634300945165086\n",
      "Epoch: 796, loss: 1.6629730009969999\n",
      "Epoch: 797, loss: 1.6625158869648702\n",
      "Epoch: 798, loss: 1.662058752583282\n",
      "Epoch: 799, loss: 1.6616015980153327\n",
      "Epoch: 800, loss: 1.661144423424069\n",
      "Epoch: 801, loss: 1.6606872289724457\n",
      "Epoch: 802, loss: 1.6602300148233666\n",
      "Epoch: 803, loss: 1.659772781139716\n",
      "Epoch: 804, loss: 1.6593155280842276\n",
      "Epoch: 805, loss: 1.6588582558196603\n",
      "Epoch: 806, loss: 1.6584009645086637\n",
      "Epoch: 807, loss: 1.6579436543138404\n",
      "Epoch: 808, loss: 1.6574863253977279\n",
      "Epoch: 809, loss: 1.6570289779227994\n",
      "Epoch: 810, loss: 1.6565716120514757\n",
      "Epoch: 811, loss: 1.656114227946082\n",
      "Epoch: 812, loss: 1.6556568257689142\n",
      "Epoch: 813, loss: 1.6551994056822215\n",
      "Epoch: 814, loss: 1.6547419678481332\n",
      "Epoch: 815, loss: 1.6542845124287548\n",
      "Epoch: 816, loss: 1.6538270395861312\n",
      "Epoch: 817, loss: 1.6533695494822191\n",
      "Epoch: 818, loss: 1.6529120422789372\n",
      "Epoch: 819, loss: 1.6524545181381303\n",
      "Epoch: 820, loss: 1.6519969772215912\n",
      "Epoch: 821, loss: 1.6515394196910111\n",
      "Epoch: 822, loss: 1.6510818457080751\n",
      "Epoch: 823, loss: 1.6506242554343484\n",
      "Epoch: 824, loss: 1.6501666490313804\n",
      "Epoch: 825, loss: 1.6497090266606327\n",
      "Epoch: 826, loss: 1.6492513884834832\n",
      "Epoch: 827, loss: 1.6487937346613033\n",
      "Epoch: 828, loss: 1.6483360653553396\n",
      "Epoch: 829, loss: 1.6478783807268191\n",
      "Epoch: 830, loss: 1.647420680936862\n",
      "Epoch: 831, loss: 1.646962966146568\n",
      "Epoch: 832, loss: 1.6465052365169488\n",
      "Epoch: 833, loss: 1.646047492208955\n",
      "Epoch: 834, loss: 1.645589733383476\n",
      "Epoch: 835, loss: 1.6451319602013301\n",
      "Epoch: 836, loss: 1.6446741728232759\n",
      "Epoch: 837, loss: 1.6442163714100093\n",
      "Epoch: 838, loss: 1.6437585561221566\n",
      "Epoch: 839, loss: 1.6433007271202869\n",
      "Epoch: 840, loss: 1.6428428845648855\n",
      "Epoch: 841, loss: 1.6423850286163986\n",
      "Epoch: 842, loss: 1.6419271594351845\n",
      "Epoch: 843, loss: 1.641469277181571\n",
      "Epoch: 844, loss: 1.6410113820157566\n",
      "Epoch: 845, loss: 1.6405534740979375\n",
      "Epoch: 846, loss: 1.6400955535882342\n",
      "Epoch: 847, loss: 1.6396376206466519\n",
      "Epoch: 848, loss: 1.6391796754331924\n",
      "Epoch: 849, loss: 1.6387217181077738\n",
      "Epoch: 850, loss: 1.6382637488302112\n",
      "Epoch: 851, loss: 1.6378057677603126\n",
      "Epoch: 852, loss: 1.6373477750577654\n",
      "Epoch: 853, loss: 1.636889770882236\n",
      "Epoch: 854, loss: 1.6364317553932972\n",
      "Epoch: 855, loss: 1.6359737287504643\n",
      "Epoch: 856, loss: 1.6355156911131912\n",
      "Epoch: 857, loss: 1.635057642640847\n",
      "Epoch: 858, loss: 1.6345995834927523\n",
      "Epoch: 859, loss: 1.6341415138281659\n",
      "Epoch: 860, loss: 1.6336834338062574\n",
      "Epoch: 861, loss: 1.6332253435861528\n",
      "Epoch: 862, loss: 1.632767243326893\n",
      "Epoch: 863, loss: 1.6323091331874569\n",
      "Epoch: 864, loss: 1.6318510133267747\n",
      "Epoch: 865, loss: 1.631392883903679\n",
      "Epoch: 866, loss: 1.630934745076967\n",
      "Epoch: 867, loss: 1.6304765970053368\n",
      "Epoch: 868, loss: 1.630018439847443\n",
      "Epoch: 869, loss: 1.6295602737618695\n",
      "Epoch: 870, loss: 1.6291020989071143\n",
      "Epoch: 871, loss: 1.628643915441636\n",
      "Epoch: 872, loss: 1.6281857235238082\n",
      "Epoch: 873, loss: 1.6277275233119426\n",
      "Epoch: 874, loss: 1.6272693149642699\n",
      "Epoch: 875, loss: 1.626811098638963\n",
      "Epoch: 876, loss: 1.6263528744941507\n",
      "Epoch: 877, loss: 1.6258946426878584\n",
      "Epoch: 878, loss: 1.6254364033780442\n",
      "Epoch: 879, loss: 1.6249781567226222\n",
      "Epoch: 880, loss: 1.624519902879426\n",
      "Epoch: 881, loss: 1.6240616420062215\n",
      "Epoch: 882, loss: 1.6236033742606986\n",
      "Epoch: 883, loss: 1.623145099800494\n",
      "Epoch: 884, loss: 1.622686818783163\n",
      "Epoch: 885, loss: 1.6222285313661942\n",
      "Epoch: 886, loss: 1.6217702377070133\n",
      "Epoch: 887, loss: 1.6213119379629832\n",
      "Epoch: 888, loss: 1.6208536322913771\n",
      "Epoch: 889, loss: 1.6203953208494095\n",
      "Epoch: 890, loss: 1.6199347253361083\n",
      "Epoch: 891, loss: 1.61947030537703\n",
      "Epoch: 892, loss: 1.6190052383721036\n",
      "Epoch: 893, loss: 1.6185401451337573\n",
      "Epoch: 894, loss: 1.6180751471953343\n",
      "Epoch: 895, loss: 1.6176102685482647\n",
      "Epoch: 896, loss: 1.617145514074179\n",
      "Epoch: 897, loss: 1.6166808848560172\n",
      "Epoch: 898, loss: 1.6162163811683712\n",
      "Epoch: 899, loss: 1.6157520030637056\n",
      "Epoch: 900, loss: 1.6152877504894683\n",
      "Epoch: 901, loss: 1.6148600929650592\n",
      "Epoch: 902, loss: 1.6144915619538884\n",
      "Epoch: 903, loss: 1.61412100737119\n",
      "Epoch: 904, loss: 1.6137498602140545\n",
      "Epoch: 905, loss: 1.6133783954983159\n",
      "Epoch: 906, loss: 1.6130066616350818\n",
      "Epoch: 907, loss: 1.6126346629297112\n",
      "Epoch: 908, loss: 1.6122623953952142\n",
      "Epoch: 909, loss: 1.6118898537644841\n",
      "Epoch: 910, loss: 1.6115170328498187\n",
      "Epoch: 911, loss: 1.6111439277937047\n",
      "Epoch: 912, loss: 1.6107705341029361\n",
      "Epoch: 913, loss: 1.61039684764103\n",
      "Epoch: 914, loss: 1.6100228646130903\n",
      "Epoch: 915, loss: 1.6096485815498345\n",
      "Epoch: 916, loss: 1.609273995292199\n",
      "Epoch: 917, loss: 1.6088991029765134\n",
      "Epoch: 918, loss: 1.6085239020204767\n",
      "Epoch: 919, loss: 1.608148390109734\n",
      "Epoch: 920, loss: 1.6077725651850654\n",
      "Epoch: 921, loss: 1.6073964254302906\n",
      "Epoch: 922, loss: 1.607018330854489\n",
      "Epoch: 923, loss: 1.6066371138467586\n",
      "Epoch: 924, loss: 1.6062552429918824\n",
      "Epoch: 925, loss: 1.605873187383003\n",
      "Epoch: 926, loss: 1.605491022198914\n",
      "Epoch: 927, loss: 1.605108745757184\n",
      "Epoch: 928, loss: 1.6047263420751077\n",
      "Epoch: 929, loss: 1.6043437931809401\n",
      "Epoch: 930, loss: 1.603961081507512\n",
      "Epoch: 931, loss: 1.6035781903302027\n",
      "Epoch: 932, loss: 1.6031951038210126\n",
      "Epoch: 933, loss: 1.6028118070283186\n",
      "Epoch: 934, loss: 1.6024282858436791\n",
      "Epoch: 935, loss: 1.6020445269674006\n",
      "Epoch: 936, loss: 1.6016605178752388\n",
      "Epoch: 937, loss: 1.6012762467865456\n",
      "Epoch: 938, loss: 1.600891702633997\n",
      "Epoch: 939, loss: 1.6005068750345168\n",
      "Epoch: 940, loss: 1.6001217542617354\n",
      "Epoch: 941, loss: 1.5997363312195598\n",
      "Epoch: 942, loss: 1.5993505974168938\n",
      "Epoch: 943, loss: 1.5989645449435255\n",
      "Epoch: 944, loss: 1.5985781664470147\n",
      "Epoch: 945, loss: 1.598191455110553\n",
      "Epoch: 946, loss: 1.5978044046317503\n",
      "Epoch: 947, loss: 1.5974170092023072\n",
      "Epoch: 948, loss: 1.597029263488442\n",
      "Epoch: 949, loss: 1.5966411626121777\n",
      "Epoch: 950, loss: 1.5962527021333908\n",
      "Epoch: 951, loss: 1.5958638780323426\n",
      "Epoch: 952, loss: 1.5954746866932368\n",
      "Epoch: 953, loss: 1.5950851248880453\n",
      "Epoch: 954, loss: 1.5946951897611503\n",
      "Epoch: 955, loss: 1.5943048788145233\n",
      "Epoch: 956, loss: 1.5939141898933795\n",
      "Epoch: 957, loss: 1.5935231211724086\n",
      "Epoch: 958, loss: 1.593131671142519\n",
      "Epoch: 959, loss: 1.5927398385979483\n",
      "Epoch: 960, loss: 1.5923476226239763\n",
      "Epoch: 961, loss: 1.591955022584948\n",
      "Epoch: 962, loss: 1.5915620381127655\n",
      "Epoch: 963, loss: 1.5911686690957751\n",
      "Epoch: 964, loss: 1.5907749156680167\n",
      "Epoch: 965, loss: 1.5903807781988666\n",
      "Epoch: 966, loss: 1.5899862572829906\n",
      "Epoch: 967, loss: 1.589591353730695\n",
      "Epoch: 968, loss: 1.589196068558539\n",
      "Epoch: 969, loss: 1.5888004029803247\n",
      "Epoch: 970, loss: 1.588404358398323\n",
      "Epoch: 971, loss: 1.5880079363948307\n",
      "Epoch: 972, loss: 1.5876111387240448\n",
      "Epoch: 973, loss: 1.5872139673041044\n",
      "Epoch: 974, loss: 1.5868164242094778\n",
      "Epoch: 975, loss: 1.5864185116636564\n",
      "Epoch: 976, loss: 1.5860202320318995\n",
      "Epoch: 977, loss: 1.5856215878144095\n",
      "Epoch: 978, loss: 1.5852225816396945\n",
      "Epoch: 979, loss: 1.584823216258044\n",
      "Epoch: 980, loss: 1.584423494535388\n",
      "Epoch: 981, loss: 1.5840234194472196\n",
      "Epoch: 982, loss: 1.5836229940728173\n",
      "Epoch: 983, loss: 1.5832222215895908\n",
      "Epoch: 984, loss: 1.5828211052676737\n",
      "Epoch: 985, loss: 1.5824196484647333\n",
      "Epoch: 986, loss: 1.5820178546208035\n",
      "Epoch: 987, loss: 1.5816157272534863\n",
      "Epoch: 988, loss: 1.5812132699531898\n",
      "Epoch: 989, loss: 1.5808104863785948\n",
      "Epoch: 990, loss: 1.580407380252236\n",
      "Epoch: 991, loss: 1.5800039553563006\n",
      "Epoch: 992, loss: 1.5796002155285094\n",
      "Epoch: 993, loss: 1.5791961646581478\n",
      "Epoch: 994, loss: 1.5787918066823563\n",
      "Epoch: 995, loss: 1.5783871455823641\n",
      "Epoch: 996, loss: 1.5779821853800147\n",
      "Epoch: 997, loss: 1.577576930134369\n",
      "Epoch: 998, loss: 1.567537230034472\n",
      "Epoch: 999, loss: 1.5651291570763952\n",
      "Epoch: 1000, loss: 1.5936632873265273\n"
     ]
    }
   ],
   "source": [
    "# initialize weights and biases\n",
    "# in Keras/TensorFlow/PyTorch etc. these are usually randomized in the beginning\n",
    "w1 = 1\n",
    "w2 = 0.5\n",
    "w3 = 1\n",
    "w4 = -0.5\n",
    "w5 = 1\n",
    "w6 = 1\n",
    "bias1 = 0.5\n",
    "bias2 = 0\n",
    "bias3 = 0.5\n",
    "\n",
    "# just for comparison after the training\n",
    "original_w1 = 1\n",
    "original_w2 = 0.5\n",
    "original_w3 = 1\n",
    "original_w4 = -0.5\n",
    "original_w5 = 1\n",
    "original_w6 = 1\n",
    "original_b1 = 0.5\n",
    "original_b2 = 0\n",
    "original_b3 = 0.5\n",
    "\n",
    "# our training data\n",
    "# x1 = input1, x2 = input2, y = true_value\n",
    "data = [\n",
    "    [1, 0, 2],\n",
    "    [2, 1, 6],\n",
    "    [3, 3, 17]\n",
    "]\n",
    "\n",
    "# use generated training data from our helper function\n",
    "data = generate_train_data()\n",
    "\n",
    "# learning rate\n",
    "LR = 0.001\n",
    "epochs = 1000\n",
    "\n",
    "# let's initalize a list for loss visualizations\n",
    "loss_points = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for row in data:\n",
    "        # this is where we do Forward pass + backpropagation\n",
    "        input1 = row[0]\n",
    "        input2 = row[1]\n",
    "        true_value = row[2]\n",
    "\n",
    "        # NODE 1 OUTPUT\n",
    "        node_1_output = input1 * w1 + input2 * w3 + bias1\n",
    "        node_1_output = activation_ReLu(node_1_output)\n",
    "        node_1_output\n",
    "\n",
    "        # NODE 2 OUTPUT\n",
    "        node_2_output = input1 * w2 + input2 * w4 + bias2\n",
    "        node_2_output = activation_ReLu(node_2_output)\n",
    "        node_2_output\n",
    "\n",
    "        # NODE 3 OUTPUT\n",
    "        # we can just use Node 1 and 2 outputs, since they\n",
    "        # already contain the previous weights in their result\n",
    "        node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "        node_3_output = activation_ReLu(node_3_output)\n",
    "        node_3_output\n",
    "\n",
    "        # LOSS FUNCTION - we are going to use MSE -> mean squared error\n",
    "        # MSE formula for LOSS => (predicted_value - true_value) ^ 2\n",
    "        predicted_value = node_3_output\n",
    "        loss = (predicted_value - true_value) ** 2\n",
    "        \n",
    "        # BACKPROPAGATION - LAST LAYER FIRST\n",
    "        # solving the partial derivative of the loss function with respect to w5\n",
    "        deriv_L_w5 = 2 * node_1_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w5 = w5 - LR * deriv_L_w5\n",
    "\n",
    "        deriv_L_w6 = 2 * node_2_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w6 = w6 - LR * deriv_L_w6\n",
    "\n",
    "        deriv_L_b3 = 2 * 1 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_b3 = bias3 - LR * deriv_L_b3\n",
    "\n",
    "        # BACKPROPAGATION - THE FIRST LAYER\n",
    "        # FROM THIS POINT ONWARD WE HAVE TO USE THE MORE COMPLEX VERSION\n",
    "        # OF UPDATING THE VALUES => CHAIN RULE\n",
    "\n",
    "        # weight 1\n",
    "        deriv_L_w1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input1\n",
    "        deriv_L_w1 = deriv_L_w1_left * deriv_L_w1_right\n",
    "        new_w1 = w1 - LR * deriv_L_w1\n",
    "\n",
    "        deriv_L_w2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w2_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input1\n",
    "        deriv_L_w2 = deriv_L_w2_left * deriv_L_w2_right\n",
    "        new_w2 = w2 - LR * deriv_L_w2\n",
    "\n",
    "        deriv_L_w3_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w3_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input2\n",
    "        deriv_L_w3 = deriv_L_w3_left * deriv_L_w3_right\n",
    "        new_w3 = w3 - LR * deriv_L_w3\n",
    "\n",
    "        deriv_L_w4_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w4_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input2\n",
    "        deriv_L_w4 = deriv_L_w4_left * deriv_L_w4_right\n",
    "        new_w4 = w4 - LR * deriv_L_w4\n",
    "\n",
    "        deriv_L_b1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * 1\n",
    "        deriv_L_b1 = deriv_L_b1_left * deriv_L_b1_right\n",
    "        new_b1 = bias1 - LR * deriv_L_b1\n",
    "\n",
    "        deriv_L_b2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b2_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * 1\n",
    "        deriv_L_b2 = deriv_L_b2_left * deriv_L_b2_right\n",
    "        new_b2 = bias2 - LR * deriv_L_b2\n",
    "\n",
    "        # ALL DONE! FINALLY UPDATE THE EXISTING WEIGHTS\n",
    "        w1 = new_w1\n",
    "        w2 = new_w2\n",
    "        w3 = new_w3\n",
    "        w4 = new_w4\n",
    "        w5 = new_w5\n",
    "        w6 = new_w6\n",
    "        bias1 = new_b1\n",
    "        bias2 = new_b2\n",
    "        bias3 = new_b3\n",
    "\n",
    "    loss_points.append(loss)\n",
    "    print(f\"Epoch: {epoch + 1}, loss: {loss}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAGdCAYAAABO2DpVAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAJpZJREFUeJzt3QmQVNXd9/F/z9IzwzAzDCMIyLC4gYhYLkRxSyVuQVxj8VZ80aBJxVeCcclmSMpEy9IhST151CxEfRLN80Tk1argViJxA/RVEDAqYkQQFAQB2WaBoWe7b/1P9+3pHkCm8fY93ae/n6qb7unuGe7cjD2/+Z//OSfieZ4nAAAAASgK4osAAAAoggUAAAgMwQIAAASGYAEAAAJDsAAAAIEhWAAAgMAQLAAAQGAIFgAAIDAlErKuri7ZtGmTVFVVSSQSCfufBwAAh0DX02xubpYhQ4ZIUVFR7gQLDRX19fVh/7MAACAAGzZskKFDh+ZOsNBKhX9i1dXVYf/zAADgEDQ1NZnCgP97PGeChT/8oaGCYAEAQH45WBsDzZsAACAwBAsAABAYggUAAAgMwQIAAASGYAEAAAJDsAAAAIEhWAAAgMAQLAAAQGAIFgAAIDAECwAAEBiCBQAAsBcsNm7cKFdffbXU1dVJRUWFnHDCCbJs2bLgzggAAOStjDYh27lzp5x55pnyta99TebNmycDBgyQ1atXS21trdj2u3+ukqa9HXLDV4+SQTXltk8HAICClFGw+PWvf222TH344YeTj40cOVJywWNLN8jnzTH5X6fWEywAAMiHoZCnn35aTj31VJk8ebIMHDhQTjrpJHnooYe+8HNisZjZwz31yIaixC6unnhZ+foAACDgYLF27VqZNWuWHHPMMTJ//nyZNm2a3HTTTfK3v/3tgJ/T0NAgNTU1yUMrHtkQkXiy8MgVAABYE/G83v8qjkajpmLx+uuvJx/TYLF06VJ54403Dlix0MOnFQsNF42NjVJdXS1BmdDwknzWuFeeufEsOWFoTWBfFwAAiPn9rQWCg/3+zqhiMXjwYBkzZkzaY8cdd5ysX7/+gJ9TVlZmTiD1yIbESAhDIQAAWJRRsNAZIatWrUp77MMPP5Thw4eLbZEIQyEAAORVsLj11ltl8eLFcs8998iaNWtk9uzZ8uCDD8r06dPFtkSukC6SBQAA+REsxo8fL3PnzpXHHntMxo4dK3fddZfce++9MmXKFMmVYEGsAAAgT9axUBdffLE5cg2zQgAAsM+ZvUL8igU1CwAA7HEnWCRuqVgAAGCPM8GiKFGy6CJYAABgjTPBwi9ZZLDeFwAACJgzwaJ7gSwAAGCLO8GCBbIAALDOmWDB7qYAANjnTLBgHQsAAOxzJ1gkmzdtnwkAAIXLmWDhYygEAAB7nAkWNG8CAGCfc82b7G4KAIA9zgQLdjcFAMA+d4JFculN22cCAEDhcidYsI4FAADWuRMsEre0WAAAYI87wYLdTQEAsM6hYBG/ZXdTAADscSdYJG6JFQAA2ONOsGCBLAAArHMnWCRuGQoBAMAeZ4JFkV+xsH0iAAAUMGeCRXJ9LJIFAADWONi8SbIAAMAWd4IFFQsAAKxzJ1gkahbsbgoAgD3OBIsiZ74TAADyl3MVCwoWAADY406wYHdTAACscyZY+Lq6bJ8BAACFy70lvW2fCAAABcyZYFHE7qYAAFjnTLBgd1MAAOxzJ1h0d28CAABL3AkWiVsWyAIAwB53ggXNmwAAWOdQsIjfUrAAAMAed4JF4pYFsgAAsMedYEHFAgAA6xzcK4RkAQCALc7tbkqsAADAHmeCBbubAgBgnzPBwu/eZCgEAAB7HFwgy/KJAABQwNwJFiyQBQCAdc4EC3Y3BQDAPueGQgAAgD3uDYVQsAAAwBp3gkXilt1NAQCwx73pprbPAwCAAuZMsChiKAQAAOucCRbsbgoAgH3uBAt2NwUAwDp3ggW7mwIAYJ07wYKKBQAA1jkULFjSGwAA2xwKFvFbKhYAANjjTrBI3LJAFgAA9rgTLFggCwAA65xbIIuxEAAA7HFwgSwAAGCLO8EiUbGgxwIAAHucCRY+cgUAAPY4Eyxo3gQAIM+CxR133GGGHFKP0aNHSy5gd1MAAOwryfQTjj/+eHnxxRe7v0BJxl8iK9jdFAAA+zJOBRokBg0aJLmG2aYAAORhj8Xq1atlyJAhcuSRR8qUKVNk/fr1X/j6WCwmTU1NaUdW9wohWQAAkB/B4rTTTpNHHnlEnn/+eZk1a5asW7dOzj77bGlubj7g5zQ0NEhNTU3yqK+vl6wOhZArAADIj2AxceJEmTx5sowbN04uvPBCee6552TXrl3y+OOPH/BzZsyYIY2Njcljw4YNkg3sbgoAgH1fqvOyX79+cuyxx8qaNWsO+JqysjJzZBs9FgAA5Pk6Fi0tLfLRRx/J4MGDxTZ2NwUAIM+CxY9//GNZuHChfPzxx/L666/LFVdcIcXFxXLVVVeJbX7FAgAA5MlQyKeffmpCxPbt22XAgAFy1llnyeLFi8192yKJmgWzQgAAyJNgMWfOHMlVRSzpDQCAdc7sFeKPhdBjAQCAPc4EC9axAADAPneCBUMhAABY506wSDZv2j4TAAAKlzPBwm/epGYBAIA9zg2FdHXZPhMAAAqXQ8HC3yuEigUAALY4Eyx89FgAAGCPM8GiiN1NAQCwzr0eC0oWAABY406w8O+QKwAAsMa5oRAqFgAA2ONMsCgrjX8rSz/eKR2dzDkFAMAGd4JFSfxb2birVX79/Ae2TwcAgILkTLAoLy1O3n/o1XVWzwUAgELlXMUCAADY48xv47KUigUAALDDnWBBxQIAAOuc+W1cVkLFAgAA25wJFuWJ6aYAAMAeZ34bU7EAAMA+JysW/r4hAAAgXE5WLPzlvQEAQLjcCRYpFYvOLvYLAQDABmeCRTk9FgAAWOdMsCgt7h7+qO1TavVcAAAoVM4Ei0gkIo9cN97cLy125tsCACCvOPUbeGhthbltY9t0AACscCpYRIvjfRZtHQQLAABscCtYJPYLiREsAACwwslgodNNmXIKAED4nAoWqTucMhwCAED4nKxYKIIFAADhcypYlBRFkvuExDo7bZ8OAAAFx6lgoWtZRBNrWFCxAAAgfE4FC8XMEAAA7HEuWPi7nFKxAAAgfA4GC4ZCAACwxdmhEJb1BgAgfO4FC5o3AQCwxuHmTaabAgAQNueCBT0WAADY41ywYLopAAD2OBssqFgAABA+d5s3mRUCAEDo3B0KaSdYAAAQNndX3qRiAQBA6NwLFqX0WAAAYIuzPRasYwEAQPicCxZULAAAsMe9YJGsWBAsAAAIm3vBojTevMmsEAAAwudcsGAdCwAA7HG2x4LmTQAAwudcsGDbdAAA7HG4YkGwAAAgbM4Fi2hxonmTYAEAQOicCxZlbJsOAIA1zgULtk0HAMAehysWzAoBACBszgULKhYAANjj7Lbp9FgAABA+54IFFQsAAOxxLljQYwEAgD1Ob5vueZ7t0wEAoKB8qWAxc+ZMiUQicsstt0iuKEsskNXliXTo/wAAgNwPFkuXLpUHHnhAxo0bJ7lYsVA0cAIAkAfBoqWlRaZMmSIPPfSQ1NbWSi5uQqZo4AQAIA+CxfTp02XSpEly3nnnHfS1sVhMmpqa0o5sKiqKSGlxJP5v08AJAECoSjL9hDlz5shbb71lhkJ6o6GhQe68804Ju2rR3tlJxQIAgFyuWGzYsEFuvvlmefTRR6W8vLxXnzNjxgxpbGxMHvo1sq2slEWyAADI+YrF8uXLZevWrXLyyScnH+vs7JRFixbJH/7wBzPsUZyYleErKyszh40+CyoWAADkcLA499xzZcWKFWmPXXfddTJ69Gi57bbb9gkVtmeG0GMBAEAOB4uqqioZO3Zs2mOVlZVSV1e3z+M2+RULhkIAAAiXcytvplcsCBYAAOT0rJCeFixYILmGHgsAAOxws2LB1ukAAFjhZLBg63QAAOxwMliwdToAAHa4GSwSC2RRsQAAIFxOBgummwIAYIfb003bCRYAAITJyWCRnG7aSY8FAABhcjJYULEAAMAON4NFsmJBsAAAIExuBgt/23QqFgAAhMrxHguCBQAAYXIyWLBtOgAAdjgZLNiEDAAAO5wMFmybDgCAHU4Gi2gxu5sCAGCD45uQESwAAAiTk8GCbdMBALDDyWDBtukAANjhZLCgYgEAgB1OBouyEpo3AQCwwfFNyBgKAQAgTE4GC5b0BgDADucXyPI8z/bpAABQMNwMFokFsjRTdHQRLAAACIvTFQtFAycAAOFxusdCMeUUAIDwOBksiooiUlocMfdZJAsAgPA4GSwUW6cDABA+Z4NFWSmLZAEAEDZngwUVCwAAwudssOhey4IeCwAAwuJ8xYKhEAAAwuNssEhdfRMAAITD2WBBjwUAAOFzNliwdToAAOFzN1iwdToAAKFzNliwdToAAOFzf4GsdoIFAABhcTZYULEAACB8BdBjQbAAACAsBVCxoHkTAICwOBssqFgAABA+d4MFPRYAAITO3WDBrBAAAELnbLBgVggAAOFzNliwbToAAOFzNliwCRkAAOFzNliwbToAAOFzNlhEi9ndFACAsDkbLMpKqFgAABA2Z4NF1A8WbJsOAEBonK9YMN0UAIDwuBssWCALAIDQORssWCALAIDwFcAmZPRYAAAQFmeDBRULAADCVxALZHmeZ/t0AAAoCO4Gi8QCWZopOroIFgAAhMH5ioVikSwAAMLhfI+FYiMyAADC4WywKCqKSGlxxNxn63QAAMLhbLBQbJ0OAEC4nA4WydU3CRYAAITC6WBBxQIAgBwOFrNmzZJx48ZJdXW1OSZMmCDz5s2T3F/Lgh4LAAByLlgMHTpUZs6cKcuXL5dly5bJ17/+dbnssstk5cqVkssVCzYiAwAgHCWZvPiSSy5J+/juu+82VYzFixfL8ccfLzlbsWBZbwAAci9YpOrs7JQnnnhCdu/ebYZEDiQWi5nD19TUJGEpK2HrdAAAcrp5c8WKFdK3b18pKyuTG264QebOnStjxow54OsbGhqkpqYmedTX10tY2IgMAIAcDxajRo2St99+W5YsWSLTpk2TqVOnyvvvv3/A18+YMUMaGxuTx4YNGyQsbJ0OAECOD4VEo1E5+uijzf1TTjlFli5dKvfdd5888MAD+329Vjb0sIGKBQAAebaORVdXV1oPRU4ukEWPBQAAuVex0GGNiRMnyrBhw6S5uVlmz54tCxYskPnz50suomIBAEAOB4utW7fKt7/9bfnss89MI6YulqWh4vzzz5dc1N1jQbAAACDngsVf/vIXySfdFQuaNwEACIPTe4VQsQAAIFxuBwt6LAAACJXbwYJZIQAAhMrpYMGsEAAAwuV0sGDbdAAAwuV0sGDbdAAAwlUQFQuGQgAACIfTwSJaTPMmAABhcjpYlJUkhkKoWAAAEAq3gwXbpgMAECqngwXTTQEACJfTwYIFsgAACJfTwYKKBQAA4XI6WNBjAQBAuJwOFlQsAAAIV4Es6d0lnufZPh0AAJzndrBILJClmaKji2ABAEC2FUTFQrXSZwEAQNa5HSxKiqS0OGLut+ztsH06AAA4z+lgEYlEpKq81NxvJlgAAJB1TgcLVVVeYm6b97bbPhUAAJznfLCoTlQsmggWAABkXQFVLBgKAQAg2wqnYtFKxQIAgGwrmIpFExULAACyrgCCBbNCAAAIi/PBorrCr1gwFAIAQLY5HyyoWAAAEJ4CCBaJigXNmwAAZF3BzAphgSwAALKvAIIF61gAABAW94NFBStvAgAQFueDBStvAgAQngIIFvGKxZ62Tuno7LJ9OgAAOK1gKhaKqgUAANnlfLAoLS6SitJic59gAQBAdjkfLBSrbwIAEI6CCBZ+nwXBAgCA7CqQYMHMEAAAwlAQwcJffZNlvQEAyK6CCBZULAAACEdBBAtW3wQAIBwFESyoWAAAEI6CCBbscAoAQDgKJFgk1rFopWIBAEA2FdQ6Fs0xKhYAAGRTgQQLeiwAAAhDYc0KYR0LAACyqiCCBRULAADCUVgrb+5tF8/zbJ8OAADOKqiKRXunJ7GOLtunAwCAswoiWFRGSyQSid9n9U0AALKnIIJFUVFEqspYywIAgGwriGCRtpYFFQsAALKmYIJF90Zk3RWLV1d/LlfOel1u+J/lsq0lZvHsAABwQ3x8oAD0SwSLXXvazO27n+6S7zyy1DR0qu27YzLn+glSXJRoxgAAABkrmIpFXd+oud3W0mamnN75zPsmVJxwRI30iRbL0o93yiOvf2z7NAEAyGsFEywO61tmbnXI47U122T5JzulrKRI/jL1VPnFpOPMc39e+JHsbe+0fKYAAOSvAgoWiYpFc0zue3G1uf+/TxsmA6vLZfIp9TKkplw+b47JzHkfsIgWAACHqOAqFq9/tF2WfbJTosVFcsNXjzKPRUuK5OeJqoUOh/xnIngAAIDMFFyw2Lir1dyec+wAOby6PPn8xeOGyB2XjDH3739ptdz74odULgAAyFDBNW/6Jo0btM9rrj1zpPxs4mhz/94XV5thka4uwgUAAL1VcBUL39dHHb7f1+nwyO0XxysXDyxaK9f/z3IW1QIAoJcKMlgcXl0mNX3i61rsz3fPGin/MflE03vx4r+3yEX3vyr/b822kM4UAIACCRYNDQ0yfvx4qaqqkoEDB8rll18uq1atknxQES1O3h9eV3nQ1195ylB54v9MkCP6VciGHa0y5b+WyA8e+5es/bwly2cKAECBBIuFCxfK9OnTZfHixfLCCy9Ie3u7XHDBBbJ7927JB6cMrzW3Pzr/2F69/sT6fjL/1nPk2xOGm4+feWeTnP+fi+TW//u2vLV+J82dAAD0EPG+xG/Hzz//3FQuNHCcc845vfqcpqYmqampkcbGRqmurpYw6ToVunT36EGZ/7srNzXKf/zzQ3n5g63Jx0YPqpKLThgs3xg7SI4Z2Fci/t7sAAA4pre/v79UsFizZo0cc8wxsmLFChk7dmygJ5ar3tmwS/77jU/k2Xc3SayjK/m4DpmcdmR/Of3IOjl1eK2MqKs027UDAOCCrAeLrq4uufTSS2XXrl3y2muvHfB1sVjMHKknVl9fn7fBwqebmf3z/S0y/73N8urqbdLW2R0ylO4/ctzgajl+SLWMGlQlI+sqZcRhlTKoupzAAQDIO1kPFtOmTZN58+aZUDF06NADvu6OO+6QO++8c5/H8z1YpNod6zB7jyxZt10Wr90h721sTKtmpNL9SYbX9TEVjSH9KmRwTbkMqik39zV06KJdOhsFAICCCRY33nijPPXUU7Jo0SIZOXLkF77W1YrFF+no7JJ123bL+581ycpNTbJ6S7N8sn2PrN+xRzoOsuCWtmno1NgBfcvksKoyOawyGr/tG5W6ysRjfaPmNf0ro1JaTAgBAORpsNCX/uAHP5C5c+fKggULTH9Ftk7M1cChS4pr6NCQsWnXXtnc2CqbGvU2fvQcUjmY2j6lUtsnKrWVUXO/n97Xx8zH0bTn+yXuE0YAANn6/V2SyRfVqaazZ8821Qpdy2Lz5s3mcf2HKioqMj7JQlNSrMMglQdcR0OD2/bdbSZgfN4SMzuxbmtpk+163xxtydsdu2OixY+de9rNIdt6P+W3qqxE+lXGQ4YGkf7JQKIBJH6/X4X/vH5cKn3LSpj1AgA4qIwqFgf6xfLwww/Ltdde26uvUcgViyB1dnmyc4+GjjZzu3N3WyJktJnG0h27281t/ON22bGnTRpb2+VQ5wCVFEUSISMeOvzKSPKxRDUk+ZwGlIpo2sJkAID8lZWKBQtC5Y7ioojps+i5B8rBwkhTazx8xMNIe1rwMEFEA0lr/DE99HltRNXekHjFpC2j89RmVT901KRVQXoEE32usjuY0MAKAPkpo2CB/A8jpveiMn2n14Npbes0YWN/oUPDSPx+uzTqaxLP6eMaRjSUbGmKmSMTldHiZCUkLXx8QTDR4KLDTQAAewgWOCgdzqiI6tTYioyqW7vbOs0QjQkaidDRuCc9fJhw0tr9sQ7XaO+Ifu7utlbT7JqJ6vKSZOio8cNH2tBNj2BSEZWq8hLWFgGAgBAskBXaj6MNn3rU9+/953V1edK8tyM5XBMPHSlVkUQw2ZkIIf5Qjn6OatrbYY71O3r/b2qm6DlMY24r0qsiqc2s+rFWVWhoBYB0BAvkFK0c6Jb2eoyQg+9CmzqVNx40UoNIevhIVk4Sja0aWva0dabPrslAaXFEahLhw/SQmNt9g0n88e5gUl5KQysAdxEs4ATtrajrW2aOTMQ6OqUxESriQzPdocMEk2QvSWpfSbtZb6S9Uxta41OBM1FeWmRCh1/98Kf9xu/HA0l6H0m88ZWGVgD5gGCBglZWUiwDq/Uoz6h/pLW9s7sqkggmyQrJ7p5DOH6fSbuZmbO3vUs2t++VzU17MzpXHVYyQzaJqbzdoSR1Zk1pWhWluqLUNO0CQFgIFkCGtK+iT7TEHLrHSyaBpDnWkVYF6Q4j+xu6iQcUf/2RlliHOTJpaNUWkOrynjNreqw90mPWjQ7daBMs/SMADgXBAgiJ/qLWX/J61Pfvk/H6I/sfnkkM3/SokOhrNMRoINFgoofuV9NbWuUwU3h7LHyWHK5JCSbxKkr8NbqrL4EEKGwECyCP1h8ZmUFDa3uiobW7d6RHMPGf250eTHSYR8OMLi+vh0jvl4uPFhf1fu2RlGBCQyvgDoIF4CjdbC7T1VnV3vbO5JCMho6eC5+l9410N7xqM6s2tW5tjpkjExWlxQdekTU54yb9OQ0kbKgH5B6CBYA0Wj3Q4/AMG1p16q4ZrjnIomj+kI7/sU731SpJa2OnfNaYWUOrbqiXHK5JCR/J3X0Tu/6m3tcQw3ANkD0ECwBfmv6iriwrMccRGTS0mgXRUhtak8Mz6SuypvaR6HO6CJrSz9Xj052tGe1fkxY6UsNHYgM9/35/7R1hd18gIwQLAHYXREvs8zKsLrOG1tT+ETNck5hZEz/ilZOem+3pUI3uX6NTfTOZ7quLofnDNN0BZN9qiF8p6d+HpeJRuAgWAPKyoVWrCXocyv41+4SPtPvpIUXXHdH+kc+bY+boLc0UaUM0+wkf/VKHbRK9I6w7gnxHsABQgPvX9Mlod9/9BY4DV0jaTIDR3pFMZ9b46474QzDx8NEjkOgCaSnDNPoYTazIJQQLAOjF7r6ZLIbmLxW/Y58w0r2JXs/72jeSuu5Ipk2s/Sq7g0hq6PArJT2fY4ovsoVgAQA5sFS8bqTnN6ju2J0ePuKPpQSRlAZXDSN+E+uGHb1vYtXFzPyhmnjY0OGZfashyepJZZQZNegVggUA5MhGepmuO6Kzapr2tneHjtQ+kZ6BJCWkdHTFpwfvaWvNaIl43Qgv2RvSI3TEA0kimPi7+VaWmmoKYaSwECwAIE/prJP4KqaZNbFqdWPX7sRQTeqwTCKY6AyaHT1CSltHlzkynVFTkjjHtNCRcj89mMSHbbTPhBk1+YtgAQAFumdNb6f4+gug7RM60oZnuisk/mt04TOtjmxriZmjtzRTpC777s+Y6b71l4nvnv5L30juIFgAAHq9ANrQ2syWh/dn0JjQsZ8ZNMlAoqFld3zzPJ1Ro8FkR4Z71WgPSDJ8VO67X42/AJofTqiOZAfBAgCQFVpBGFxTYY7e0uEWXQ7er3r4i6D51RI/jHTvYRO/1UXT/KXhN2WwNLy2f9T0CB/7Vkri97v3rIma2ULYP4IFACBnaIPowKpycxxK34jfF+Jvlpe6X03q43rbEotP8Y0vHZ/ZFF9/aXgNIf7qsdWJ25qUo7qiZJ/nddaQywgWAICC6xvpWR1JHZZJbpSXshx8ah+J9o0cytLwqaEkPXzse7+6vDuQaKUk/lipmSb8RbNspv71TfPan1w4KqOF4IJEsAAAFKRDrY5opSO18uEvaqZH0952aWrt8VhrR/I5rZBoKNnaHDNHpnTfGg0YfgjpDiUlUlJUJAs//Ny87leXjBFbCBYAAPSSVguqykvNkWlFoCuxm68fPFIDiIaO7jCSCCI9XqOVEt23pnup+P0bWlshdRmshxI0ggUAACHv5luf4edqpUSbU5PhI1Ep0aXg9XZ7S0z+tOAj89rxI/qLTQQLAADyoFLSJ1pijgPNsply+nB58f0tMnHsILGJYAEAgAOO6FchU88YYfs0hL12AQBAYAgWAAAgMAQLAAAQGIIFAAAIDMECAAAEhmABAAACQ7AAAACBIVgAAIDAECwAAEBgCBYAACAwBAsAABAYggUAAAgMwQIAAOTv7qa6p7xqamoK+58GAACHyP+97f8ez5lg0dzcbG7r6+vD/qcBAEAAv8dramoO+HzEO1j0CFhXV5ds2rRJqqqqJBKJBJqkNKxs2LBBqqurA/u6SMd1Dg/XOhxc53BwnfP/Wmtc0FAxZMgQKSoqyp2KhZ7M0KFDs/b19SLyQ5t9XOfwcK3DwXUOB9c5v6/1F1UqfDRvAgCAwBAsAABAYJwJFmVlZfKrX/3K3CJ7uM7h4VqHg+scDq5z4Vzr0Js3AQCAu5ypWAAAAPsIFgAAIDAECwAAEBiCBQAACIwzweKPf/yjjBgxQsrLy+W0006TN9980/Yp5Y2GhgYZP368WQ114MCBcvnll8uqVavSXrN3716ZPn261NXVSd++feXKK6+ULVu2pL1m/fr1MmnSJOnTp4/5Oj/5yU+ko6Mj5O8mf8ycOdOsPnvLLbckH+M6B2fjxo1y9dVXm2tZUVEhJ5xwgixbtiz5vPat//KXv5TBgweb58877zxZvXp12tfYsWOHTJkyxSwy1K9fP/nud78rLS0tFr6b3NTZ2Sm33367jBw50lzDo446Su666660vSS4zodm0aJFcskll5hVLvV94sknn0x7Pqjr+u6778rZZ59tfnfqap2/+c1vDvGM008u782ZM8eLRqPeX//6V2/lypXe9773Pa9fv37eli1bbJ9aXrjwwgu9hx9+2Hvvvfe8t99+27vooou8YcOGeS0tLcnX3HDDDV59fb330ksvecuWLfNOP/1074wzzkg+39HR4Y0dO9Y777zzvH/961/ec8895x122GHejBkzLH1Xue3NN9/0RowY4Y0bN867+eabk49znYOxY8cOb/jw4d61117rLVmyxFu7dq03f/58b82aNcnXzJw506upqfGefPJJ75133vEuvfRSb+TIkV5ra2vyNd/4xje8E0880Vu8eLH36quvekcffbR31VVXWfqucs/dd9/t1dXVec8++6y3bt0674knnvD69u3r3XfffcnXcJ0Pjf63/Ytf/ML7xz/+oSnNmzt3btrzQVzXxsZG7/DDD/emTJli3v8fe+wxr6KiwnvggQe8L8OJYPGVr3zFmz59evLjzs5Ob8iQIV5DQ4PV88pXW7duNT/ICxcuNB/v2rXLKy0tNW8avn//+9/mNW+88UbyP4KioiJv8+bNydfMmjXLq66u9mKxmIXvInc1Nzd7xxxzjPfCCy94X/3qV5PBguscnNtuu80766yzDvh8V1eXN2jQIO+3v/1t8jG9/mVlZebNVb3//vvm2i9dujT5mnnz5nmRSMTbuHFjlr+D/DBp0iTvO9/5Ttpj3/zmN80vKsV1DkbPYBHUdf3Tn/7k1dbWpr136H87o0aN+lLnm/dDIW1tbbJ8+XJTBkrdj0Q/fuONN6yeW75qbGw0t/379ze3en3b29vTrvHo0aNl2LBhyWust1pqPvzww5OvufDCC81mOCtXrgz9e8hlOtShQxmp11NxnYPz9NNPy6mnniqTJ082w0UnnXSSPPTQQ8nn161bJ5s3b0671roHgg6jpl5rLR/r1/Hp6/X9ZcmSJSF/R7npjDPOkJdeekk+/PBD8/E777wjr732mkycONF8zHXOjqCuq77mnHPOkWg0mvZ+okPhO3fuPOTzC30TsqBt27bNjPOlvtEq/fiDDz6wdl75Snef1TH/M888U8aOHWse0x9g/cHTH9Ke11if81+zv/8P/OcQN2fOHHnrrbdk6dKl+zzHdQ7O2rVrZdasWfLDH/5Qfv7zn5vrfdNNN5nrO3Xq1OS12t+1TL3WGkpSlZSUmMDNtY772c9+ZkKtBuDi4mLzXnz33XebcX3Fdc6OoK6r3mp/TM+v4T9XW1tbmMECwf81/d5775m/OhAs3cL45ptvlhdeeME0SiG7AVn/UrvnnnvMx1qx0J/rP//5zyZYIBiPP/64PProozJ79mw5/vjj5e233zZ/mGjDIde5cOX9UMhhhx1mknLPznn9eNCgQdbOKx/deOON8uyzz8orr7yStrW9Xkcdctq1a9cBr7He7u//A/85xIc6tm7dKieffLL5y0GPhQsXyv3332/u618KXOdgaKf8mDFj0h477rjjzIya1Gv1Re8beqv/f6XS2Tfaac+1jtMZSVq1+Na3vmWG6K655hq59dZbzUwzxXXOjqCua7beT/I+WGhp85RTTjHjfKl/rejHEyZMsHpu+UJ7gzRUzJ07V15++eV9SmN6fUtLS9OusY7B6Zu0f431dsWKFWk/yPqXuU5z6vkGX6jOPfdcc430rzr/0L+qtWzs3+c6B0OH8npOmdY+gOHDh5v7+jOub5yp11pL+jr2nHqtNeRpIPTpfx/6/qJj2RDZs2ePGbNPpX/o6TVSXOfsCOq66mt0Wqv2dqW+n4waNeqQh0EMz5HpptoN+8gjj5hO2Ouvv95MN03tnMeBTZs2zUxbWrBggffZZ58ljz179qRNg9QpqC+//LKZBjlhwgRz9JwGecEFF5gpq88//7w3YMAApkEeROqsEMV1DoZO5y0pKTHTIVevXu09+uijXp8+fby///3vadP19H3iqaee8t59913vsssu2+90vZNOOslMWX3ttdfMbJ5CnwaZaurUqd4RRxyRnG6qUyN1+vNPf/rT5Gu4zoc+e0ynlOuhv6p/97vfmfuffPJJYNdVZ5LodNNrrrnGTDfV36X63wnTTRN+//vfmzdkXc9Cp5/qvF30jv7Q7u/QtS18+sP6/e9/30xN0h+8K664woSPVB9//LE3ceJEMw9a31x+9KMfee3t7Ra+o/wNFlzn4DzzzDMmhOkfHaNHj/YefPDBtOd1yt7tt99u3lj1Neeee663atWqtNds377dvBHr2gw6pfe6664zb/iIa2pqMj+/+t5bXl7uHXnkkWbthdTpi1znQ/PKK6/s931Zw1yQ11XXwNCp2fo1NCRqYPmy2DYdAAAEJu97LAAAQO4gWAAAgMAQLAAAQGAIFgAAIDAECwAAEBiCBQAACAzBAgAABIZgAQAAAkOwAAAAgSFYAACAwBAsAABAYAgWAABAgvL/AVlWjbAvNv8kAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_points)\n",
    "# plt.ylim(-1, 5)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL WEIGHTS AND BIASES\n",
      "w1: 1\n",
      "w2: 0.5\n",
      "w3: 1\n",
      "w4: -0.5\n",
      "w5: 1\n",
      "w6: 1\n",
      "b1: 0.5\n",
      "b2: 0\n",
      "b3: 0.5\n",
      "\n",
      "\n",
      "#################################\n",
      "NEW WEIGHTS AND BIASES\n",
      "w1: 1.9745745043749072\n",
      "w2: 1.5954037453790957\n",
      "w3: 0.36455679957598336\n",
      "w4: 0.28349079054111503\n",
      "w5: 1.1393290934180376\n",
      "w6: 0.91732926944733\n",
      "b1: -1.1121701201811454\n",
      "b2: -0.954870595006538\n",
      "b3: 3.7281899463832677\n"
     ]
    }
   ],
   "source": [
    "print(\"ORIGINAL WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {original_w1}\")\n",
    "print(f\"w2: {original_w2}\")\n",
    "print(f\"w3: {original_w3}\")\n",
    "print(f\"w4: {original_w4}\")\n",
    "print(f\"w5: {original_w5}\")\n",
    "print(f\"w6: {original_w6}\")\n",
    "print(f\"b1: {original_b1}\")\n",
    "print(f\"b2: {original_b2}\")\n",
    "print(f\"b3: {original_b3}\")\n",
    "\n",
    "print(\"\\n\\n#################################\\n\\n\")\n",
    "\n",
    "print(\"NEW WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {new_w1}\")\n",
    "print(f\"w2: {new_w2}\")\n",
    "print(f\"w3: {new_w3}\")\n",
    "print(f\"w4: {new_w4}\")\n",
    "print(f\"w5: {new_w5}\")\n",
    "print(f\"w6: {new_w6}\")\n",
    "print(f\"b1: {new_b1}\")\n",
    "print(f\"b2: {new_b2}\")\n",
    "print(f\"b3: {new_b3}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction function, just doing the forward pass\n",
    "# again (but only that)\n",
    "def predict(x1, x2):\n",
    "    input1 = x1\n",
    "    input2 = x2\n",
    "\n",
    "    # NODE 1 OUTPUT\n",
    "    node_1_output = input1 * w1 + input2 * w3 + bias1\n",
    "    node_1_output = activation_ReLu(node_1_output)\n",
    "\n",
    "    # NODE 2 OUTPUT\n",
    "    node_2_output = input1 * w2 + input2 * w4 + bias2\n",
    "    node_2_output = activation_ReLu(node_2_output)\n",
    "\n",
    "    # NODE 3 OUTPUT\n",
    "    # we can just use Node 1 and 2 outputs, since they\n",
    "    # already contain the previous weights in their result\n",
    "    node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "    node_3_output = activation_ReLu(node_3_output)\n",
    "    return node_3_output\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7281899463832677"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try using the model with our prediction function\n",
    "result = predict(0, 3)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3, 9],\n",
       " [1, 6, 9],\n",
       " [3, 4, 14],\n",
       " [0, 4, 5],\n",
       " [0, 5, 5],\n",
       " [1, 6, 11],\n",
       " [0, 3, 7],\n",
       " [1, 6, 10],\n",
       " [2, 3, 9],\n",
       " [4, 6, 22],\n",
       " [0, 6, 7],\n",
       " [3, 3, 16],\n",
       " [4, 5, 22],\n",
       " [3, 5, 15],\n",
       " [4, 3, 22],\n",
       " [2, 4, 8],\n",
       " [3, 5, 16],\n",
       " [2, 5, 11],\n",
       " [4, 6, 25],\n",
       " [3, 4, 17],\n",
       " [4, 5, 24],\n",
       " [2, 3, 11],\n",
       " [3, 5, 15],\n",
       " [3, 5, 15],\n",
       " [2, 3, 7],\n",
       " [1, 3, 6],\n",
       " [1, 4, 8],\n",
       " [4, 4, 21],\n",
       " [0, 3, 6],\n",
       " [1, 5, 9],\n",
       " [3, 6, 16],\n",
       " [1, 5, 9],\n",
       " [3, 6, 18],\n",
       " [0, 5, 6],\n",
       " [3, 4, 17],\n",
       " [3, 5, 15],\n",
       " [2, 6, 14],\n",
       " [3, 4, 13],\n",
       " [0, 4, 7],\n",
       " [0, 4, 5],\n",
       " [0, 6, 7],\n",
       " [3, 3, 16],\n",
       " [1, 3, 8],\n",
       " [3, 3, 15],\n",
       " [0, 5, 9],\n",
       " [4, 6, 26],\n",
       " [4, 6, 22],\n",
       " [0, 4, 4],\n",
       " [0, 3, 5],\n",
       " [2, 4, 8],\n",
       " [1, 5, 6],\n",
       " [2, 3, 11],\n",
       " [1, 5, 9],\n",
       " [1, 6, 11],\n",
       " [3, 4, 14],\n",
       " [0, 6, 7],\n",
       " [3, 4, 13],\n",
       " [1, 5, 8],\n",
       " [2, 5, 10],\n",
       " [3, 5, 17],\n",
       " [1, 4, 8],\n",
       " [3, 5, 18],\n",
       " [0, 6, 8],\n",
       " [3, 4, 15],\n",
       " [2, 6, 11],\n",
       " [3, 4, 16],\n",
       " [3, 6, 18],\n",
       " [0, 3, 7],\n",
       " [4, 4, 20],\n",
       " [0, 3, 4],\n",
       " [2, 4, 9],\n",
       " [4, 4, 23],\n",
       " [4, 5, 23],\n",
       " [4, 6, 22],\n",
       " [3, 6, 18],\n",
       " [2, 4, 9],\n",
       " [1, 4, 7],\n",
       " [2, 4, 10],\n",
       " [0, 4, 4],\n",
       " [1, 6, 10],\n",
       " [3, 4, 16],\n",
       " [1, 6, 9],\n",
       " [4, 5, 22],\n",
       " [0, 4, 8],\n",
       " [2, 6, 10],\n",
       " [1, 5, 10],\n",
       " [4, 3, 22],\n",
       " [3, 4, 17],\n",
       " [0, 4, 8],\n",
       " [0, 3, 3],\n",
       " [1, 3, 8],\n",
       " [3, 6, 17],\n",
       " [4, 4, 20],\n",
       " [1, 5, 8],\n",
       " [2, 6, 12],\n",
       " [3, 5, 15],\n",
       " [0, 3, 4],\n",
       " [1, 5, 6],\n",
       " [3, 4, 16],\n",
       " [3, 5, 17]]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just to see what data we have, for reference\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
